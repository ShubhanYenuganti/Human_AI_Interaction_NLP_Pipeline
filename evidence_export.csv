"Source","Your finding about AI features","Your finding about Human performance degradation types","Your finding about Causal links between them","Excerpt","Justification","Validation (y/n)"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Core driver of autonomous systems and cross-domain innovation","","","Artificial Intelligence (AI) is at the core of recent scientific and industrial advances, such as Autonomous Driving (AD) [61,103,160] and Unmanned Aerial Vehicles (UAVs) [182,268]. AI technology is a cross-domain innovation driver for numerous novel application use cases [140]","This excerpt directly mentions AI's role in enabling autonomous systems (Autonomous Driving and UAVs) and its function as an innovation driver across domains, which are key AI capabilities distinguishing it from conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-specific traits enabling next-generation autonomous safety-critical systems","","","to deal with AI-specific traits. These solutions are also of interest for multiple transportation domains such as avionics[ 19,109], railway[ 19,20,200,208] and automotive[ 233,256], and industrial domain applications such as robotics[ 259] and driverless industrial trucks[ 134,141]. In all of these domains, AI technologies can be used to develop both traditional functional safety systems, as well as next-generation autonomous safety-critical systems[ 33,137,147,280].","This excerpt directly references 'AI-specific traits' and describes AI technologies' capability to develop 'next-generation autonomous safety-critical systems,' which aligns with the research focus on novel AI characteristics in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Black-box nature and explainability limitations","","","The most notorious include the “black box” nature of AI solutions causing limitations regarding their explainability and analyzability [3,51,104,235,268,282], and compliance limitations concerning software development lifecycle phases, such as specification correctness and completeness, design, testing, verification, and validation [107,164,190,194,200,207,219,268,278].","This directly addresses the opacity/explainability characteristic from the research focus, as it mentions the 'black box' nature causing limitations in explainability and analyzability, which are key features of novel AI systems versus conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: runtime learning/adaptation","","","runtime learning/adaptation of AI-based safety-critical systems (runtime) in Section 5,","This directly mentions 'runtime learning/adaptation' as a characteristic of AI-based safety-critical systems, which aligns with the context-aware/adaptive behavior priority (dynamic environment response, real-time learning).","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Multi-dimensional AI trustworthiness","","","AI trustworthiness spans several dimensions, such as engineering, ethics and legal, and this survey focuses on the safety engineering dimension.","This describes AI trustworthiness as spanning multiple dimensions (engineering, ethics, legal), which is a characteristic of complex AI systems that goes beyond the technical focus of conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Challenge of integrating AI with safety standards","","","However, reconciling both cutting-edge and state-of-the-art AI technology with safety engineering processes and safety standards is an open challenge that must be addressed before AI can be fully embraced in safety-critical systems.","This mentions the challenge of reconciling AI technology with safety processes, implying AI's advanced and potentially non-traditional nature that differs from conventional automation which is typically designed to comply with established standards.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI assistance to human engineers","","","AI can also support and assist human safety engineers in developing safety-critical systems.","This highlights AI's capability to assist human engineers, which is a feature of advanced AI systems compared to traditional automation that typically operates independently or with limited human interaction.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-enabled autonomous systems with ML learning","","","Artificial Intelligence (AI) can enable the development of next-generation autonomous safety-critical systems in which Machine Learning (ML) algorithms learn optimized and safe solutions.","This directly describes AI's role in enabling autonomous systems where machine learning algorithms learn solutions, which is a core AI capability distinct from conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based systems from traditional to autonomous","","","Focusing on the industrial and transportation domains, this survey structures and analyzes challenges, techniques, and methods for developing AI-based safety-critical systems, from traditional functional safety systems to autonomous systems.","This explicitly mentions AI-based safety-critical systems and the transition from traditional to autonomous systems, indicating AI's role in enabling more advanced, autonomous capabilities beyond conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Multiple AI Types and Safety Adaptations","","","These complexities are compounded by a significant fragmentation of the research contributions targeting the use of AI for developing autonomous systems with [ 267] and without specific safety considerations [ 190], different safety AI challenges [ 13,110,147], multiple use cases [19,140], multiple types of AI [ 87], different lifecycle phases (e.g., design [ 208,276], test[53,114,116],verification[ 5,77,117]),genericAIsolutions(e.g.,reinforcementlearning[ 16]) and safety adaptations (e.g., safe reinforcement learning [ 94]), with references to multiple existing[ 120,131]and noveldomain-specificsafetystandards[ 58,133,215,267,275,280,280].","This excerpt directly mentions 'multiple types of AI' and specific AI capabilities such as 'reinforcement learning' and 'safe reinforcement learning', which are novel AI characteristics compared to conventional automation, highlighting data-driven and adaptive features.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Online inference and actionable outputs","","","When the learned ML solution executes on an embedded system (electronics/software implementation with model parameters), it performs inferences in which the ML solution provides online actionable outputs based on the inputs provided.","This describes AI's capability to process inputs and generate outputs in real-time, which is a key feature distinguishing AI systems from static automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Multiple learning approaches including trial-and-error","","","The learning process can be, for instance, supervised (using labeled data), unsupervised (not using labelled data), semisupervised (using both labeled and unlabeled data) and reinforcement learning (“a machine learning agent(s) learns through an iterative process by trial and error”) [16,94,138].","This shows AI's capability for various learning methods, particularly reinforcement learning which involves adaptive behavior through trial and error.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Data-driven learning without explicit programming","","","Finally, Machine Learning (ML) is “the art and science of letting computers learn without being explicitly programmed” [112]. It is a subfield of AI that uses algorithms to learn from example training data sets that implicitly specify the intended functionalities, features, rules and constraints.","This represents a core AI capability where systems learn from data rather than following predetermined rules, distinguishing AI from conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Stochastic Nature and Uncertainties","","","Due to the intrinsic stochastic nature of ML training and associated epistemic uncertainties [277, 280], the achievable confidence usually depends on “complex hypotheses” [147] related to the different properties of the training and inference input data (e.g., data drift, distribution, correlation),","This directly relates to non-deterministic/data-driven decision-making, highlighting variability, uncertainty, and model-dependent aspects of AI systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability/Interpretability","","","—Explainability/Interpretability : “Extent to which an ML system can provide an explanation about a decision in a form understandable by a human” (e.g., see surveys [4,26,104]).","This directly addresses the opacity/explainability characteristic from the research focus, describing an AI feature related to transparency and human understanding of decisions.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Robustness","","","—Robustness : “Ability of the system to perform its intended function in the presence of: (a) Abnormal inputs (e.g., sensor failure), (b) Unknown inputs (e.g., unspecified conditions)”.","This represents an AI capability related to handling variability and unpredictable conditions, which aligns with context-aware/adaptive behavior and non-deterministic aspects.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Autonomous goal and domain modification","","","—Autonomous systems operate in an “open environment” (e.g., AD systems operate in an “open parameter space in which an infinite number of different traffic situations can occur” [ 222]) without human-in-the-loop control and supervision (e.g., AD SAE level 5 [ 231], avionics 3B [76], generic levels 7-10 [ 250]). As defined by ISO 22989, autonomy constitutes the highest level of automation in which “the system is capable of modifying its operating domain or its goals without external intervention, control or oversight” [ 138].","This excerpt directly describes a key AI capability where systems can autonomously modify their operating domain or goals without external intervention, highlighting a novel characteristic of advanced AI systems compared to conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: automation | Feature: Automation definition and automatic system characteristics","","","Automation/automated is defined as “pertaining to a process or system that, under specified conditions, functions without human intervention” [138]. —Automatic systems operate in a “closed environment” with well-defined safety rules and constraints known at design time [105]. Thus, the system is neither autonomous nor heteronomous. It simply executes an automation of safety functions without human intervention (e.g., railway interlocking system [161]) in compliance with applicable FuSa standards.","This excerpt explicitly defines automation and describes automatic systems as operating in closed environments with well-defined safety rules and constraints known at design time, which contrasts with novel AI characteristics like non-deterministic behavior or context adaptation. It helps distinguish conventional automation from more advanced AI systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Similarity-based classification algorithms","","","(4) Analogizers are similarity-based classification algorithms (e.g., Support Vector Machine (SVM)).","This excerpt defines analogizer AI, which uses classification algorithms based on similarity, a common AI technique for pattern recognition and data analysis not typically found in conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Iterative optimization algorithms","","","(5) Optimization algorithms aim to discover optimum or satisfactory solutions performing iterative updates and comparison procedures (e.g., Genetic Algorithm (GA)).","This excerpt explains optimization algorithms, which are AI techniques for finding solutions through iterative processes, highlighting AI's ability to handle complex optimization tasks dynamically.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Learning algorithms based on optimization and neural networks","","","(1) Connectionists are design learning algorithms based on optimization techniques such as gradient descent, where models are represented as Neural Networks (NNs) and specialized Deep Learning (DL) models [103,212] such as Deep Neural Networks (DNNs) [181],","This excerpt directly describes a type of AI (connectionist) that uses learning algorithms and neural networks, which are core AI capabilities distinguishing it from conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Probabilistic graphical models for inference","","","(2) Bayesians are probabilistic outcome-based graphical model representations for probabilistic inference such as Bayesian and Markov networks.","This excerpt details Bayesian AI, which involves probabilistic models and inference, a key AI feature that introduces uncertainty and data-driven decision-making compared to deterministic automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Logic-focused algorithms and rule-based systems","","","(3) Symbolists are logic-focused algorithms such as rule-based programming (e.g., “always stop in front of a stop sign”), Constraint Programming (CP), decision trees (e.g., random decision forest [269]), fuzzy logic [172] and rational agents [156].","This excerpt outlines symbolist AI, which uses logic-based approaches like rule-based programming and decision trees, representing AI capabilities that can handle complex reasoning tasks beyond simple automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Failure from Unforeseen Conditions","","","safety-critical systems could fail even in the absence of an electronic/software failure. For example, the intended safety function fails due to unexpected operating conditions not considered in the perception ML algorithm training [162].","This describes how AI/ML systems can produce unpredictable failures due to data/model limitations, aligning with non-deterministic/data-driven decision-making where outputs vary with training data and model states.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: control | Feature: autonomous decision-making without human control","","","a novel scenario in which the safety system makes autonomous decisions without human control/supervision in an open environment.","This represents an AI feature because it describes autonomous decision-making without human intervention, which is a key characteristic of advanced AI systems versus conventional automation that typically requires human supervision.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI implementation in safety functions and development","","","In aproduct/runtime, a safety function can be implemented using AI technology ( A), or a non-safety-related function that could interfere with safety function(s) ( C) or be interference-free ( D). Furthermore, AI technology can also be used in the safety-critical development process(B).","This represents an AI feature as it explicitly mentions the use of AI technology for implementing safety functions and in development processes, highlighting AI's role in safety-critical applications.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Runtime field learning and dynamic reconfiguration","","","(2) Runtime: The AI-based safety-critical system integrates AI technology with runtime field learning capability (online). A runtime can also be considered a product variant that integrates dynamic reconfiguration (IEC61508-7C.3.10) and becomes a “one of a kind” system.","This excerpt directly mentions 'runtime field learning capability' and 'dynamic reconfiguration', which are key characteristics of context-aware/adaptive AI systems that respond dynamically to their environment, aligning with the research focus on novel AI features like context-aware/adaptive behavior.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: control | Feature: Safety Bag/Diverse Monitor Runtime Checker","","","the safety bag /diverse monitor (C.3.4 IEC 61508-7) technique (a.k.a., run-time checker), safely monitors that the results provided by an AI item are safe [120,264]. So, the safety bag becomes the safety function that prevents unsafe states, and the AI item does not require safety standard compliance.","This excerpt highlights a control mechanism (safety bag/diverse monitor) that monitors AI outputs in real-time to ensure safety, representing a novel AI feature where external validation compensates for potential AI unpredictability or opacity, aligning with the research focus on managing AI characteristics in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based ADAS functionality and driver responsibility","","","AI-based ADAS using class III AI technology are not considered safety-critical systems, and the driver itself is responsible for driving the vehicle, monitoring the ADAS operation and taking vehicle control in a short time if the ADAS detects and notifies that can no longer provide the intended functionality [66,163,285].","This excerpt describes the capabilities and limitations of AI-based ADAS, highlighting its role in driving assistance and the reliance on human oversight, which relates to AI features in high-risk industries like transportation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Data-driven decision-making","","","In any case, a relevant difference between traditional safety engineering and ML workflows is that the former is specification-driven and the latter data-driven [ 217].","This highlights a key difference where ML workflows rely on data-driven approaches, which is a core aspect of non-deterministic AI systems where decisions vary based on data and model states.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Uncertainty-related failures","","","VDE-AR-E2842-61-1 [ 280] states that AI technology should be considered a third type of technology (in addition to electronics and software) due to its unique characteristics (e.g., uncertainty-related failures).","This directly mentions uncertainty-related failures as a unique characteristic of AI, which aligns with non-deterministic/data-driven decision-making where outputs vary and failures can be unpredictable.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Uncertainty in formal verification","","","Regarding AI-based heteronomous and autonomous systems, the generic application of offline formal verification seems questionable due to limitations such as the uncertainty and difficulty of explicitly formalizing all safety specifications, rules and constraints required for the safety verification, and the potential high","This excerpt directly mentions 'uncertainty' as a limitation in formalizing safety specifications for AI-based systems, which relates to non-deterministic/data-driven decision-making where outputs vary with data/model states, leading to unpredictable failures.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based safety-critical system development with inference execution","","","4 PRODUCT - AI-BASED SAFETY-CRITICAL SYSTEM This section describes the challenges, techniques, and methods used to develop AI-based safety-critical systems (the product) from traditional FuSa to autonomous systems. The description structure follows the product layers presented in Section 3.4 and summarized in Figure 3: AI system (Section 4.1), AI item (Section 4.2), and inference execution platform (Section 4.3). We also provide a brief summary of tools and training platforms (Section 4.4).","This excerpt explicitly mentions 'AI-based safety-critical systems' and 'inference execution platform', which are key AI features involving computational inference and execution in safety-critical contexts, distinguishing them from conventional automation by highlighting AI-specific development and deployment aspects.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Model learning and verification with operational data feedback","","","management, model learning, and model verification phases. The resulting verified model is then deployed to an execution platform. And the model execution can feed the data management phase with operational data for future model releases.","This excerpt mentions 'model learning' and 'model verification' phases, which are AI-specific capabilities involving data-driven model development and validation, and describes a feedback loop where model execution provides operational data for future releases, indicating adaptive or iterative improvement processes.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability as a high-level property","","","For example, the safety assurance case arguments of an AI-item (Section 4.2) can be built on claims of high-level properties [17,147,228], such as the ML properties defined in Section 2.3 (e.g., explainability, monitorability, auditability, provability), arguments based on specific methods used for uncertainty","This mentions explainability as a key property for AI systems, directly addressing opacity/lack of explainability by highlighting it as a requirement for safety assurance in AI items.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Uncertainty-related failures and management","","","However, for the latter, the safety assurance case should also support the management of uncertainty-related failures (see VDE-AR-E2842-61 [280]) inherent to heteronomous, autonomous and (non-trivial) AI-based systems. This AI uncertainty management includes, among others, uncertainty sources identification and uncertainty reduction argumentation [248,265].","This directly addresses non-deterministic/data-driven decision-making by highlighting uncertainty-related failures inherent to AI systems and the need for uncertainty management, which relates to variable outputs and unpredictable failures.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Unspecifiable safety functions due to scenario variability","","","the safety functions (and previous safety goals) can only be specified as “intended functionality” with a set of high-level goals and objectives [39], or iterative partial specifications [234], because it is not generally feasible to fully specify the safety functions (w.r.t. all possible scenarios) with a set of safety requirements, rules, constraints (e.g., [32]).","This excerpt highlights the non-deterministic nature of AI systems, where safety functions cannot be fully specified for all possible scenarios, indicating unpredictable behavior and uncertainty in decision-making, which contrasts with conventional automation's deterministic specifications.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Semantic gap from incomplete specifications","","","This creates a “semantic gap” [49,193] between the intended functionality and the specified functionality, which sometimes is based on examples where anomalous and edge/corner case examples are a minority.","This excerpt illustrates the opacity and non-deterministic aspects of AI systems, where the gap between intended and specified functionality leads to unpredictable failures, especially in handling rare or anomalous cases, unlike conventional automation with clear specifications.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Novel testing techniques for AI-based autonomous systems","","","the validation should consider the definition of a strategy with a framework that combines multiple testing techniques and approaches, with the adaptation of existing techniques and the definition of novel techniques specific for AI-based autonomous systems.","This excerpt highlights a key AI feature: the need for novel and adapted testing techniques specifically designed for AI-based autonomous systems, as conventional methods are insufficient, indicating the unique challenges posed by AI characteristics like non-determinism or opacity.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: automation | Feature: Neural network-based automatic control","","","Beyond flight controllers, a 2001 review by Lisboa identified a diverse set of industrial use of NNs in safety-related areas [ 180]. Examples include power generation and transmission, process industries, and transport industries. A common theme among many applications is that NNs were used for automatic control.","This excerpt mentions the use of neural networks (NNs) for automatic control in safety-related industrial applications, which relates to AI capabilities in automation, though it does not explicitly detail novel AI characteristics like non-determinism or opacity.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: dynamic environment adaptation","","","autonomous systems as they operate in complex and continuously evolving environments [33,50].","This describes AI systems' ability to adapt to complex and continuously evolving environments, highlighting context-aware and adaptive behavior.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: implementation and deployment","","","AI items are implemented as electronics, software, model configuration and combinations of the previous using traditional FuSa standard technical requirements (e.g., IEC 61508-3 software development guidelines) and deployed on execution platforms (see Section 4.3).","This outlines the technical implementation and deployment of AI items, including software and model configurations, which are key AI capabilities.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability challenge in NN VVT","","","Discussed challenges of NN VVT include state-space explosion, robustness, explainability, co-engineering of NNs and conventional software, and challenges in specifications of ML concepts.","This directly references 'explainability' as a challenge for neural networks, which aligns with the opacity/explainability characteristic of AI systems where internal reasoning is not transparent.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: control | Feature: Safety bag and safety envelope techniques for AI adoption","","","dimensional design space that limits the application of formal verification and brute-force testing approaches [33,183,268,278]. A similar limitation applies to online approaches such as the safety bag technique, but in this case, formally specified operational rules can be used to specify safety envelopes (a.k.a., safety monitor, runtime monitor, runtime verification, supervisor, guardian agent, safety layer, safety net) [18,64,71,105,183,223,235,240,241,263]. For example, model checking has already been applied in some specific applications (e.g., AD vehicle overtaking [223]) for the development of formally defined safety envelope software (runtime monitor / verification) [183,223]. Safety bag and safety envelope type techniques provide a potentially generic safe approach for the adoption of cutting-edge and state-of-the-art AI technology solutions (as a compensatory measure to adapt Class III AI technology to Class II). However, its use must consider the safety of the system as a whole because, for example, excessive false alarms could lead to new system-level hazards (e.g., cascade errors in systems with multiple safety functions) and should also consider human cognitive limitations (e.g., cognitive overload, oversight and reaction time limitations) [210,263]. The avionics domain ASTM F3269 [18] standard describes a reference run-time assurance architecture to safely bound the behavior of “complex functions” integrated in aircraft systems such as UAVs and UASs. This architecture implements a safety bag type technique where a safety monitor monitors the safe operation of a “complex function” (e.g., AI-based function) and activates the safe state or switches to a recovery control function [18,64,71,240,241,268] if operating outside established safe operation constraints and rules.","This excerpt directly addresses novel AI characteristics by describing techniques (safety bag, safety envelope) used to manage the non-deterministic and opaque nature of AI systems in high-risk applications, ensuring safe operation through monitoring and control mechanisms.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI system safety assurance and error management","","","At all levels, the overall AI-based safety-critical must comply with the required FuSa, heteronomous, autonomous and AI standards. At the highest AI system level, developers define safety assurance cases with the arguments and required evidence needed to justify that the system is safe for its purpose; developers identify and manage uncertainty sources and successfully verify, test and validate the system. AI item developers control and mitigate systematic errors using at least the appropriate development lifecycle and techniques, appropriate tools and training platforms, and the obtained ML properties provide sufficient evidence to justify the previous assurance case argumentation. Finally, the underlying platform must avoid, control and mitigate systematic and random errors providing sufficient evidence to the previously defined assurance case argumentation.","This excerpt highlights AI-specific features such as managing uncertainty sources and using ML properties for evidence, which are critical for addressing the non-deterministic and data-driven nature of AI systems in safety-critical contexts.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: automation | Feature: Safety assurance cases for traditional FuSa systems","","","Safety assurance cases are commonly used in the development and certification/assessment of traditional FuSa systems to justify that a given safety-critical system is acceptably safe for its purpose, using a structured and evidence-supported safety argumentation [33,38,131,265]. For example, the safety case provides a structured argumentation of systematic and random errors management, from high-level architectural and lifecycle systematic aspects down to the underlying execution platform (see Table 4).","This excerpt represents conventional automation features, as it discusses safety assurance cases used in traditional functional safety systems, providing a baseline contrast to novel AI characteristics mentioned elsewhere in the chunk.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: dynamic post-release adaptation","","","Third, the AI systems themselves can be dynamic post-release if retraining of internal models is enabled (see Section 5).","This describes AI systems that can adapt and change behavior after deployment through retraining, representing a context-aware or adaptive feature compared to static conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Neural network design specifics","","","And software architecture specifications must also encompass fundamental NN design elements and specifics such as activation functions and hyperparameters controlling the learning process.","It emphasizes AI capabilities through neural network components that influence learning and behavior, relevant to AI system features.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Data-driven decision-making","","","Supervised learning relies on data (for model training and model verification) being treated as first-class citizens during software and systems engineering. As a result, data management needs a rigorous process encompassing collection, augmentation, preprocessing, analysis, and maintenance.","It highlights the data-driven nature of AI systems, where outputs vary based on model training and verification data, aligning with non-deterministic characteristics.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: adaptation | Feature: Learning behavior capture","","","Furthermore, specifications and the associated test specifications must be augmented to capture the learning behavior of NNs.","It addresses the adaptive nature of AI systems, where learning behavior must be captured in specifications, relating to context-aware or adaptive features.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: interpretability","","","qualities such as interpretability, efficiency, and privacy are much less studied [ 292].","Interpretability is directly related to the opacity/explainability characteristic of AI systems, as it addresses the ability to understand and explain AI decisions, which is a key challenge in human-AI interaction.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI capabilities in perception and generalization","","","Still, the representation learning offered by DL has enabled several breakthroughs during the 2010s and trained DL models have outperformed human performance in a range of restricted tasks. From the perspective of this review, the use of DL has disrupted computer vision and enabled perception systems able to generalize to diverse operational contexts. Advances in the automotive industry have been particularly prominent, with DL being a key enabler for AD, and in various ADAS such as automatic emergency braking and lane keeping assistance [28,61,160].","This excerpt describes novel AI characteristics such as outperforming human performance and enabling perception systems that generalize to diverse operational contexts, which are advanced capabilities beyond conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Opacity due to logic in training data","","","the potential benefits of using DL in safety-critical applications. In general, developing safety-critical systems that rely on DL shares the same challenges as NNs – as can be seen in Dey and Lee’s recently proposed three-layered conceptual framework [70]. However, the fact that contemporary deep NNs can be composed of billions of neurons, organized into complex architectures, further amplifies all challenges. Several VTT practices mandated by FuSa become less effective, e.g., code reviews matter less if the logic resides in the training data [235] and the value of adequacy testing metrics is questionable [108].","This excerpt directly addresses opacity/explainability by noting that code reviews become less effective when the logic resides in training data, which is a characteristic of black-box AI systems where internal reasoning is not transparent.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Data-driven functionality specification","","","Regarding ML properties for the construction of safety assurance cases, there is a rich variety of research contributions applicable to both NNs and DL models: —Data Quality : The training data implicitly specify the intended functionality, rules and constraints. So data quality is of paramount importance as described by Ashmore et. al [ 17], and the data management phase must produce datasets that exhibit at least properties such as: relevance, completeness, balance, and accuracy [ 17,217]. Training data is split for model training and model verification.","This represents an AI feature because it highlights how ML models rely on training data to implicitly define their intended behavior, rules, and constraints, which is a core aspect of data-driven decision-making in AI systems, distinguishing them from conventional automation that may use explicit programming.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Auditability and verification of DNN behavior","","","Auditability : Huang et al. propose a framework for the automated safety verification of DNNs made classification decisions [ 117]. Verification is also put forward by Kuper et al. [170] as a viable solution to confirming that NNs behave as intended. In addition, they further suggest to create and use design principles for NNs that produce DNNs that are more amenable to verification [ 170].","This excerpt directly addresses the opacity/explainability characteristic by proposing automated safety verification and design principles to confirm that neural networks behave as intended, which relates to trust and regulation challenges in AI systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Black-box AI explainability","","","made by black-box type AI-based items [104] so that the user is aware of the rationale for decisions","This excerpt directly addresses the opacity and lack of explainability in AI systems, as it mentions 'black-box type AI-based items' and the need for users to understand the rationale behind decisions, which aligns with the research focus on trust and regulation challenges.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Runtime explanations for ML decisions","","","decision trees can provide runtime explanations of decisions made by an ML-based co-pilot to an aircraft pilot, who must understand them and react safely in case of wrong decisions [147].","This excerpt illustrates the explainability aspect of AI systems by detailing how decision trees offer runtime explanations to enhance transparency and enable safe human intervention, addressing the research focus on opacity and trust challenges.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Adaptive and context-aware behavior in AI systems","","","Generic adaptative control system II, II Connectionist, Symbolist Generic, Connectionist Safe adaptation [ 172,173] Safe adaptation [ 142] AAerospace Adaptative guidance I, II Connectionist Limited adaptation [ 150] A, C Industrial ILC-based hydraulic machinery I, II Optimization Limited actuation [ 272]","This excerpt directly mentions adaptive control systems, safe adaptation, limited adaptation, and limited actuation, which are features of context-aware and adaptive AI behavior, aligning with the research focus on dynamic environment response and variability handling in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: SWaP constraints in AI applications","","","Additionally, in some specific applications, such as AD [258] and UAV systems (e.g., drone) [71,182], execution platforms must meet Size, Weight, and Power (SWaP) constraints while providing the required computing performance and FuSa compliance support [209,210].","This excerpt mentions AI applications (AD and UAV systems) that require execution platforms to handle specific constraints (SWaP), which is a feature of advanced AI systems in high-risk industries, contrasting with simpler conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI software frameworks and execution platforms","","","Execution platforms are commonly composed of a hardware platform with High Performance Computing (HPC) capability (e.g., Graphics Processing Unit (GPU)), a software framework (e.g., hypervisor, AUTOSAR, Robot Operating System (ROS)) and an AI software framework (e.g., YOLO, TensorFlow).","This excerpt highlights AI-specific components like AI software frameworks (e.g., YOLO, TensorFlow) integrated into execution platforms, which are novel compared to conventional automation that may not use such data-driven or model-based tools.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Constrained AI Runtime Learning/Adaptation","","","it is feasible to consider constrained AI runtime learning/adaptation approaches (Section 5.1), for which correctness and completeness of all possible variants is considered in the safety-critical system development process and safety certification/assessment.","This directly mentions AI runtime learning/adaptation, which is a key characteristic of context-aware/adaptive AI systems that respond dynamically to environments, aligning with the research focus on novel AI features like context-aware/adaptive behavior.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Runtime AI Online Learning/Adaptation","","","5 RUNTIME - AI ONLINE LEARNING/ADAPTATION This section describes selected techniques and methods for the AI online learning/adaptation of AI-based safety-critical systems (runtime). By default, runtime adaptation leads to a “one of a kind” safety-critical system instantiation that, if unconstrained, is beyond the scope of current and novel safety standards [142,165]. For example, in this scenario, an AD system might adapt and learn new behaviors [225] that were not considered, verified, and validated in the offline development and safety certification/assessment process [165]. And this adaptation could even be implemented as continuous [11] and lifelong learning [205].","This excerpt directly discusses context-aware/adaptive behavior in AI systems, specifically runtime adaptation and learning that responds dynamically to environments, leading to variability and unpredictability, which aligns with the research focus on novel AI characteristics.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Runtime Learning/Adaptation with Safety Monitoring","","","can also ensure that the outputs provided by the AI-item subject to runtime learning/adaptation are safe. As previously explained, the safety bag becomes the safety function and the AI-item becomes a non-safety function ( C). For example, the avionics Intelligent Flight Control System (IFCS) aims to safely optimize aircraft flight performance with two NNs, one trained offline and the second one while the aircraft is in operation ( Online Learning Neural Network (OLNN) )[260]. And the system runs two safety monitors, one for each NN, where the OLNN safety monitor checks the safeness of the provided outputs.","This excerpt directly mentions 'runtime learning/adaptation' and 'Online Learning Neural Network (OLNN)', which are novel AI characteristics involving dynamic environment response and real-time adaptation, contrasting with conventional static automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: runtime learning and adaptation","","","runtime learning/adaptation, either through a safety compliant adaptation ( Class I) or a safety bag that checks the adaptation outcome ( Class II, see Section 5.1.1). For example, Johnson et al. [150] describe using NNs to perform adaptive control of an autonomous launch vehicle guidance system. The system uses an adaptive NN-based error cancellation algorithm to cancel the control error due to differences between the actual vehicle dynamics and the design-time vehicle model, with a “bounded weight update law” that safely constrains the runtime learning/ adaptation.","This directly describes context-aware/adaptive behavior where AI systems dynamically respond to environments through real-time learning and adaptation, a key novel AI characteristic versus conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: safe adaptive control with learning algorithms","","","Additionally, Jacklin et al. [142] describe challenges and example techniques for the development of safe adaptive control solutions using learning algorithms such as NNs (e.g., learning convergence, speed of learning convergence, learning algorithm stability).","This excerpt mentions the use of learning algorithms (such as NNs) for adaptive control solutions, which involves dynamic response and real-time learning capabilities. It specifically addresses challenges like learning convergence and stability, indicating the adaptive and potentially non-deterministic nature of these AI systems compared to conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: runtime adaptation for safety-critical control","","","runtime learning/adaptation algorithm to be safety-compliant. This is because both must perform safety functions, safe inference, and safe runtime learning/adaptation. For example, Kurd et al. [172,173] describe a safety-critical “gas turbine aero engine control” based on a hybrid TAI (connectionist ,fuzzy) that performs runtime adaptation to provide safe control while safely adapting to the engine degradation and environmental change.","This excerpt directly describes an AI system feature where the system dynamically adapts its behavior in real-time (runtime adaptation) to changing conditions (engine degradation, environmental change) while maintaining safety compliance, which aligns with the context-aware/adaptive behavior characteristic of novel AI systems versus conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: runtime learning/adaptation with safety constraints","","","runtime learning/adaptation cannot exceed given dangerous output actuation values (e.g., excessive force, energy, voltage). This could be implemented in different ways, such as design-time constraints (e.g., limited input energy leads by design to limited output energy), AI-based safety function that guarantees a limited actuation (A1, Class I) or a safety bag that monitors and ensures that output actuation values are within safe limits (C, Class II, see Section 5.1.1).","This directly mentions 'runtime learning/adaptation' as an AI characteristic that enables dynamic response and adaptation to environments, which is a key novel feature of AI systems compared to conventional automation. It specifically addresses how this adaptive behavior must be controlled to prevent excessive outputs, highlighting the variability and unpredictability inherent in such systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Uncertainty Quantification","","","Quantify uncertainty Bayesian [86,92,158,159]","This represents a non-deterministic/data-driven AI feature as it involves handling uncertainty, which is a key characteristic of AI systems with variable outputs and model-dependent decisions.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: control | Feature: Safety monitoring for adaptation","","","a runtime monitor can be used to monitor and ensure that the learning/adaptation actuation results are safely limited (e.g., compensatory force, dynamic behavior, settling time [272]).","This represents an AI feature as it involves control mechanisms to manage the safety of adaptive AI systems, ensuring that learning and adaptation processes operate within defined constraints to prevent unpredictable failures.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Online adaptation and learning","","","ILC algorithm for hydraulic machinery systems that can be used online to adapt and learn the compensating force required to reduce overshoot and settling time even with unknown knowledge of the valve dynamics.","This represents an AI feature as it demonstrates real-time learning and adaptation to dynamic environments (unknown valve dynamics), handling variability and unpredictability through data-driven adjustments.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI software tools for design optimization","","","state-of-the-art non-safety related AI software tools, engineers and methods can be used for the design optimization proposal activity.","This excerpt explicitly references 'AI software tools' as part of the design and implementation process, highlighting AI capabilities in optimizing safety functions, which aligns with novel AI characteristics in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: adaptation | Feature: Offline-defined runtime configuration transitions","","","systems[ 201]canbetranslatedinthesafety-criticaldomainasalibraryofpossibleconfigurations defined and assessed offline, to which the system can transition during runtime ( Class I). This is the adaptation of a common approach used in the development of traditional safety-critical systems, where all possible configuration and operational modes are defined and assessed offline (e.g.,normaland degradedmodes of operation).","This represents an AI/autonomous system feature as it involves dynamic adaptation during runtime (context-aware/adaptive behavior) through predefined configurations, contrasting with static conventional automation by enabling transitions between operational modes based on conditions.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based development assistance techniques","","","6 PROCESS - AI-BASED DEVELOPMENT ASSISTANCE This Section describes AI-based offline techniques and methods that support and facilitate thetraditional safety engineering of safety-critical systems (Section 6.1) and the AI safety engineeringofAI items (Section6.2).","This represents an AI system feature as it highlights AI-based methods (a novel characteristic) used to support safety engineering, indicating advanced capabilities beyond conventional automation in facilitating development processes for safety-critical systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: platforms/frameworks | Feature: Integration into design tools","","","AI solutions are also commonly integrated into hardware ASIC design tools, FPGA synthesis tools and software compilers [115,176,286].","This excerpt demonstrates the integration of AI solutions into diverse platforms (e.g., ASIC, FPGA, compilers), which is a feature enabling novel applications in system development, contrasting with traditional automation that might be more limited in such integrations.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Comprehensive TAI coverage","","","AI-based solutions that cover all TAIs summarized in Section 3.1.","This excerpt highlights a capability of AI-based solutions to comprehensively address Trustworthy AI aspects, which is a novel characteristic compared to conventional automation that may not inherently cover such broad ethical and safety dimensions.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: ML-based solutions and workflow for AI safety engineering","","","Concerning the AI safety engineering of AI-based (sub)systems and items, most research contributions describe ML-based solutions for connectionist-based products. So this Section follows the ML workflow described in Section 3.5and Figure 2(b). As summarized in Table 7b, research contributions that target the data management and model learning phases are scarce, and solutions that target the model verification phase are more abundant specially for the VVT activities of heteronomous/autonomoussystems.","This represents an AI feature as it explicitly mentions AI-based systems, ML-based solutions, and the ML workflow, which are key characteristics of AI systems compared to conventional automation, involving data-driven and model-dependent processes.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: automation | Feature: Automated ML model design via autoML","","","optimal DL hyperparameters for developing ML models for autonomous driving tasks is time-consuming for engineers. And autoML has been (functionally) evaluated as a successful approach for the design automation of perception tasks ML models, with results that outperformed the ones obtained by trial-error approaches by experienced engineers (higher accuracy, less latency) [ 283].","This represents an AI feature as it highlights the automation of ML model development (autoML) for perception tasks in autonomous driving, which is a key capability of AI systems compared to conventional automation, focusing on design automation and performance optimization.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Adaptable safety-critical systems with Bayesian learning","","","Finally, Fan et al. [ 86] and Fisac et al. [ 90] describe Bayesian model learning solutions via Bayesian NNs or statistical Gaussian processes, which support the optimization and safe control design of adaptable safety-critical systems with control stability and safe limits.","This represents a context-aware/adaptive AI feature as it involves dynamic response and adaptation through Bayesian model learning, supporting the design of systems that can adjust to environments while maintaining safety, which contrasts with static conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Unsupervised modeling and generation of test scenarios","","","Connectionist solutions: DL technologies can “discover intricate structures well in high-dimensional data and learn the idea of correct representation of data” [ 8,254]. Therefore, they are commonly used for the unsupervised modeling and generation of test scenarios/cases, such as vehicle maneuver modeling using autoencoder and Generative Adversarial Network (GAN) solutions [ 167]. One advantage of this approach is that in both cases, the learned model has been trained to generate trajectories that even the discriminator (for GAN) is not able to distinguish between real life or synthetic trajectories [ 167].","This excerpt describes AI capabilities (DL technologies) for unsupervised modeling and generation of test scenarios, highlighting their ability to discover structures in data and generate realistic trajectories, which is a novel AI characteristic compared to conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Unsupervised generation and optimization of test cases","","","Bayesian solutions [ 2,93,148,293] are also commonly used for the unsupervised generation of test data, test cases and test scenarios using the learned probability distribution for the given problem to generate variants. For example, generation of intersection scenes [ 148] and traffic scenarios [ 281]. And for a given test scenario, Bayesian optimization can be used to learn from observed system outputs and define test cases that could violate predefined safe operation boundaries [ 93].","This excerpt describes AI capabilities (Bayesian solutions and optimization) for unsupervised generation of test data and scenarios, and learning from system outputs to identify potential safety violations, which are novel AI features enabling adaptive and data-driven testing processes.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI self-verification capability","","","AI technology ( process) can also be used for the verification of AI-items.","This represents an AI feature where AI technology is used to verify other AI systems, demonstrating an advanced capability beyond conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: lack of definition for trustworthiness","","","trustworthiness “has not generally accepted definition” at least in the context of AI-based safety-critical systems.","This represents an AI feature because it addresses the opacity and trust challenges in AI systems, as trustworthiness is a key concern for novel AI characteristics like non-deterministic behavior and lack of explainability in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Complex AI lifecycle phases","","","Concerning cybersecurity, the lifecycle of AI is complex by nature, and it involves several phases such as planning, data management, model training, model evaluation and operation.","This excerpt highlights a characteristic of AI systems - their complex, multi-phase lifecycle involving data management, model training, and evaluation, which distinguishes them from simpler conventional automation systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: Trustworthiness and verification in AI-based safety-critical systems","","","Indeed, the increasing importance of trustworthiness in the development of AI-based safety-critical systems is emphasized in the VDE-AR-E2842-61 standard with the Trustworthiness Performance Level (TPL) (TPL 0-4) definition that requires trustworthiness attributes traceability through the AI-based system development activities, design patterns supporting the verification of AI properties, and compliance with specific techniques/measures pending definition details in the current draft[ 280].","This represents an AI feature as it directly addresses the development and verification of AI properties in safety-critical systems, highlighting the need for traceability, design patterns, and compliance measures specific to AI, which are novel characteristics compared to conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based safety-critical systems with autonomy levels and runtime decision-making","","","AI-based safety-critical systems that can potentially provide significant societal benefits (e.g., potential car accidents and fatalities reduction with AD systems [155,222]) with new risks, e.g., which is the acceptable residual risk? [49]. Moreover, as analyzed by Widen et al. [285] and Koopman et al. [163] for the automotive AD domain, the safety culture associated to the engineering ethics should also encompass the overall business ethics considering aspects such as cooperation with governments for the definition of safe technology regulations, high safety requirements for road testing and deployment, safe management of tradeoff dilemmas between financial risks and safety risks, marketing-engineering-regulation coherency for delivered autonomy levels (e.g., L2+ [66,163]) and transparency. On the other hand, machine ethics is associated with the moral and ethical decisions that an AI-based product/runtime must make during operation.","This excerpt directly mentions AI-based safety-critical systems, autonomy levels (e.g., L2+), and runtime decision-making, which are key AI features distinguishing them from conventional automation in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Variable moral decision-making based on data/cultural inputs","","","autonomous vehicle driving morale dilemma in which people of different ages, genders and professions are in deadly danger. The result of these experiments confirmed that cultural variation and other variation sources (e.g., economic) lead to different moral and ethical decision preferences, concluding that there is no single universal preference for machine ethics. However, the German ethical guidelines strictly prohibits decisions made on human classifications (e.g., gender, age) [163,184]. In any case, we should request AI-based safety-critical systems to anticipate and mitigate dangerous situations to avoid such moral dilemmas (e.g., defensive driving strategies in AD system) [163,184].","This excerpt describes how autonomous systems' decisions vary with data inputs (cultural, economic variations) and model states (ethical preferences), leading to unpredictable outcomes and lack of universal standards, which aligns with non-deterministic/data-driven features where outputs are uncertain and model-dependent.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability requirement for AI decisions","","","such as providing explainability [193] to support “the right to obtain an explanation of the decision” made by AI-algorithms (“meaningful information about the logic involved” [81]) on behalf of an individual, as established by the General Data Protection Regulation (GDPR)[81].","This directly addresses the opacity/explainability characteristic of AI systems, highlighting the challenge of providing transparency and meaningful information about the logic behind AI decisions to meet regulatory and trust requirements.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: context_adaptive | Feature: Iterative and dynamic life cycle with data collection for ML updates","","","the iterative and dynamic life cycle of AI-based systems (e.g., collect operational data to update the ML model) in the context of industrial and transportation domain systems with long product lifetimes (e.g., >=30 years [209]).","This represents context-aware/adaptive behavior as it involves AI systems dynamically responding to environments by updating models based on operational data, highlighting variability and real-time adaptation in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: non_deterministic | Feature: Data-driven decision-making","","","As the ML workflow is data-driven, the data management must ensure the appropriate data quality (e.g., edge/corner cases, data distributional drift) for the safe model training and verification. Data must provide a complete, correct and representative specification of the intended safety functionalities, rules and constraints.","This excerpt directly mentions 'ML workflow is data-driven', which aligns with the non-deterministic/data-driven characteristic where outputs vary with data/model states, leading to uncertainty and unpredictable failures. It specifically discusses data quality requirements for safe model training, including edge/corner cases and data distributional drift, which are key aspects of data-driven AI systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability as an ML property for safety assurance","","","System-level safety assurance cases use ML properties to justify that the system is safe for its purpose (e.g., explainability, provability, robustness, auditability).","This directly references 'explainability' as a property of ML systems, which relates to the opacity/explainability characteristic of novel AI systems, addressing trust and regulation challenges in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: automated decision-making","","","Specific AI technology instantiations that perform automated decision-making (A1) have already been used with compensatory measures (e.g., safety bag) for the development and certification of automatic safety-critical systems (e.g., railway interlocking [ 161]).","This directly describes an AI capability (automated decision-making) used in safety-critical systems, which is a core feature distinguishing AI from conventional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: AI-based safety management without offloading","","","However, there is still a significant pending research effort and challenge to define generic AI methods, techniques and processes for developing AI-based safety-critical systems that cannot offload safety management onto humans or non-AI systems.","This addresses a key AI feature: autonomous safety management without reliance on humans or other systems, which is a novel characteristic compared to traditional automation.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: AI_capability | Feature: heteronomous safety functions with human supervision","","","And the use of AI technology for developing specific heteronomous safety functions that require human supervision ( A2) is also common in the latest ADAS systems.","This describes AI applications in safety functions that involve human-AI interaction (supervision), highlighting a specific AI feature in automotive systems.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: Explainability as a pivotal attribute","","","All properties are important, but explainability is critical. From a safety engineering perspective, explainability is a pivotal attribute in supporting an AI item’s understandability, verifiability, and auditability. And from a trustworthiness perspective, it is foundational to support the “right to obtain an explanation” and support legal liability analyses providing explainability information for different actors (e.g., engineer, lawyer).","This directly addresses the opacity/explainability characteristic from the research focus, discussing explainability as a critical property for AI systems in high-risk industries, supporting safety engineering and trustworthiness perspectives.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","Category: opacity | Feature: explainability for different actors","","","Finally, trustworthiness leads us to multiple, multidimensional and multidisciplinary future research directions combining engineering, law and ethics disciplines, among others. For example, engineering and machine ethics, liability considerations, explainability for different actors, analysis of human vs. autonomous system behaviors.","This represents an AI feature related to opacity/explainability, as it explicitly mentions 'explainability for different actors,' which addresses challenges in transparency and interpretability of AI systems, a key characteristic in high-risk industries.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","","Category: situational_awareness | Severity: 7","","the driver itself is responsible for driving the vehicle, monitoring the ADAS operation and taking vehicle control in a short time if the ADAS detects and notifies that can no longer provide the intended functionality","Severity Justification: The requirement for rapid human intervention in response to AI system failure suggests high cognitive demands and risk of degraded performance under time pressure. | Relevance Justification: Directly addresses human performance in monitoring and responding to AI system failures, which relates to situational awareness and cognitive load in human-AI interaction.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","","Category: cognitive_overload | Severity: 7","","and should also consider human cognitive limitations (e.g., cognitive overload, oversight and reaction time limitations)[ 210, 263].","Severity Justification: Cognitive overload and oversight are significant degradations that can impair decision-making and safety in high-risk environments. | Relevance Justification: Directly addresses cognitive limitations related to AI safety systems, aligning with the research focus on human-AI interaction and performance issues.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","","","AI Feature: black box nature of AI solutions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: limitations regarding explainability and analyzability, and not recommended for use in safety-critical systems","The most notorious include the “black box” nature of AI solutions causing limitations regarding their explainability and analyzability [3,51,104,235,268,282], and compliance limitations concerning software development lifecycle phases, such as specification correctness and completeness, design, testing, verification, and validation [ 107,164,190,194,200,207,219,268,278]. Due to these limitations (challenges), AI techniques have not been recommended for use in safety-critical systems [ 56,120,200].","Causal Strength Justification: Direct causation is explicitly stated with 'causing' and 'Due to these limitations', linking AI features to specific effects without inference. | Relevance Justification: Directly addresses how opacity (black box nature) causes limitations in explainability and analyzability, which are critical for human trust and performance in high-risk industries, aligning with the research focus on opacity/lack of explainability.","y"
"Artificial Intelligence for Safety-Critical Systems in Industrial and Transportation Domains: A Survey","","","AI Feature: perception ML algorithm training | Evidence Type: direct | Causal Strength: 8 | Performance Effect: safety-critical systems could fail even in the absence of an electronic/software failure","tems, such as Advanced Driver-Assistance Systems (ADAS) [190], led to a novel scenario where safety-critical systems could fail even in the absence of an electronic/software failure. For example, the intended safety function fails due to unexpected operating conditions not considered in the perception ML algorithm training [162]. Thus, there was a need for a novel type of safety standards, complementary with FuSa standards, such as the automotive domain SOTIF [133].","Causal Strength Justification: Direct causal language 'led to' and 'fails due to' explicitly links AI features (ADAS and ML algorithm training) to system failures, indicating a strong, explicit cause-effect relationship. | Relevance Justification: The excerpt directly addresses how AI features (ADAS and ML algorithms) cause system failures, which relates to human performance in safety-critical contexts, though it does not explicitly mention human performance effects like trust or cognitive load.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Operator Support Automation","","","computerized operator support systems that assess various alarms and provide fault diagnoses to operators [26, 27, 28, 29], computer-based procedure systems that provide necessary data and procedures to assist operators by identifying tasks in real-time to foster safety goal achievement [30, 31, 32]","This represents conventional automation features in high-risk industries, specifically operator support systems that provide diagnoses and real-time task identification, which aligns with the research focus on examining AI vs. conventional automation characteristics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automation levels for autonomous vehicles","","","Motivated by the pursuit of self-driving vehicles, the Society of Automotive Engineers (SAE) has led the most recent, prominent, and industry-relevant efforts in regard to defining levels of automation. Per these efforts, Table 3 reflects six levels of automation, ranging from purely manual (i.e., human driver) control to a fully automated, human-out-of-the-loop vehicle [37]. We highlight the progression from Level 3 to Level 4, at which point we observe a tipping point in automation, such that vehicles can be manufactured as autonomously operated machines, the classical","This excerpt directly references AI/autonomous system characteristics through the discussion of self-driving vehicles and their defined levels of automation, which is a key aspect of novel AI systems compared to conventional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Novel six-level automation approach","","","In Section 3, we use concepts from the latter (e.g., sustained operational and tactical control and fallback) and build upon the current nuclear industry automation levels in order to propose a novel six-level approach to automating reactor operations.","This excerpt mentions 'sustained operational and tactical control and fallback' and a 'novel six-level approach to automating reactor operations,' which are specific automation features and frameworks relevant to the research focus on AI/autonomous system characteristics in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Novel automation concepts integration","","","This article aims to build upon the existing language of NUREG-0700 by introducing and integrating novel automation concepts borrowed from other non-nuclear safety-critical industries.","This excerpt highlights the introduction of 'novel automation concepts' from other industries, which aligns with the research focus on examining new AI/autonomous system features compared to conventional automation, though it does not specify the exact features.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Fully autonomous human-out-of-the-loop operations","","","This is particularly applicable in the case of fully autonomous human-out-of-the-loop-based operations (a key facet of the “unattended” attribute adopted in the FB Initiative [21]). Revising the levels of automation should be approached from two perspectives: (1) the current human factors engineering concepts (ultimate safety, resilience, performance, and capacity for root-cause analysis), and (2) the long-term economic and commercial outcomes/benefits enabled by modern automation technology.","This excerpt directly mentions 'fully autonomous human-out-of-the-loop-based operations,' which is a key AI/autonomous system characteristic involving high levels of automation without human intervention, relevant to the research focus on novel AI features in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automation tipping point at Level 4","","","The automation tipping point occurs at Level 4, with the automated flight system becoming able to sufficiently control operational and tactical tasks, such that a human pilot is no longer required.","This excerpt highlights a key AI/automation feature: the ability of an automated system to independently manage operational and tactical tasks, reducing or eliminating the need for human oversight, which aligns with advanced automation capabilities in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Automated fallback and hazard response","","","Level3: The system recognizes a crash scene and requests that the driver resume control and provide fallback (e.g., engage hazard lights and enter the unobstructed shoulder lane). Level4/5: Even if the driver is unresponsive to the fallback request, the automated system is able to achieve the minimal risk condition or circumnavigate the hazard.","This demonstrates AI capabilities in context-aware hazard recognition and adaptive response, including autonomous decision-making to achieve minimal risk conditions without human intervention, which is a key feature of advanced AI systems in high-risk industries like transportation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automated driving (Levels 3-5)","","","The term automated driving is reserved for automation Levels 3–5, and refers to the collective system (software and hardware) that affords the capability to perform DDTs on a sustained basis.","This text describes automated driving as involving software and hardware for sustained capability, which is a key AI/autonomous feature in high-risk industries, emphasizing the integration and sustained operation aspects relevant to novel AI characteristics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Driving automation systems (sustained basis)","","","Driving automation systems refer to any Level 1–5 system that performs all or part of the DDT on a sustained basis (e.g., adaptive cruise control and/or lane centering).","This text specifies that driving automation systems operate on a sustained basis, which is a core feature of AI/autonomous systems in high-risk industries like transportation, contrasting with momentary interventions of conventional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Active safety systems (non-automation)","","","Active safety systems such as electronic stability control, automatic emergency braking, and airbag deployment are excluded from automation, as they do not perform driving tasks on a sustained basis, but rather provide momentary intervention during hazardous situations.","This text distinguishes active safety features from automation, highlighting a key characteristic of conventional automation (momentary intervention) versus sustained AI/autonomous operation, which aligns with examining novel AI features versus conventional automation in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automated detection and flight control","","","AutomationPilot-in-command performs object and event detection and response, while the flight automation system provides lateral, longitudinal, and vertical control in limited operational domains.","This excerpt describes automation system capabilities including object/event detection and response, and flight control functions, which represent conventional automation features rather than novel AI characteristics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automated flight system with fallback","","","AutomationAutomated flight system (AFS) performs object and event detection and response, as well as sustained flight tasks in specific and limited operational domains. Fallback-ready pilot required.","This excerpt describes automated flight system capabilities including object/event detection and response, sustained flight tasks, and the requirement for fallback-ready pilots, which represent conventional automation features rather than novel AI characteristics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: adaptation | Feature: Adaptive Automation with task-specific levels","","","Adaptive Automation : With the exception of Levels 4 and 5 (addressed below), it is important to establish that the levels shown in Table 5 are not prescriptive for an entire reactor system, but rather a particular task or set of tasks; different tasks may be suited to different automation levels.","This represents an AI feature as it describes adaptive automation, which involves dynamic, context-aware behavior where automation levels adjust based on specific tasks, aligning with novel AI characteristics like context-aware/adaptive behavior in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automated system with operator oversight","","","ExceptionAutomated reactor operation system (AROS) performs sustained tactical and operational tasks in specific and limited operational domains. Upon request, an operator must approve tactical and operational decisions and provide fallback.","This text explicitly mentions an automated system (AROS) performing tasks, which relates to automation levels and control mechanisms in human-AI interaction, though it does not specifically address the novel AI characteristics like non-deterministic behavior, opacity, or context-adaptiveness.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Autonomous accident processing capabilities","","","the AROS has additional capabilities enabling it to process DBAs/BDBAs and perform the necessary operational and tactical processes in addition to providing fallback.","This describes an AI system (AROS) with advanced autonomous capabilities to process complex accident scenarios (DBAs/BDBAs) and execute operational and tactical processes, which aligns with novel AI characteristics in high-risk industries like handling dynamic, high-stakes situations without direct human control.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Autonomous fallback and risk management","","","However, at Levels 4 and 5, we observe a departure from this standard, as the responsibility for fallback and obtaining minimal risk conditions shifts from the human operator staff to the AROS.","This represents an AI feature as it highlights the system's ability to autonomously handle critical safety functions (fallback and minimal risk conditions) at higher automation levels, contrasting with human-dependent conventional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Autonomous decision-making and execution","","","Scenario 3: The reactor supervisor is unresponsive; AROS creates and executes the optimal plan of action.","This excerpt shows the AI system (AROS) making and executing decisions independently in the absence of human oversight, a key feature distinguishing AI from conventional automation that typically requires explicit human commands or pre-programmed responses.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: lack of adaptation and predictive behavior","","","this technology requires near-constant supervision and frequent guidance, as these systems are not typically designed to predict and adapt to changing scenarios. Rather, when receiving operational data from a sensor or sensor network, they react based on their prescribed logic.","This excerpt highlights a key limitation of conventional automation (non-AI) systems: they are not designed to predict or adapt to changing scenarios, contrasting with context-aware/adaptive AI features. It emphasizes their reliance on prescribed logic and need for human oversight, which aligns with the research focus on distinguishing AI from traditional automation in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: AI-based optimal decision generation","","","control systems can utilize real-time sensing data so as to provide (1) predictive “online” modeling and simulation (M&S) as well as (2) optimal decisions generated via artificial intelligence (AI)-based technology (e.g., reinforcement learning).","This excerpt directly mentions AI-based technology (specifically reinforcement learning) generating optimal decisions, which is a novel AI capability compared to conventional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: opacity | Feature: Transparent and explainable decision process","","","Transparent, explainable, and trustworthy decision process for the operations staff, reactor supervisor, and regulator (could be onsite or at a remote location).","This directly corresponds to the research focus on opacity/lack of explainability (black-box behavior → trust/regulation challenges), as it explicitly mentions transparency and explainability as desired characteristics for the decision process.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Self-awareness for diagnostics","","","Additionally, self-awareness will be critical to enable recognition of improper operation and to issue diagnostics so as to provide relevant information for determining the ultimate efficacy of a sensor in relation to an automation system.","Self-awareness is a novel AI characteristic that enables systems to monitor their own state and detect issues, which is distinct from conventional automation that typically lacks such introspective capabilities.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: context_adaptive | Feature: Real-time autonomous adaptation","","","Adjusting in real-time to changing parameters such as boundary/initial conditions, material properties, source terms, and geometry—without input from a user—reflects an approach that fundamentally differs from traditional M&S in the nuclear industry.","This excerpt directly describes context-aware/adaptive behavior where the system adjusts to dynamic environmental changes (changing parameters) in real-time without human intervention, which is a key characteristic of novel AI/autonomous systems versus conventional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Advanced automation sensor technologies","","","In the context of advanced automation and developing the AROS, the fundamental role of sensors (i.e., to provide real-time data) is unchanged. Sensor data will still be used to obtain relevant information for general situational awareness, plant health conditions, and the operational status of reactor components. Due to the expected characteristics of next-generation reactor systems (i.e., shrinking core sizes [SMRs, MRs, and FBs], ease of installation [SMRs, MRs, and FBs], and portability [FBs]), installation of a multitude of penetrating sensors and associated data processing hardware is expected to be infeasible. Therefore, sensor modalities—the configuration, location, and number of sensors—are expected to change. Rather than detail possible sensor architecture solutions and configurations, we instead discuss the anticipated ideal characteristics of enabling sensor technologies for advanced automation (i.e., Levels 3, 4, and 5).","This represents an AI feature as it explicitly mentions 'advanced automation' and 'Levels 3, 4, and 5', which relate to AI capabilities in high-risk industries, though it does not directly address non-deterministic, opacity, or context-adaptive characteristics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: non_deterministic | Feature: Uncertainty Propagation in Online Models","","","First, online models must account for and properly propagate uncertainty; the sensitivity to this uncertainty must also be well understood. In addition to traditional uncertainties in nuclear data and numerical techniques, online calculations introduce uncertainty through real-time sensor data as well as changes in material properties and geometry.","This represents a non-deterministic AI feature because it highlights variable outputs and unpredictable failures due to uncertainty in data-driven decision-making, as online models depend on real-time data and dynamic conditions, leading to inherent unpredictability.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: High-level automation with human-machine interface","","","At Levels 4 and 5, the interaction between the reactor supervisor and the AROS must be accounted for. Here, the AROS is required to maintain operational control of the reactor system at all times in the absence of a traditional reactor operations staff. The role of the reactor supervisor is to provide tactical and strategic commands to the AROS as necessary. Effective, reliable, and resilient design of this human-machine interface is essential to ensure proper and safe reactor operation at all times.","This represents an AI feature as it discusses automation levels (4 and 5) where an autonomous system (AROS) maintains operational control, highlighting advanced AI capabilities in high-risk industries like power, with a focus on human-AI interaction and interface design for reliability and safety.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Sustained tactical and operational task performance with awareness and adaptation","","","The defining characteristic of an AROS—as well as what effectively enables automation Level 3—is the automation system’s capability to perform sustained tactical and operational tasks. Though potentially limited to a specific operational domain, sustained operations inherently involve maintaining a keen awareness of interconnected components and their current and expected modes of operation. This includes (1) the ability to detect and diagnose abnormalities and modify operations plans by applying real-time information, and (2) a general self-awareness to anomalies and other external factors, as well as the ability to react based on understanding the system state, the resources available, and what responses are appropriate.","This excerpt directly describes advanced AI/autonomous system capabilities (AROS) that enable higher automation levels (Level 3), including context-aware behavior (maintaining awareness of interconnected components and modes), adaptive response (detecting/diagnosing abnormalities, modifying plans with real-time information, reacting to anomalies), and self-awareness—key novel characteristics distinguishing AI from conventional automation in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: opacity | Feature: Explainability for Trustworthiness","","","General trustworthiness. Achieving general trustworthiness of AI is the focus of explanatory or explainable AI. In this context, explainability accounts for algorithmic fairness, potential identification of data biases, algorithms that perform as expected, and explanations rationalizing the decisions made [57].","This represents an AI feature related to opacity/explainability, as it explicitly mentions explainable AI, algorithmic fairness, and rationalizing decisions, which are key characteristics for addressing black-box behavior and trust challenges in AI systems.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Digital Twin dynamic decision-making","","","we define a DT as a collection of information constructs that mimic the structure, context, and behavior of a unique physical asset, which are dynamically updated with information from the physical asset and, in turn, provide decisions of value","This describes an AI/autonomous system capability where digital constructs mimic and adapt to physical assets in real-time to provide decisions, aligning with context-aware/adaptive behavior and data-driven decision-making in high-risk industries","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: Real-time data integration with predictive modeling","","","These heterogeneous data will require real-time incorporation with potentially sensitive, fidelity-appropriate, physics-based predictive M&S. Considerations will also need to be made in accounting for (1) the lack of spatio-temporal uniformity among the varying data streams and M&S approaches, and (2) the biases introduced to offset such non-uniformities (and how these biases change for different components, systems, and operational domains).","This represents an AI capability because it involves real-time processing of complex, heterogeneous data streams and integration with predictive modeling and simulation (M&S), which are characteristic of advanced AI systems that handle dynamic, multi-modal inputs to support decision-making in high-risk environments.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Integration of critical components for automation progression","","","Progressing to automation Level 3 requires integration of design-appropriate critical components, each of which support safe, sustained reactor operations.","This excerpt focuses on automation levels and the integration of components to support safe operations, which relates to AI system features in safety-critical environments like nuclear reactors.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Component-level digital twins for operator assistance","","","At automation Levels 1 and 2, component-level DTs may be used to provide general operator staff assistance through online health monitoring and diagnostics/prognostics.","This excerpt discusses automation levels and AI capabilities (digital twins) that provide operator assistance through monitoring and diagnostics, which relates to AI system features in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Simplicity in lower automation levels via empirical models","","","Relative to higher automation levels, simplicity is attained through decreased data heterogeneity, empirical relationships for predictive M&S (e.g., pump curves and other empirical relationships), and fewer and less complex possible operational states and control configurations.","This excerpt highlights automation characteristics such as data heterogeneity and empirical relationships for predictive modeling, which are relevant to AI system features in automation contexts.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: System-wide digital twins at higher automation levels","","","At automation Levels 3 and higher, DT technology is expected to become “system-wide” and pose significant challenges.","This excerpt discusses automation levels and the evolution of AI technology (digital twins) to system-wide applications with associated challenges, which is a key AI feature in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automation tipping point and paradigm shift","","","Crossing the proposed automation tipping point will be particularly challenging, as this is where traditional reactor control paradigms shift from human- to machine-centric, and may lead to an evolution in how control room designs, operations procedures, and operator training/education are measured.","This excerpt directly addresses a key aspect of automation levels and the transition from human to machine control, which is a core theme in human-AI interaction, particularly in high-risk industries like nuclear power. It highlights the challenges and potential evolution in system design and human roles as automation increases.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: interface | Feature: interface development for human-AI interaction","","","appropriate interfaces must still be developed (e.g., physical controls and/or natural language processing).","This represents an AI feature as it discusses interface types, which are essential for enabling effective human-AI interaction and control in automated systems.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: AI_capability | Feature: trust and understanding of information","","","ensure that operations staff understand and trust the information/context being provided [65, 66].","This represents an AI feature as it addresses trust and understanding, which are critical for AI systems, especially in high-risk industries, linking to opacity/explainability concerns.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: adaptation | Feature: adaptive automation","","","the concept of adaptive automation will strongly impact the areas of human-machine interaction and general human factors.","This represents an AI feature as it describes adaptive automation, which aligns with context-aware/adaptive behavior in AI systems, involving dynamic responses to environments.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Automation levels with AROS integration","","","Utilizing key concepts and definitions posed by non-nuclear safety-critical industries, we developed a six-level approach to the automation of reactor operations, ranging from no automation (Level 0) to the utilization of an AROS to provide sustained operational and tactical control under the supervision of a reactor supervisor (Level 5).","This excerpt details a structured automation approach, highlighting the integration of an AROS (likely an AI or autonomous system) for operational control, which aligns with AI capabilities in high-risk industries by enabling advanced automation beyond conventional systems.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","Category: automation | Feature: Radical operational changes from automation","","","High levels of automation enable radical changes to reactor operations, affording new opportunities in terms of both economics and safety.","This excerpt emphasizes the transformative impact of high automation levels, which is a key characteristic of novel AI systems compared to conventional automation, as they can drive significant improvements in efficiency and safety in high-risk settings.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","Category: behavioral_changes | Severity: 4","","This results in operations staff being consistently engaged to provide fallback and operational, tactical, and strategic commands.","Severity Justification: The text implies a sustained demand on human operators to compensate for automation limitations, which could lead to fatigue or workload issues over time, but it does not explicitly describe severe degradation like errors or skill loss. | Relevance Justification: The text indirectly relates to human performance by highlighting the need for constant human engagement due to automation's lack of adaptability, which aligns with behavioral changes in human-AI interaction, though it is not explicitly about novel AI degradations vs. traditional automation.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","Category: cognitive_bias | Severity: 7","","targeting the imperfections that stem from human error and our various cognitive biases [48].","Severity Justification: Human error and cognitive biases are fundamental and persistent issues in high-risk industries, directly impacting safety and performance, though the text frames them as targets for mitigation rather than describing their degradation. | Relevance Justification: This directly references human performance problems (human error and cognitive biases) in the context of AI/automation systems, aligning with the research focus on novel AI characteristics vs. traditional automation, though it does not describe degradation per se but rather existing imperfections.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","Category: performance_metrics | Severity: 6","","considerations must be made to account for potentially lackluster operator performance during mostly automated operations, weigh the trade-offs between possible operational errors corresponding to limited autonomy and human-provided fallback, and ensure that operations staff understand and trust the information/context being provided [65, 66].","Severity Justification: The degradation is described as 'potentially lackluster operator performance,' which suggests a moderate but significant impact on performance that requires specific considerations to address. | Relevance Justification: This directly addresses human performance degradation in the context of automation, specifically mentioning operator performance issues during automated operations, which aligns with the research focus on human-AI interaction in high-risk industries.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","Category: behavioral_changes | Severity: 5","","Crossing the proposed automation tipping point will be particularly challenging, as this is where traditional reactor control paradigms shift from human- to machine-centric, and may lead to an evolution in how control room designs, operations procedures, and operator training/education are measured.","Severity Justification: The text implies a significant shift in control paradigms that could affect human roles and performance, but does not explicitly detail specific degradations. | Relevance Justification: It relates to human performance in the context of automation changes, though it focuses on broader operational evolution rather than direct degradation metrics.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","","AI Feature: programmable-logic-based control | Evidence Type: direct | Causal Strength: 8 | Performance Effect: operations staff being consistently engaged to provide fallback and operational, tactical, and strategic commands","This results in operations staff being consistently engaged to provide fallback and operational, tactical, and strategic commands.","Causal Strength Justification: Direct causation is explicitly stated with 'This results in', linking the technology's characteristics to a specific human performance outcome. | Relevance Justification: The causal link is directly extracted from the chunk and relates to human performance in high-risk industries, though it does not explicitly mention the novel AI features (non-deterministic, opacity, adaptive) specified in the research focus.","y"
"Automation Levels for Nuclear Reactor Operations: A Revised
Perspective","","","AI Feature: automation tipping point | Evidence Type: direct | Causal Strength: 7 | Performance Effect: evolution in control room designs, operations procedures, and operator training/education","Crossing the proposed automation tipping point will be particularly challenging, as this is where traditional reactor control paradigms shift from human- to machine-centric, and may lead to an evolution in how control room designs, operations procedures, and operator training/education are measured.","Causal Strength Justification: Direct causal language 'may lead to' is used, explicitly linking the automation shift to changes in human performance elements, though it is speculative ('may') rather than definitive. | Relevance Justification: Directly addresses how automation (a key AI/autonomous system characteristic) affects human performance in high-risk industries, aligning with the research focus on novel AI features vs. conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-enhanced energy distribution","","","For instance, smart grids leverage artificial intelligence to enhance energy distribution efficiency, reducing wastage and ensuring uninterrupted supply.","This describes a specific application of artificial intelligence in energy systems, demonstrating AI capabilities for operational improvement.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: Real-time data analysis and optimization","","","The integration of artificial intelligence and machine learning in energy management systems allows for real-time data analysis, optimizing energy production and consumption patterns.","This directly mentions artificial intelligence and machine learning capabilities for data-driven optimization, which is a core AI feature in the research focus.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: Unpredictable human-automation interactions and systemic failures","","","Additionally, the integration of safety mechanisms such as Safety Integrity Levels (SIL) has shown to reduce fatal risks in automated processes, yet these systems must contend with the unpredictability of human-automation interactions and the potential for systemic failures.","This excerpt highlights a key challenge in automation systems: the unpredictability arising from human-automation interactions and the risk of systemic failures, which relates to general automation characteristics in high-risk industries, though it does not specifically mention novel AI features like non-deterministic decision-making, opacity, or context-aware behavior.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: adaptive automation frameworks and SafetyOps for real-time safety interventions","","","automation systems, such as adaptive automation frameworks, have been designed to assist human operators in critical scenarios, thus reducing the likelihood of accidents arising from human oversight or fatigue (Inagaki & Itoh, 2007) [13]. Similarly, frameworks like SafetyOps have emerged, integrating advanced practices to ensure the safety lifecycle of autonomous systems, thereby mitigating risks through real-time interventions (Siddique, 2020) [32].","This represents an AI feature as it describes context-aware/adaptive behavior through adaptive automation frameworks and SafetyOps, which integrate advanced practices for dynamic environment response and real-time risk mitigation in autonomous systems, aligning with the research focus on context-aware/adaptive behavior in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: non_deterministic | Feature: data-driven decision-making","","","Workers face indirect risks associated with new technologies, such as data -driven safety solutions.","This directly references 'data-driven safety solutions' which aligns with the non-deterministic/data-driven AI characteristic where decisions are based on data/models rather than fixed rules.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: non_deterministic | Feature: data-dependent safety systems","","","Similarly, IoT -based solutions, like those implemented in safety helmets for construction sites, aim to proactively alert workers to potential dangers but rely heavily on the accuracy and timeliness of data.","This describes systems that 'rely heavily on the accuracy and timeliness of data' for decision-making, which is characteristic of data-driven AI approaches where outputs vary with data quality.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-powered monitoring systems","","","AI-powered monitoring systems play pivotal roles. This literature review provides an in-depth examination of these technologies, emphasizing their applications and contributions to advancing energy industries.","This represents an AI feature as it explicitly refers to AI-powered systems, indicating advanced capabilities beyond conventional automation, though it does not detail specific novel characteristics like non-determinism or opacity.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: Cybersecurity Vulnerabilities","","","However, these advancements also bring about concerns related to cybersecurity and the vulnerability of automated systems to external threats, underscoring the need for robust intrusion detection mechanisms (Schuster & Paul, 2012) [30].","This excerpt discusses vulnerabilities and cybersecurity concerns in automated systems, which relates to general AI capabilities and risks, though it does not explicitly mention novel AI characteristics like non-deterministic or adaptive behavior, focusing more on security aspects of automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: Dynamic Adaptation and Learning Integration","","","Moreover, as automation systems become increasingly sophisticated, new frameworks are being developed to dynamically adapt to evolving operational environments. The integration of learning and real-time monitoring systems has proven effective in managing safety risks, more proactive approach to addressing potential failures (McDermid, Jia & Habli, 2019) [25].","This excerpt directly mentions 'dynamically adapt to evolving operational environments' and 'integration of learning and real-time monitoring systems,' which aligns with the context-aware/adaptive behavior characteristic of novel AI systems, involving dynamic response and real-time learning for safety management.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: Risk Mitigation and Human-Machine Synergy","","","While automation presents remarkable opportunities for enhancing safety in various domains, its adoption must be accompanied by a thorough understanding of its limitations and risks. By leveraging advanced technologies, adhering to stringent safety standards, and fostering human-machine synergy, the potential risks associated with automation can be mitigated.","This excerpt addresses the adoption of automation with a focus on understanding limitations and risks, and leveraging advanced technologies and human-machine synergy, which relates to general AI capabilities and interface types, though it does not specify novel AI features like opacity or non-deterministic behavior.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-driven predictive analytics","","","AI-driven analytics support grid management by predicting energy demand and supply fluctuations, ensuring stability in power distribution networks (Smith, 2016; Kryukov, 2016) [33, 20].","This directly mentions AI-driven analytics performing predictive functions (predicting energy demand and supply fluctuations), which is a core AI capability involving data-driven decision-making and modeling, aligning with the research focus on AI/autonomous system characteristics in high-risk industries like power.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: Machine learning-based monitoring and optimization","","","AI-powered monitoring systems are at the forefront of automation technologies in the energy industry. These systems leverage machine learning algorithms to analyze vast datasets, predict failures, and optimize processes.","This excerpt directly describes AI capabilities (machine learning algorithms) that analyze data, predict failures, and optimize processes, which are characteristic of AI systems versus conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: Sector-specific AI optimization","","","In the oil and gas sector, AI facilitates reservoir management, drilling optimization, and environmental impact assessments. For renewable energy, AI algorithms optimize energy storage and distribution, balancing supply and demand while reducing waste.","This excerpt highlights AI's role in facilitating and optimizing complex processes across different energy sectors, demonstrating its application-specific capabilities beyond conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: Unpredictable performance and failure in automated systems","","","Over-reliance on technology represents a significant risk, as increased dependence on automated systems can lead to vulnerabilities when these systems fail or perform unpredictably.","This excerpt directly addresses a key characteristic of automated systems—their potential for unpredictable performance and failure—which is a critical consideration in high-risk industries, aligning with the research focus on novel AI/autonomous system features versus conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: AI and robotics introduction","","","The introduction of AI and robotics in the energy sector reduces the demand for certain roles, leading to economic and social implications for affected workers.","This excerpt explicitly mentions the introduction of AI and robotics, which are key AI system features, and describes their impact on workforce roles, highlighting automation capabilities in a high-risk industry context.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: automation risks","","","While the benefits of automation for safety are well-documented, it is essential to consider the challenges and limitations associated with its implementation. Automation can introduce new risks, such as over-reliance on systems or the potential for technical failures.","This excerpt highlights risks associated with automation, such as over-reliance and technical failures, which are relevant to the research focus on comparing AI/autonomous systems with conventional automation in high-risk industries, though it does not specify novel AI characteristics.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: autonomous alerts","","","Moreover, smart technologies, such as the SAFE helmet, exemplify how integrated sensors and autonomous alerts can proactively enhance worker protection in industrial settings (Altamura et al., 2019) [1].","This excerpt mentions 'autonomous alerts,' which is a feature of AI/autonomous systems that can operate independently to provide warnings or notifications, aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI integration in safety concepts","","","The evolving relationship between humans and machines necessitates novel safety concepts that integrate advanced technologies such as IoT, AI, and ICT, promoting harmonious interactions within industrial environments (Kanamaru & Ogihara, 2019) [16].","This excerpt directly mentions AI as an advanced technology being integrated into novel safety concepts for human-machine collaboration, which relates to AI capabilities in industrial environments.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: Real-time adaptive behavior based on human cognitive states","","","Advanced human -machine interfaces are also transforming the landscape of occupational safety. Real -time monitoring systems that measure human cognitive states, such as EEG -based systems, enable machines to adapt their behavior based on the operator’s mental workload, thereby enhancing safety in energy operations.","It represents a context-aware/adaptive AI feature, as the systems dynamically respond to environmental inputs (human cognitive states) to adjust behavior, aligning with novel AI characteristics like dynamic environment response and variability handling.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: adaptive control measures","","","By integrating real-time safety monitoring and adaptive control measures, systems can address potential hazards proactively, sustaining safety across all operational components (Bi et al., 2021).","This excerpt mentions 'adaptive control measures' and 'real-time safety monitoring', indicating systems that dynamically respond to environmental changes, aligning with context-aware/adaptive behavior in AI, such as dynamic environment response and real-time learning.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: adaptation | Feature: Adaptive safety protocols and integrative technologies","","","Human -machine collaboration is indispensable in balancing the benefits of automation with the necessity of human oversight in energy operations. Through advanced interfaces, adaptive safety protocols, and integrative technologies, this collaboration fosters a safer and more adaptable operational environment.","This excerpt mentions 'adaptive safety protocols' and 'integrative technologies' which represent AI/autonomous system features that enable dynamic response and environment adaptation, aligning with the context-aware/adaptive behavior characteristic of novel AI systems versus conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: complexity and risk management in automation","","","Automation in environments like oil and gas or renewable energy must account for a wide range of factors, including the reliability of sensors, the potential for system failure, and the possibility of unforeseen interactions between automated components.","This excerpt describes automation features involving system reliability, failure potential, and component interactions, which relate to AI/autonomous system characteristics like non-deterministic behavior and context-aware adaptation in high-risk environments, though it does not explicitly mention AI-specific traits.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: automation | Feature: rapid abnormality detection and response","","","Automation can also drastically improve incident response times. Automated systems can detect abnormalities more rapidly than human operators, allowing for faster responses to emerging issues, such as equipment malfunctions or hazardous conditions.","This describes a capability of automated systems (a form of AI/autonomous technology) to enhance operational efficiency and safety through faster detection and response, which aligns with general AI capabilities in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-driven safety analytics","","","AI-driven safety analytics represents a transformative development in understanding and preventing workplace accidents. Advanced systems analyze historical safety data to identify patterns and predict potential risks before they manifest.","This excerpt describes an AI capability (AI-driven safety analytics) that analyzes historical data to identify patterns and predict risks, which is a data-driven approach characteristic of AI systems versus conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-driven safety analytics and machine learning for risk prediction","","","Emerging trends in automation and worker safety are poised to revolutionize industry practices through advanced technologies such as AI -driven safety analytics, enhanced machine learning algorithms for risk prediction, and real -time safety monitoring facilitated by IoT integration.","This excerpt directly mentions AI-driven safety analytics and enhanced machine learning algorithms for risk prediction, which are specific AI capabilities that enable data-driven decision-making and predictive analytics, distinguishing them from conventional automation by leveraging advanced computational models to analyze safety data and predict risks.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: adaptation | Feature: Dynamic adaptation of safety rules and action plans","","","Ishigooka et al. (2022) [14] introduce the concept of “Symbiotic Safety,” where action plans and safety rules dynamically adapt to optimize collaboration between humans and machines. This approach minimizes risks while maintaining operational , particularly in autonomous systems, where automated processes must comply with stringent safety requirements.","This represents a context-aware/adaptive AI feature where safety rules and action plans dynamically change to optimize human-machine collaboration, showing adaptation to collaboration needs rather than static automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: Adaptive risk prediction","","","Enhanced machine learning algorithms further elevate risk prediction capabilities by adapting to dynamic workplace conditions.","This directly describes AI (machine learning algorithms) exhibiting context-aware/adaptive behavior by adapting to dynamic workplace conditions, which is a novel characteristic compared to static conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: Machine learning for safety compliance","","","Such systems exemplify how machine learning can contribute to maintaining compliance with safety protocols while fostering a culture of accountability.","This excerpt directly mentions machine learning as a feature contributing to safety protocols and accountability, which is a key AI capability in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: context_adaptive | Feature: Real-time monitoring and adaptive responses","","","the adoption of IoT devices and AI -based system s enables real - time monitoring and adaptive responses to dynamic industrial conditions.","This excerpt mentions AI-based systems enabling adaptive responses to dynamic conditions, which aligns with the context-aware/adaptive behavior characteristic of novel AI systems, involving dynamic environment response and real-time adaptation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: Real-time decision-making capability","","","By integrating these methodologies into comprehensive safety platforms, industries can achieve real-time decision-making and proactive risk management.","This represents an AI feature because it highlights the capability for dynamic, data-driven decision-making in real-time, which is a characteristic of advanced AI systems compared to conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: adaptation | Feature: Adaptation to evolving challenges","","","By adopting AI-driven analytics, refining machine learning algorithms, and embracing IoT-enabled systems, industries can foster safer working environments and adapt to evolving safety challenges.","This represents an AI feature because it explicitly describes the use of AI-driven analytics and machine learning to enable adaptation to dynamic and evolving safety challenges, which is a key characteristic of context-aware and adaptive AI systems.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","Category: AI_capability | Feature: AI-driven safety analytics and machine learning","","","Emerging trends in automation, such as AI-driven safety analytics, enhanced machine learning algorithms, and IoT-based real-time monitoring systems, highlight the potential for transformative advancements in worker safety. These technologies enable predictive insights, adaptive responses, and dynamic risk management, significantly reducing workplace hazards.","This excerpt directly mentions AI-driven safety analytics and enhanced machine learning algorithms, which are specific AI capabilities that enable predictive insights and adaptive responses, distinguishing them from conventional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: behavioral_changes | Severity: 7","","One critical issue is the phenomenon of risk compensation, where individuals may engage in riskier behavior due to over-reliance on automation, potentially offsetting its intended safety benefits (Itoh, Sakami & Tanaka, 2000) [15].","Severity Justification: Risk compensation directly leads to riskier behavior, which can compromise safety in high-risk industries, making it a significant performance degradation. | Relevance Justification: This directly addresses human performance degradation in automation contexts, specifically behavioral changes due to over-reliance, which aligns with the research focus on human-AI interaction and performance issues.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: behavioral_changes | Severity: 6","","However, this shift brings with it psychosocial stresses due to changing roles and responsibilities, particularly in Industry 4.0.","Severity Justification: Psychosocial stresses due to changing roles and responsibilities can moderately impact human performance by affecting mental well-being and adaptation, but the text does not specify severe outcomes like errors or failures. | Relevance Justification: This directly relates to human performance degradation in the context of automation and Industry 4.0, aligning with the research focus on human-AI interaction challenges, though it is not explicitly tied to novel AI characteristics.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: behavioral_changes | Severity: 6","","Worke r adaptability remains a critical concern. Robotic systems reduce traditional risks but also introduce novel threats, requiring workers to adapt rapidly to unfamiliar technologies.","Severity Justification: The text highlights adaptability as a critical concern and novel threats, suggesting moderate severity due to the need for rapid adaptation to unfamiliar technologies, which could lead to errors or inefficiencies. | Relevance Justification: This is highly relevant to human performance degradation as it directly addresses worker adaptability issues in response to novel threats from robotic systems, aligning with the focus on novel AI-related degradations vs traditional automation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: behavioral_changes | Severity: 7","","Over-reliance on technology represents a significant risk, as increased dependence on automated systems can lead to vulnerabilities when these systems fail or perform unpredictably. Automation, while minimizing human intervention, may lead to oversight errors or system failures.","Severity Justification: The text highlights significant risks from over-reliance and increased dependence on automated systems, which can lead to vulnerabilities and oversight errors, indicating moderate to high severity in performance degradation. | Relevance Justification: This directly relates to human performance degradation in the context of novel AI/autonomous systems, as it addresses over-reliance and dependence on technology leading to vulnerabilities and errors, aligning with the research focus on non-deterministic decision-making and adaptive behavior.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: skill_degradation | Severity: 7","","Without adequate training, the workforce may struggle to adapt, which could compromise both safety and productivity.","Severity Justification: The text explicitly links lack of training to compromised safety and productivity, suggesting a direct impact on performance, so severity is moderately high. | Relevance Justification: This is highly relevant to skill/knowledge issues, as it addresses training deficiencies leading to adaptation struggles and performance compromises in automated systems.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: skill_degradation | Severity: 6","","Many workers may not have the technical expertise required to operate or maintain automated systems, which can lead to a dependency on a smaller, more specialized workforce.","Severity Justification: The issue is described as a significant barrier that can compromise safety and productivity, but it is framed as a skill gap rather than an explicit degradation, so severity is moderate. | Relevance Justification: This directly relates to skill/knowledge issues from the priority search, as it discusses workforce skill gaps affecting operation and maintenance of automated systems, though it is not explicitly novel AI-related.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","Category: behavioral_changes | Severity: 7","","over-reliance on automation can lead to misunderstandings and potentially hazardous situations.","Severity Justification: The degradation is described as leading to potentially hazardous situations, which implies a significant risk to safety and operational integrity, though it is presented as a potential outcome rather than a guaranteed severe consequence. | Relevance Justification: This directly addresses human performance degradation in the context of automation, specifically highlighting over-reliance as a cause of misunderstandings and hazards, which aligns with the research focus on human-AI interaction challenges in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation technologies (robotic drilling systems, autonomous underwater vehicles, digital twins) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reducing accidents caused by human error","These advancements enable precise operations in extreme conditions, thereby reducing accidents caused by human error.","Causal Strength Justification: Direct causation is explicitly stated with 'thereby reducing accidents caused by human error', linking automation advancements to a reduction in human error-related accidents. | Relevance Justification: The excerpt directly addresses how automation affects human performance by reducing accidents from human error, but it does not specify novel AI characteristics like non-deterministic, opacity, or adaptive behavior as requested in the task.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation (implied AI/autonomous systems from research focus) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: individuals engage in riskier behavior","One critical issue is the phenomenon of risk compensation, where individuals may engage in riskier behavior due to over-reliance on automation, potentially offsetting its intended safety benefits (Itoh, Sakami & Tanaka, 2000) [15].","Causal Strength Justification: Direct causal language 'due to' explicitly links over-reliance on automation to riskier behavior, with a clear mechanism described. | Relevance Justification: Directly addresses how automation (implied AI from research focus) affects human performance through risk compensation behavior, which is a key human performance issue in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: enhances worker safety","Automation significantly enhances worker safety by reducing exposure to hazardous environments, preventing repetitive stress injuries, and addressing fatigue-related risks.","Causal Strength Justification: Direct causation is indicated by 'by reducing', which explicitly links automation to safety enhancement through specific mechanisms. | Relevance Justification: The excerpt directly addresses human performance (worker safety) and uses causal language, but it does not specifically mention novel AI characteristics like non-deterministic, opacity, or adaptive features as requested in the task.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation | Evidence Type: direct | Causal Strength: 7 | Performance Effect: oversight errors or system failures","Automation, while minimizing human intervention, may lead to oversight errors or system failures.","Causal Strength Justification: Direct causation is indicated by 'may lead to', linking automation to oversight errors or system failures explicitly. | Relevance Justification: It directly connects automation to human performance issues (oversight errors) and system failures, relevant to AI features in high-risk contexts.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automated systems | Evidence Type: direct | Causal Strength: 8 | Performance Effect: vulnerabilities","Over-reliance on technology represents a significant risk, as increased dependence on automated systems can lead to vulnerabilities when these systems fail or perform unpredictably.","Causal Strength Justification: Direct causation is indicated by 'can lead to', linking increased dependence to vulnerabilities explicitly. | Relevance Justification: It addresses how reliance on technology (including AI/automation) affects outcomes, though not specifically human performance; it relates to risk in high-risk industries.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: AI and robotics introduction in the energy sector | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Reduced demand for certain roles leading to economic and social implications for affected workers","Workforce displacement poses another critical challenge associated with automation. The introduction of AI and robotics in the energy sector reduces the demand for certain roles, leading to economic and social implications for affected workers.","Causal Strength Justification: Direct causal language using 'reduces...leading to' explicitly connects AI introduction to workforce displacement effects | Relevance Justification: Directly addresses AI impact on workforce, though not specifically about human performance in operational contexts","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation | Evidence Type: direct | Causal Strength: 9 | Performance Effect: maintaining workplace safety","During the COVID -19 pandemic, automation played a crucial role in maintaining workplace safety by reducing employee exposure to potential contagion. Automated systems, including digitized and soft automation configurations, minimized human interaction in manufacturing environments, demonstrating their adaptability in addressing emergent risks (Heredia et al., 2022) [11].","Causal Strength Justification: Direct causation is explicitly stated with 'by reducing', linking automation to reduced exposure and maintained safety. | Relevance Justification: The excerpt directly connects an AI/automation feature (reducing exposure) to human safety performance, but it does not address novel AI characteristics like non-deterministic, opacity, or adaptive behavior beyond general adaptability mentioned.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: minimizing burnout and improving morale","Automated systems can operate continuously without performance degradation, significantly lowering the risk of incidents due to worker exhaustion. In manufacturing, for instance, automation ensures consistent production quality and reduces the physical and cognitive workload on employees, thereby minimizing burnout and improving morale ( Sedore, 2023 ).","Causal Strength Justification: Direct causation is explicitly stated with 'thereby', linking reduced workload to minimized burnout and improved morale. | Relevance Justification: The excerpt directly connects an AI/automation feature (reducing workload) to human performance effects (burnout and morale), though it does not specify novel AI characteristics like non-deterministic, opacity, or adaptive behavior.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: Real-time monitoring systems that measure human cognitive states, such as EEG-based systems | Evidence Type: direct | Causal Strength: 8 | Performance Effect: enhancing safety in energy operations","Real -time monitoring systems that measure human cognitive states, such as EEG -based systems, enable machines to adapt their behavior based on the operator’s mental workload, thereby enhancing safety in energy operations.","Causal Strength Justification: Direct causation is indicated by 'enable' and 'thereby enhancing', explicitly linking the AI feature to the safety outcome. | Relevance Justification: Directly relates to context-aware/adaptive behavior from the research focus, showing how AI adapts to human states to improve safety, though not explicitly tied to human performance degradation.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: integration of automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: new types of safety risks","Moreover, the integration of automation often leads to new types of safety risks.","Causal Strength Justification: Direct causal language 'leads to' explicitly connects cause and effect. | Relevance Justification: Directly addresses causal relationship between automation and safety risks, though not specifically tied to human performance or novel AI characteristics.","y"
"Automation and worker safety: Balancing risks and benefits in oil, gas, and renewable energy industries","","","AI Feature: over-reliance on automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: misunderstandings and potentially hazardous situations","over-reliance on automation can lead to misunderstandings and potentially hazardous situations.","Causal Strength Justification: Direct causal language ('can lead to') explicitly connects the cause (over-reliance) to the effects (misunderstandings, hazardous situations). | Relevance Justification: Directly addresses how an AI/automation characteristic (over-reliance) causally affects human performance (misunderstandings, hazardous situations), though it does not specify the exact novel AI features (non-deterministic, opacity, adaptive) from the research focus.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Capabilities of intelligent systems","","","it is expressly dangerous to over-rely and overestimate the capabilities of intelligent systems.","This directly references the capabilities of intelligent systems, which is a core AI feature relevant to the research focus on novel AI characteristics versus conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: automation | Feature: Smart automated systems and human roles","","","Smart automated systems contribute to the overall system performance and safety by automating routine tasks, identifying and anticipating hazards and failure events, and supporting operator awareness and decision-making in high-complexity troubleshooting situations; even so, the human still has the important task of monitoring possible automation failures, carrying out manual procedures if needed, and performing decision-making.","This excerpt details how smart automated systems automate tasks, identify hazards, and support decision-making, which are key features of AI and automation in high-risk industries, emphasizing the interaction between automation and human oversight.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI-driven automation benefits and drawbacks","","","AI-driven automation has certainly demonstrated its value by increasing productivity, reducing cost, and adding a new layer of safety; however, the rapid and widespread adoption of these intelligent systems without proper consideration of the ethical, societal, and safety implications has quickly brought to light some drawbacks.","This excerpt describes AI-driven automation as intelligent systems that increase productivity, reduce cost, and add safety, highlighting their capabilities and the implications of their adoption, which aligns with general AI features in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI-driven automation for performance and safety","","","While AI-driven automation can increase the performance and safety of systems, humans should not be replaced in safety-critical systems but should be integrated to collaborate and mitigate each other’s limitations.","This directly describes AI-driven automation as a feature that increases system performance and safety, which is a core AI capability compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI methods for collaborative intelligence","","","In this survey, we search and review recent work that employs AI methods for collaborative intelligence applications, specifically those that focus on safety and safety-critical industries.","This highlights AI methods as a feature enabling collaborative intelligence applications, which is a novel AI characteristic in safety-critical contexts.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI for human-machine interaction and safety","","","We aim to contribute to the research landscape and industry by compiling and analyzing a range of scenarios where AI can be used to achieve more efficient human–machine interactions, improved collaboration, coordination, and safety.","This describes AI as a feature that improves human-machine interactions, collaboration, coordination, and safety, which are key AI capabilities in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Self-evolving behavior","","","major concerns were raised regarding the risks of harm to the health, safety, or fundamental rights of people in their interaction with autonomous systems/machines, particularly those with self-evolving behavior, with close interaction with humans, or those used as a safety component or product.","This refers to the context-aware/adaptive behavior of AI systems, where 'self-evolving behavior' implies dynamic environment response, real-time learning, and variability handling that can lead to unpredictability.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Low model transparency and explainability","","","safety issues related to low model transparency and explainability, vulnerability to adversarial attacks and human bias [ 4], and system safety and robustness certification [ 5].","This directly addresses the opacity/explainability characteristic of AI systems, highlighting the 'black-box' nature that makes internal reasoning not visible and poses trust/regulation challenges.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: ML/AI implementation limitations in safety-critical systems","","","which can apply to hazards caused by the limitations of the implemented ML/AI in advanced driver-assistance systems.","This directly references ML/AI (machine learning/artificial intelligence) as implemented components in advanced driver-assistance systems, highlighting their role in safety-critical applications and the need to address their limitations through standards.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI as a new digital technology requiring regulatory updates","","","However, with the fast development and introduction to the market of CI technology, other current legislation and standards may require updating to deal with the emergent challenges of new digital technologies, such as the artificial intelligence, Internet of Things and robotics domains.","This excerpt explicitly identifies artificial intelligence as a new digital technology that presents emergent challenges, indicating it is a distinct feature or domain that differs from conventional systems and requires updated safety standards.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI role in cobots and collaborative robotics","","","In the recent work of Borboni et al. (2023) [ 18], the potential role of AI in the use of cobots for industrial applications was explored, and the state-of-the-art research on AI-based collaborative robotic applications was analyzed.","This excerpt highlights the potential role of AI in cobots and AI-based collaborative robotic applications, indicating AI capabilities in enhancing human-AI interaction and automation in industrial settings, which is relevant to the research focus on novel AI characteristics in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI methods for robot control and adaptation","","","Hua et al. (2021) [ 17] surveyed state-of-the-art deep learning, reinforcement, imitation, and transfer learning AI methods for robot control and adaptation to diverse complex environments and tasks, including their application to human–robot collaboration.","This excerpt directly mentions AI methods (deep learning, reinforcement, imitation, transfer learning) used for robot control and adaptation to complex environments and tasks, which relates to AI capabilities in handling variability and dynamic responses, aligning with the research focus on context-aware/adaptive behavior in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Model and classification scheme for gesture recognition","","","A model and classification scheme of gesture recognition for human–robot collaboration is proposed, with four technical components: sensor technologies, gesture identification, gesture tracking, and gesture classification.","This describes an AI capability involving a structured model and classification scheme for gesture recognition, which may involve adaptive or context-aware behavior in processing gestures, relevant to the research focus on novel AI characteristics such as context-aware adaptation in human-AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Technologies and algorithms for gesture recognition","","","The work reports on the most important technologies and algorithms of gesture recognition existing in the current research.","This highlights AI capabilities through the use of technologies and algorithms for gesture recognition, which are data-driven and potentially non-deterministic, fitting the focus on novel AI features like non-deterministic decision-making in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: Gesture recognition for human-robot communication","","","Liu and Wang (2018) [19] also covered human–robot collaboration in their review, specifically, gesture recognition used for communication between human workers and robots.","This represents an AI capability involving gesture recognition, which is a form of human-AI interaction interface enabling communication between humans and robots, aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Context-aware monitoring and adaptation","","","• By monitoring the human and the task context and using the knowledge to detect critical conditions, support, or adapt to the operator’s cognitive status for optimal system performance;","This describes an AI system feature where the system dynamically responds to the environment (human and task context) and adapts to the operator's cognitive status, which aligns with context-aware/adaptive behavior in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Data annotation and active learning","","","• By annotating data to be employed by AI algorithms. This step is crucial for the performance of supervised machine learning (ML); however, due to the effort and cost of manual labeling of large amounts of data, more efficient learning strategies have been developed, such as those using active learning that tries to maximize the model’s performance while querying a human to annotate a data sample as few times as possible.","This highlights an AI capability related to machine learning processes, where human interaction is used to improve data-driven decision-making, though it does not directly mention non-deterministic or adaptive features.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Physical capability amplification","","","• By amplifying the operator’s physical capabilities, as in the case of exoskeletons that can be worn by the user and enhance their physical performance, or telerobots that allow the human to perform a variety of complex tasks using the capabilities of robots;","This represents an AI capability where the system enhances human abilities, which is a feature of collaborative intelligence systems in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic task substitution and collaboration","","","• By embodying human physical capabilities as in the case of cobots, that can work alongside humans performing complementary tasks or substituting the operator in a dynamic way.","This describes an adaptive feature where AI systems (cobots) can dynamically substitute or complement human tasks, showing context-aware behavior in human-machine collaboration.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Expert validation and explanation of AI behavior","","","• By using expert knowledge to validate and explain intelligent machine behavior that, despite the emergence of explainable AI techniques, is still required to ensure the outcomes and that the generated explanations match the expected behavior in a reliable and unbiased way.","This directly addresses opacity/explainability issues in AI systems, noting that despite explainable AI techniques, human validation is still needed to ensure reliable and unbiased behavior, which is a key feature in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Black-box nature and vulnerability to adversarial attacks","","","Due to the black-box nature of AI/DL-based systems and their weakness to adversarial attacks and out-of-distribution inputs [ 25], the unique skills of humans such as pattern discrimination, high-level conceptualization, and hazard identification can be used by the system to mitigate safety risks.","This directly mentions the 'black-box nature' of AI/DL-based systems, which is a key characteristic of opacity/lack of explainability, and their 'weakness to adversarial attacks and out-of-distribution inputs', which relates to non-deterministic/data-driven decision-making and unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Intelligent attentional selection and human state awareness","","","Human gaze data can be used by AI to develop the intelligent attentional selection of information, or for the AI agents to be aware of the human cognitive and emotional state, fostering more natural communication and interactions between them.","This represents an AI capability for context-aware and adaptive behavior, as AI agents dynamically respond to human cognitive and emotional states to foster natural interactions, aligning with novel AI characteristics in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Emotion and expression recognition using AI algorithms","","","The mapping found that studies have mostly focused on recognizing the emotion and stress of HCI users, followed by gestures and facial expressions identification, using, more frequently, deep learning algorithms, including CNNs, and from the classical machine learning algorithms, SVMs.","This demonstrates AI capabilities in data-driven decision-making and context-aware behavior, as AI systems analyze sensor data to recognize human states and expressions, supporting adaptive interactions in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Probabilistic nature of AI","","","both the probabilistic nature of AI and the uncertainty of human behavior make the regulatory certification of CI methods a challenge [24].","This directly describes a non-deterministic/data-driven AI characteristic where outputs vary probabilistically, leading to uncertainty and regulatory challenges, contrasting with conventional deterministic automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI technique classification","","","AI techniques generally fit into three main groups: machine reasoning, machine learning, and robotics [ 27]. These three groups are, however, a very coarse classification, blending categorizations of learning style, similarity in form or function, and the AI problem it tackles.","This excerpt directly mentions AI techniques and their classification into groups (machine reasoning, machine learning, robotics), which represents a fundamental AI capability or characteristic discussed in the chunk.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Environment-interactive learning","","","• Embodied intelligence approaches: Approaches that allow for an agent to have higher intelligence, such as movement, perception, interaction, and visualization abilities. Approaches like reinforcement learning and learning (programming) from demonstration include not only the intelligent agent but also its “body” that interacts with the world according to some constraints, and the specific environment it is situated in.","This highlights AI approaches that interact with the world and environment, which relates to context-aware/adaptive behavior where systems dynamically respond to and adapt to their surroundings, handling variability and unpredictability.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Data-driven learning methods","","","• Machine learning approaches: Approaches that allow a machine/computer to learn from data, i.e., data-driven methods, such as Artificial Neural Networks and Ensemble Learning algorithms [ 32,33]. These approaches can be sub-classified based in the degree of supervision they have during learning (unsupervised, supervised, or semi-supervised learning) or the prior knowledge (inductive, transductive or transfer learning).","This directly mentions 'data-driven methods' and learning from data, which aligns with non-deterministic/data-driven AI characteristics where outputs vary with data/model states, introducing uncertainty and potential unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI problems and interaction tasks analysis","","","We analyze the trends and gaps in the AI problems addressed, the interaction tasks, and techniques used in the latest collaborative intelligence research landscape to set the directions for future research on safer human–machine collaboration.","It explicitly addresses 'AI problems' and 'interaction tasks' in collaborative intelligence, which are core to novel AI characteristics such as non-deterministic decision-making and context-aware behavior in human-AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: collaborative intelligence task sub-categorization","","","Furthermore, we uncover from the reviewed papers a sub-categorization of collaborative intelligence tasks (Figure 1) and link it to the previously identified types of CI interactions (Machine assists human-in-the-loop or human-in-the-loop assists the machine).","It describes 'CI interactions' including 'Machine assists human-in-the-loop or human-in-the-loop assists the machine', which relates to context-aware and adaptive behavior in AI systems, a key feature in human-AI collaboration.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: collaborative intelligence implementation","","","Our aim is to promote the progress and implementation of collaborative intelligence in safety-critical industries by providing a wide array of application examples, their limitations, and insight into outstanding safety concerns.","It directly mentions 'collaborative intelligence' in safety-critical industries, which is a key AI feature involving human-AI interaction and adaptation in high-risk environments, as specified in the research focus.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI techniques and methods","","","machine learning is the most popular technique in the retrieved papers, followed by probabilistic approaches. It is worthy of note that a large portion of work has employed either hybrid techniques or made use of more than one type of AI method to solve multiple AI problems, often necessary to achieve complex CI solutions.","This excerpt directly discusses AI system features by identifying specific AI techniques (machine learning, probabilistic approaches) and methods (hybrid techniques, multiple AI methods) used in research, which relates to AI capabilities and how they are applied to solve problems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI methods for motion planning and task scheduling","","","Different AI methods have been used in the literature to implement robot motion planning tasks or human–robot task scheduling, which aim at assisting the human and achieving optimal coordination and system performance in collaborative tasks.","This directly mentions the application of AI methods (a key AI feature) to implement specific tasks (robot motion planning and human-robot task scheduling) aimed at assisting humans and achieving optimal performance, which aligns with the research focus on AI characteristics in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Autonomous and semi-autonomous path learning","","","The main type of tasks identified within this sub-category were cobot motion path planning, cobot task planning and scheduling, and methods aimed at autonomous or semi-autonomous driving path learning that can also be translated into the cobot domain.","This excerpt explicitly mentions autonomous and semi-autonomous driving path learning as AI tasks, which relates to AI capabilities for dynamic decision-making and adaptation in high-risk domains like transportation and manufacturing.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: weak awareness of human mental models and expected robot behavior","","","the weak awareness of humans’ mental model and their expected robot behavior, a key ability for trust and reduction in safety risks [ 41].","This represents an AI feature related to human-AI interaction, specifically the system's ability (or lack thereof) to understand and align with human expectations, which is a novel characteristic compared to conventional automation that typically lacks such cognitive awareness.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: incorporation of driver intention/preference","","","For autonomous/semi-autonomous driving, of either teleoperated mobile robots [ 38,45] or vehicles [ 44], the goal of collaboration is instead the incorporation of the driver’s intention/preference in the generated paths, while assisting the driver with automated path generation, compliance with safety constraints, and the monitoring of environmental obstacles.","This represents context-aware/adaptive behavior as the AI systems dynamically respond to and incorporate the driver's intention/preference into path generation, adapting to human input while maintaining safety constraints, which is a feature of advanced AI versus static automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: dynamic real-time collision avoidance","","","For safe collaboration, earlier work defined Human Occupancy Volumes (HOVs) to constrain the path search space and compute the collision probability between human and robot [ 37], while more recent systems aimed at dynamic real-time collision avoidance using vision systems or models for human motion prediction [ 40,42,46,48].","This represents context-aware/adaptive behavior as the systems dynamically respond to the environment in real-time using vision systems or prediction models to avoid collisions, which is a key characteristic of novel AI systems versus conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI paradigms for control and planning","","","Different types of AI paradigms and methods have been applied to solve the complex tasks of robot motion planning and task scheduling, falling mostly under Search and Optimization paradigms for Control and Planning AI problems.","This directly mentions AI paradigms and methods applied to complex tasks in robotics, specifically for control and planning, which is a core AI capability in autonomous systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: RRT algorithm for motion planning","","","We highlight the classical Rapidly Exploring Random Tree (RRT) method for cobot motion planning [ 37,40,48]. The RRT algorithm is a path search algorithm that performs the efficient sampling of high-dimensional spaces with algebraic constraints (due to obstacles) and differential constraints (constraints on the system configuration variables that derive from inaccessible paths for the system) by incrementally biasing the search tree towards the unexplored state space.","This details a specific AI algorithm (RRT) used for motion planning in collaborative robots, demonstrating AI capabilities in handling complex constraints and search in high-dimensional spaces.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Neural network-based adaptive path planning","","","More modern strategies can employ neural networks for situations that require speed and reliability, and switch to a more accurate and safe baseline deterministic algorithm, using a simplex architecture for the safety assurance of the path planning system [46].","This excerpt highlights AI capabilities through the use of neural networks for dynamic decision-making and adaptation, contrasting with conventional deterministic automation by incorporating data-driven, potentially non-deterministic elements for improved performance and safety assurance.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Digital twin-enabled real-time adaptation","","","In the case of highly dynamical and cluttered environments, with strict real-time control requirements for collision avoidance, ref. [49] proposed the traditional path-planning algorithm Artificial Potential Fields connected to a digital twin of the current environment.","This excerpt represents a context-aware and adaptive AI feature, as it involves real-time response to environmental changes through a digital twin, enabling dynamic behavior adjustment in high-risk scenarios like collision avoidance, which is a novel characteristic compared to static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Real-time environmental adaptation without optimization","","","The algorithm does not require optimization and allows real-time reaction to environment changes by defining attractive forces towards a goal and repulsive forces from obstacles.","This excerpt illustrates an adaptive AI feature by enabling real-time reaction to environment changes, showcasing dynamic behavior and variability handling in response to external stimuli, which contrasts with less flexible conventional automation systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Timeline-based task planning with state variable modeling","","","Timeline-based task planners were the most commonly used methods for task planning and scheduling problems [37,39], modeling the human (state variable with uncontrollable values) and the robot behavior (state variable with partially controllable values) as multi-valued state variables to be controlled over time, based on causal, temporal, and synchronization legal constraints that dictate transitions between values and the duration of the value.","This represents an AI capability for task planning and scheduling, using advanced modeling techniques to handle dynamic interactions between humans and robots, which is a feature beyond basic automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Iterative learning through human cooperation","","","iterative learning strategies were proposed as an alternative for the data-hungry Deep Reinforcement Learning strategies, to enable learning through repeated cooperation with the human driver [44,45].","This represents a context-aware/adaptive AI feature, as it involves dynamic learning and adaptation through interaction with a human in real-time, contrasting with static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Bias detection and persuasive explanation","","","intelligent systems can detect human decision-making bias, and select the most effective way to explain and persuade the worker to take the most beneficial action [5].","This describes an AI capability where intelligent systems actively detect bias in human decisions and adapt their communication to persuade users, highlighting adaptive and context-aware behavior in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Probabilistic decision-making for safety-efficiency trade-off","","","Both usually rely on probabilistic approaches to find the optimal balance between safety and efficiency.","This represents a non-deterministic/data-driven AI feature because it involves probabilistic approaches, which inherently introduce variability and uncertainty in decision-making, aligning with the characteristic of outputs varying with data/model states and unpredictable failures in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive behavior using reinforcement learning and deep neural networks","","","adaptation to environmental changes, employing reinforcement learning methods, such as the Double Deep Deterministic Policy Gradients (D-DDPGs) [47]. When the state space, or action space, has high dimensionality and is very complex (such as in the case of a digital twin of an industrial cell), deep neural networks can be used to either represent the system states, or to approximate the optimal Q function, the optimal policy function, or the model (state transition function) in an efficient way. The network can be trained by minimizing the difference between the expected reward and the real reward received by the environment.","This excerpt directly mentions 'adaptation to environmental changes' and describes how reinforcement learning and deep neural networks enable dynamic response to complex environments, which aligns with the context-aware/adaptive behavior characteristic of novel AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Deep RL with trial-and-error learning and safety concerns","","","The combination of reinforcement learning and deep learning is called Deep RL, and has shown recently a lot of progress, even if with limited applicability to real-world problems so far due to the amount of data required to train the models and safety concerns, particularly for model-free RL, where the agent learns by trial and error.","This excerpt highlights AI capabilities like Deep RL and model-free RL, which involve learning by trial and error, indicating non-deterministic and adaptive behaviors, along with safety concerns relevant to high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Risk during learning phase","","","With RL-based solutions, there is the risk during the learning phase that the robot performs dangerous actions while interacting with the human and the environment.","This excerpt describes a non-deterministic characteristic of AI systems (RL-based solutions) where outputs vary during the learning phase, leading to unpredictable and potentially dangerous actions, which aligns with the research focus on non-deterministic/data-driven decision-making with unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: adaptation | Feature: Adaptation and proactive planning ability","","","Human motion intention prediction can confer to the robot the adaptation and proactive planning ability to better assist the human operator; however, mutual understanding is required for effective interactions (value alignment) [ 5]. An explainability metric can be computed or trained to assess the generated cobot motion plans, promoting the situational awareness and trust of the worker.","This represents an AI feature as it describes the robot's ability to adapt and plan proactively based on human motion intention prediction, which is a context-aware and adaptive behavior characteristic of novel AI systems compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: control | Feature: Robot control for human-robot collaboration","","","Robot control is an essential problem when humans collaborate with cobots or work with other types of robots, such as in teleoperation. Particularly in manufacturing tasks, cobots are required to perform dynamic complex tasks while maintaining performance, and ensuring the safety of the human collaborators.","This excerpt directly addresses control mechanisms in AI/robotic systems (cobots) for human collaboration, which is a core aspect of human-AI interaction in high-risk industries like manufacturing, though it does not explicitly mention novel AI characteristics like non-determinism or opacity.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: context-aware adaptation for safety","","","AI can be used for autonomous control, to manage the movement and behavior of robotic manipulators, providing safety solutions for automatic collision avoidance, path adaptation, and speed adaptation control, according to detected obstacles or predicted human motion.","This excerpt describes AI's ability to adapt robot behavior dynamically in response to detected obstacles or predicted human motion, showcasing context-aware and adaptive features for safety in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive and context-aware control","","","Neural networks can estimate complex dynamic models of a robot and adapt to changing environments, while fuzzy controllers allow to account for the subjective or imprecise target variables, such as the human effort in a task or speed scaling based on a collision risk assessment.","This directly describes AI features: neural networks adapt to changing environments (context-aware/adaptive behavior) and fuzzy controllers account for subjective/imprecise variables like human effort or collision risk (handling variability/unpredictability).","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: control | Feature: fuzzy control strategy for robot speed","","","A fuzzy-based control strategy was developed to monitor the time derivative of the distance between the operator and robot, the velocities, and the temperature of the closest point to the robot (to distinguish between human and non-human surfaces), and to output a velocity scaling factor based on five rules, mapping the inputs to three fuzzy sets of risk level (high, medium, and low).","This represents an AI feature as it involves a data-driven, rule-based control mechanism (fuzzy logic) that adapts robot behavior based on real-time inputs (distance, velocities, temperature) to ensure safety in human-robot interactions, aligning with context-aware/adaptive behavior characteristics.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive robot behavior to dynamic risk","","","For safe human–robot collaboration, several controller solutions were proposed for adaptation of the robot behavior according to the dynamic risk of hazards to the human workers.","This describes context-aware/adaptive behavior where the robot dynamically responds to changing environmental risks (dynamic risk of hazards), a key novel AI characteristic versus static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive robot motion and speed control","","","robot motion and speed adaptation for safety [54,56]","This directly describes context-aware/adaptive behavior where AI systems dynamically adjust robot actions (motion and speed) based on safety considerations, a key novel AI characteristic versus static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Handling uncertainties and optimizing comfort","","","Low tracking error, stable control, the handling of uncertainties in the robot’s dynamic model, and the optimization of human comfort are design requirements for safe human–robot interactions [52].","This mentions handling uncertainties in the robot's dynamic model, which relates to non-deterministic/data-driven decision-making where AI outputs vary with model states, introducing unpredictability that must be managed for safety.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Risk-based speed adaptation","","","Robot speed scaling can be further applied according to the estimated risk and hazard level of collision.","This exemplifies context-aware/adaptive behavior, where AI systems dynamically respond to environmental factors (estimated risk and hazard levels) by scaling robot speed, showcasing real-time learning and variability handling distinct from fixed automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Uncertainty quantification in optimization","","","Bayesian optimization, commonly used in ML to optimize hyperparameters, effectively optimizes objective functions by building a surrogate model of the objective, quantifying the uncertainty by Gaussian process regression, and using the surrogate to define an acquisition function to choose samples to evaluate the real objective function.","This excerpt directly mentions 'quantifying the uncertainty,' which relates to non-deterministic/data-driven decision-making where outputs vary with data/model states, introducing uncertainty as a key AI feature.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Customization and personalization","","","Customization is another key goal of Industry 5.0. The results retrieved from our search categorized under Design Optimization CI tasks highlight this trend, addressing approaches that can help optimize the design of wearable robots, collaborative robots, human–machine interfaces and interaction devices, and software to the individual user preferences, characteristics, and requirements.","This excerpt describes AI/autonomous system capabilities for customization and personalization based on individual user data, which relates to context-aware/adaptive behavior and data-driven decision-making in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Human-in-the-loop optimization with data-driven models","","","Ref. [60] proposed a human-in-the-loop design optimization approach for wearable robots (we do not consider this a situation where the human assists the machine, as it is not the expertise of the human that is used to optimize the design of the robot but force and kinematic data collected from specific movements), to minimize human metabolic energy during physical movement (computed using a musculoskeletal model of the human and the human–robot coupling).","This excerpt describes an AI system feature involving human-in-the-loop design optimization that uses data (force and kinematic data) and models (musculoskeletal model) to drive decisions, aligning with non-deterministic/data-driven decision-making and context-aware/adaptive behavior in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Hybrid optimization algorithm","","","The multi-optimization results showed that all methods achieve a safe design quickly, but the best performing method is H-EDA, a hybrid of an Estimation of Distribution Algorithm (EDA—an evolutionary algorithm that samples new solutions at each iteration from a probability distribution) and an Iterated Local Search algorithm (ILS—an extension of a local search algorithm that iteratively searches for local minima).","This excerpt mentions an AI capability involving evolutionary algorithms and local search methods for optimization, which relates to data-driven decision-making and adaptive behavior in AI systems, as it involves sampling from probability distributions and iterative search for solutions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Perception as Essential AI Capability","","","Within complex industrial environments, perception is an essential ability for quality assurance, the detection of production deviations, and the improvement of productivity and safety in human–robot collaboration activities.","This excerpt explicitly mentions perception as an essential ability for AI systems in industrial environments, highlighting a core AI capability that enables quality assurance, production deviation detection, and safety improvement in human-robot collaboration, which aligns with the research focus on AI characteristics in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Autonomous safety monitoring","","","three employed intelligent vision systems to assist the human in collaborative tasks, while autonomously ensuring appropriate safety levels (eliminating the need of human supervision for safety monitoring).","This describes AI systems (intelligent vision systems) performing autonomous safety monitoring, a capability beyond conventional automation that involves real-time decision-making without human intervention.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Real-time task coordination","","","A fourth paper performed human tracking but with the goal of the real-time estimation of task advancement for task coordination between humans and robots (Table 4).","This highlights AI's ability to perform real-time estimation and coordination in human-robot interactions, demonstrating adaptive and context-aware behavior for collaborative tasks.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: learning and testing in simulated environments","","","In some of the reviewed works, the learning and testing of safety-critical interactions and functionalities were also conducted in simulated environments, before implementation in real collaborative tasks, such as the testing of position and force tracking for a collaborative cell [ 52], and the offline training of a Deep RL model and a hazard estimator with a simple humanoid model built from the recorded skeleton data of human motion [ 54].","This represents a general AI capability, as it involves learning (e.g., Deep RL model) and testing of safety-critical functionalities in simulated settings, which is a common approach in AI development for high-risk applications.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: modalities for intent communication","","","Still, visual, haptic, or tactile modalities can also be employed to communicate the robotic system’s intent to the human in the least sensory-taxing way.","This relates to interface types in AI systems, as it discusses specific modalities (visual, haptic, tactile) for communicating robotic intent, which is part of general AI capabilities in human interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: communication of control actions","","","More research is needed to determine the best way to communicate control takeover and control actions, and how the communication method affects the subsequent human behavior.","This relates to interface and feedback mechanisms in human-AI interaction, as it discusses the communication of autonomous control actions and their influence on human behavior, which is a general AI capability.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: assessment of training data and validation","","","A common safety requirement that is overlooked is the assessment of the quality and representativeness of the training data, followed by a thorough validation of the required performance levels and robustness in all operational conditions [ 5].","This relates to general AI capabilities, as it discusses safety requirements involving training data assessment and validation of performance and robustness, which are critical for reliable AI systems in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: lack of explanation or transparency","","","However, the lack of explanation or transparency of the system’s autonomous actions can affect the situational awareness of the human, and cause overtrust or distrust towards the machine [ 5].","This directly addresses opacity/explainability, a key novel AI characteristic, by highlighting the absence of transparency in autonomous system actions and its negative effects on human trust and awareness.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: feedback | Feature: feedback modalities for communication","","","Ref. [ 58] reviewed several methods for communication between the robotic system and human operator in shared control scenarios, mostly focused on feedback modalities for communicating environmental information or assistive cues that indicate the desired operator action, relayed through the master manipulator.","This describes feedback mechanisms, a key aspect of AI interfaces, by detailing methods for communicating between robotic systems and humans in shared control scenarios, including environmental information and assistive cues.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Variability handling in human task execution","","","These types of solutions are developed to deal with the variability inherent in human task execution, between subjects and within the execution by the same subject, which can be a challenge for collaboration and coordination tasks.","This represents a context-aware/adaptive AI feature because it explicitly describes solutions that are developed to deal with variability in human behavior - both inter-subject and intra-subject variability - which requires systems to adapt to dynamic, unpredictable human inputs rather than following fixed, deterministic rules.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Probabilistic state estimation","","","a particle filter tracker (a probabilistic approach for system state estimation from partial/noisy observations), to estimate the tracking target object’s displacement and direction of movement from sequences of images, ignoring other movements in the scene.","This excerpt directly mentions a probabilistic approach for system state estimation from partial/noisy observations, which aligns with non-deterministic/data-driven decision-making as it involves uncertainty and variability in outputs based on data conditions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Generative probabilistic modeling","","","Additionally, a Gaussian Process Latent Variable Model (GPLVM; the GPLVM is a model that uses Gaussian processes to capture the latent structure of high-dimensional data in an unsupervised way and map it to a low-dimensional generative representation model of appearance) was used as a generative probabilistic prior to tracking the appearance changes of the human and consequently increasing the robustness of the approach.","This excerpt discusses a model that captures latent structure in an unsupervised way, which relates to opacity/lack of explainability as it involves complex, black-box processes for data representation, and also involves probabilistic elements contributing to non-deterministic behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Multi-modal fusion for detection and tracking","","","For multi-modal detection and human tracking, ref. [56] proposed a technique to fuse the modalities (depth and thermal image information) into a single image that can be processed by the CNN.","This excerpt describes an AI capability involving multi-modal data fusion (depth and thermal images) for detection and tracking, which is a context-aware and adaptive feature that enhances system performance in dynamic environments, distinguishing it from simpler conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Transfer learning with pre-trained CNNs","","","CNNs are particularly suited for image- and video-based classification tasks, and transfer learning can be employed with pre-trained networks to decrease training time, improve human–robot collision detection performance, and to take advantage of the human motion knowledge already existent in large open datasets [67].","This excerpt highlights an AI capability (transfer learning with CNNs) that enables improved performance in human-robot interaction by leveraging existing knowledge from large datasets, which is a data-driven approach characteristic of novel AI systems compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Unreliable detection due to data-driven variability","","","The pre-trained RGB-CNN could not provide reliable detection results, confusing humans with other objects with similar shapes.","This excerpt highlights non-deterministic behavior in AI systems, as the RGB-CNN's outputs vary and are unreliable, leading to unpredictable failures in distinguishing humans from other objects, which contrasts with conventional automation's more consistent performance.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Enhanced detection through multi-modal data fusion","","","Compared to the single-modality CNNs, the results showed that the multi-modal method could better distinguish humans from non-human objects due to the combination of temperature and spatial information, achieving the lowest percentage of false positives (2.4% versus 36.87% with only depth information and 64.35% with only temperature information), which the authors claimed can help to reduce unnecessary robot stops.","This excerpt demonstrates an AI capability where data-driven decision-making (using fused temperature and spatial data) improves performance over conventional single-modality approaches, reducing false positives and enabling more adaptive and reliable system behavior in dynamic environments.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: signal alignment and similarity measurement for motion tracking","","","Human motion tracking for task advancement monitoring brings different challenges. The method proposed to deal with this is a modified version of the Dynamic Time Warping algorithm that allows one to align signals in time, measure their similarity, and deal with occlusions or human movements not relevant to the task [66].","This represents an AI capability as it involves algorithmic processing (Dynamic Time Warping) to analyze and interpret human motion data, which is a data-driven task common in AI systems for monitoring and adaptation in interactive contexts.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Online learning from previous executions","","","The template of the task is learned online from previous executions not requiring offline training.","This represents context-aware/adaptive behavior as the system dynamically learns and adapts task templates in real-time from previous executions, enabling variability handling without pre-training.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: CNN-based motion recognition with transfer learning","","","In human–robot collaboration applications, human motion recognition is performed using CNN models combined with transfer learning [ 72,86,87] for improved recognition performance.","This excerpt directly mentions AI methods (CNN models and transfer learning) applied to human motion recognition, showcasing data-driven decision-making and adaptive learning features in high-risk industrial contexts like manufacturing and aviation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI model training for activity recognition","","","Ref. [90] trained an LSTM model with spatiotemporal human-skeleton data for activity recognition, achieving 91.4% accuracy on a dataset.","This excerpt describes an AI capability involving training a machine learning model (LSTM) with specific data (spatiotemporal human-skeleton data) to perform a task (activity recognition), which is a core AI feature distinct from conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Human intent prediction in automotive","","","In the automotive domain, human intent prediction is performed by Advanced Driver Assistance Systems (ADASs), automatic collision avoidance, and cooperative control systems.","This represents an AI feature as it involves systems that predict human intent to enable advanced automation and safety functions, contrasting with conventional automation that may not incorporate such predictive capabilities.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Motion intention prediction and goal inference","","","Motion intention prediction and goal inference from motion are common challenges in HRC scenarios that can not only support safe collaboration from collision avoidance techniques and human–robot contact intention detection but can also be employed by cognitive systems for worker workload alleviation automation and assistance.","This represents an AI feature as it involves cognitive systems performing prediction and inference tasks to support automation and safety, which are novel compared to conventional automation that may lack such adaptive understanding.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Context-aware motion intention recognition","","","Human motion intention recognition may be context-dependent, as similar motion may correspond to different tasks. To better differentiate between similar situations, ref. [ 72] proposed an extra tool identification step after motion recognition, implemented by another deep neural network, to better understand the task context.","This excerpt directly mentions context-dependent behavior in AI systems, where similar motions correspond to different tasks, requiring adaptive mechanisms like deep neural networks to differentiate situations and understand context, aligning with the context-aware/adaptive characteristic of novel AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: platforms/frameworks | Feature: Gesture-based interaction framework","","","Ref. [ 75] developed a gesture-based human–robot interaction framework that allows the worker to give instructions to a collaborative robot for tools and parts’ delivery, or for holding objects.","This excerpt explicitly mentions a 'gesture-based human–robot interaction framework,' which represents a platform or framework for human-AI interaction, supporting the development and deployment of AI/autonomous systems in collaborative settings.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: Gesture-based human-robot interaction","","","Human gestures are also useful for more intuitive and efficient robot control and instruction, providing a more ergonomic interaction modality, compared to traditional teleoperation systems and controllers.","This excerpt highlights a novel human-AI interaction interface (gesture-based control) that enhances intuitiveness and efficiency compared to conventional automation (traditional teleoperation), aligning with AI system features related to advanced interface types.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Neural network-based classification","","","To achieve the soft object/hard object interaction classification from robot torque measurements, ref. [ 92] employed a widely used neural network, the ResNet, achieving 98% accuracy on the test dataset.","This represents an AI feature as it involves a neural network (ResNet) for data-driven decision-making in classifying interactions, which is a characteristic of advanced AI systems compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Real-time, subject-independent gesture recognition with high accuracy","","","demonstrating that it is effective in recognizing in real-time 12 different types of gestures in a subject-independent manner, achieving accuracies between 90% and 100%.","Real-time recognition with high accuracy and subject-independence highlights advanced AI capabilities in dynamic, adaptive behavior and data-driven decision-making, which are key novel characteristics compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: ANN-based gesture classification","","","multiple feedforward ANNs were used to classify the different types of dynamic and static gestures.","Feedforward ANNs are a core AI technique for pattern recognition and classification, representing a non-deterministic, data-driven decision-making system as they learn from data to make predictions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: Algorithmic gesture-to-command translation with feedback","","","a parameterization robotic task manager algorithm was implemented to translate the recognized gestures into robot commands, and provide visual or speech feedback to communicate to the human worker: the gesture options available to select, if the gesture was recognized, and what gesture was identified.","This represents an AI-driven interface feature that enables human-AI interaction by algorithmically interpreting gestures and providing real-time feedback, which is characteristic of adaptive and context-aware systems in human-AI collaboration.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Uncertainty handling via probabilistic methods","","","Probabilistic methods can further be incorporated to account for the uncertainty inherent to human behavior.","This directly addresses uncertainty in decision-making, a key characteristic of non-deterministic/data-driven AI systems, as it involves variable outputs and model-dependent approaches to account for unpredictable elements like human behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Physiological signal-based intention recognition","","","Brain waves monitored with mobile EEG systems [84], body movement data collected with force myography bands [83], and human gaze data from eye-tracking and VR headsets have been shown to be able to detect the worker’s intentions from 54 milliseconds up to 2 seconds before the action, providing the robot with time to take initiative.","This represents an AI capability where machine learning models (implied by the context of LSTM-RNN networks mentioned earlier) process physiological data to predict human intentions, enabling anticipatory robot behavior - a data-driven, context-aware feature distinct from conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Error/conflict detection from physiological signals","","","EEG has also been employed to detect interaction errors/conflicts between the worker’s intention and the robot’s actions, which can be employed to improve the system’s response to errors [91].","This describes an AI capability where physiological data (EEG) is used to detect mismatches between human intent and robot actions, allowing for adaptive system responses to errors - a form of context-aware, feedback-driven behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: SVM-based error classification from brain signals","","","The evoked error-related potentials in the brain, due to either a robot error in the user intention detection, or violation of the user’s preference during autonomous behavior, were used by an SVM model to correctly distinguish the type of error above the chance level.","This describes a specific AI implementation (SVM model) that processes brain signals to classify different types of errors that occur during autonomous robot behavior, representing a data-driven, machine learning approach to understanding and responding to system failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: probabilistic behavior prediction","","","The drivers’ behavior was predicted with a probability between 0.75 and 1.","It demonstrates non-deterministic/data-driven decision-making as the output (prediction) varies with probability, reflecting uncertainty and model-dependent decisions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: real-time intention anticipation","","","the framework to anticipate in real-time the drivers’ intentions approximately three seconds before the maneuver, with more than 90% accuracy.","It exemplifies context-aware/adaptive behavior by dynamically responding to the environment in real-time to predict and adapt to driver intentions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive and fairness-aware personalized ADAS using Hierarchical Reinforcement Learning","","","The work of [ 85] was able to capture the driver’s preferences and variability with a Hierarchical Reinforcement Learning model, for an adaptive and fairness-aware personalized ADAS. The method involves using three interacting RL agents to learn to adapt to intra-, inter-and multi-human state variability, applying Q-learning-based algorithms.","This excerpt directly mentions an AI system (Hierarchical Reinforcement Learning model) that is context-aware and adaptive, as it learns to adapt to intra-, inter-, and multi-human state variability, which aligns with the research focus on dynamic environment response and variability handling in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Machine learning prediction","","","ensemble ML methods, namely, Random Forest and XGBoost-Extreme Gradient Boosting, were used to predict the workers’ actions using the aircraft’s trajectory features.","This represents an AI feature as it involves data-driven decision-making through ML models to automate task prediction, aligning with non-deterministic characteristics due to model-dependent outputs.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Iterative adaptation to user behavior","","","the performance of a time-to-crash alarm was adapted iteratively to different subject behaviors.","This directly describes an AI system feature where the automation dynamically adjusts its behavior (adaptation) based on the context (different human behaviors), which is a key characteristic of context-aware/adaptive AI systems versus static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Superior performance of personalized models","","","achieved significantly better results with the personalized models compared to general models (non-individualized).","This highlights the effectiveness of adaptive AI systems that tailor their behavior to individual contexts (personalized models), demonstrating a key advantage over non-adaptive conventional automation (general models).","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: CNN-based prediction for personalization","","","used a CNN for the prediction of controller actions/strategy to develop a personalized workload alleviation automation solution.","This mentions an AI capability (CNN for prediction) used to develop a personalized automation solution, which involves data-driven decision-making and adaptation to individual users, distinguishing it from conventional one-size-fits-all automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Human-state aware adaptation","","","The work of [103] took the first steps to accomplish complete human adaptation, including worker behavior prediction and adaptation on a short-term, and long-term policy adaptation to operator characteristics, such as expertise.","This excerpt directly mentions 'complete human adaptation' with 'worker behavior prediction and adaptation' and 'long-term policy adaptation to operator characteristics,' which aligns with context-aware/adaptive behavior in AI systems that dynamically respond to human states and environments.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Perception-based human state recognition","","","Human state recognition has been proposed by the works selected for four main applications: for driver state recognition, namely, drowsiness, attention and affective state, for mental workload recognition in aviation, for human performance prediction in control rooms, and for physical and affective state recognition in HRC scenarios. These tasks can be classified as perception tasks directed towards the awareness of the workers’ internal state, from physiological or behavioral indicators of state.","This excerpt highlights AI capabilities in perception tasks, specifically recognizing human states like drowsiness, attention, and affective states, which are novel AI features compared to conventional automation, as they involve data-driven decision-making from physiological or behavioral indicators.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Optimization and feature combination for AI performance","","","We highlight the use of Particle Swarm Optimization for the selection of optimal neural network hyperparameters, and the combination of the classical SVM classifiers with hand-crafted and deep-learning-based features for optimal recognition performance.","This represents an AI capability involving optimization techniques (Particle Swarm Optimization) and hybrid model approaches (combining SVM with hand-crafted and deep-learning features) to enhance recognition performance, which are characteristic of advanced AI systems versus conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Data-driven decision-making with machine learning","","","SVMs integrated with bootstrap aggregation were applied to fuse multi-modal physiological data (eye-tracking data, skin conductance response, and respiratory function) and learn to predict operator performance.","This represents an AI feature as it involves machine learning (SVMs) and data fusion techniques to make predictions based on physiological data, aligning with non-deterministic/data-driven characteristics through model-based learning and adaptation to data.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Real-time emotion recognition using deep learning","","","On the human–robot collaboration domain, ref. [ 98] aimed at recognizing human emotion for improved assistance and safety of the human partner. A standard web camera was used to monitor human emotion in real-time, and a transfer learning approach was used with a Deep CNN for improved classification accuracy. The highest cross-validation accuracy value reached was 97.82%.","This represents an AI capability as it involves data-driven decision-making through a Deep CNN for classification, enabling adaptive and context-aware behavior in human-robot interaction, which is a novel characteristic compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Context disregard in machine learning models","","","This additional context is commonly disregarded by machine learning models and may be required for better generalization across domains.","This directly mentions a characteristic of machine learning models (an AI technology) regarding their handling of context, which relates to their data-driven and potentially opaque nature in decision-making.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Real-time human state estimation for adaptive control","","","performing online estimation of human arm impedance/stiffness from surface electromyography and stretch sensor data, for the improvement and tuning of the exoskeletons’ strength amplification controller.","This demonstrates context-aware/adaptive behavior by dynamically responding to human physiological states (arm impedance/stiffness) to improve controller performance, showing real-time adaptation to environmental changes.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Machine learning model for performance evaluation","","","Using a random forest regression model, the estimation performance was evaluated offline with a validation dataset and online (no ground truth) by monitoring the controllers’ stability while the arm stiffness was changing.","This shows AI capability through the use of a random forest regression model for performance evaluation, demonstrating data-driven decision-making in both offline validation and online monitoring contexts.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Human Mental Model Estimation","","","Estimating the human mental model, meaning a human’s mental representation of how a system works, provides insights into how operators perceive and interact with a system, supporting the development of improved user-centric designs, better communication in collaborative tasks, modeling of the decision-making process, detection of cognitive biases, prediction of human behavior and performance, and detection of potentially dangerous situations stemming from human-system model disparities.","This represents an AI capability for understanding and modeling human cognition and interaction with systems, which is a key aspect of human-AI interaction research. It enables prediction of human behavior, detection of cognitive biases, and identification of safety risks from mismatches between human and system models.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Cognitive computational modeling for behavior anticipation","","","Ref. [106] used a cognitive computational modeling approach to simulate pilots’ mental models and anticipate their behavior in human–AI teams.","This represents an AI capability where computational models simulate human cognition to predict behavior in collaborative settings, which is a novel AI characteristic compared to conventional automation that typically follows fixed rules without such cognitive modeling.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Unsupervised machine learning for pattern identification","","","the unsupervised machine learning method Agglomerative Hierarchical Cluster Analysis (HCA) to learn and identify individual behavior patterns in situation models from complex and unstructured empirical data, also helping to identify individual differences in mental representations.","This represents an AI feature as it involves unsupervised machine learning (a data-driven AI method) to analyze complex, unstructured data and identify patterns, which is a core capability of AI systems compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Individually simulating model for behavior anticipation","","","The individually simulating model (ISM) performed significantly better (37% increase in accuracy in anticipating a first pilot action and about 13% for a second action) than the normative model (NM) in anticipating the pilot’s behavior in an engine fire event.","This excerpt describes an AI model (ISM) that adapts to individual differences in pilot behavior, showing context-aware and adaptive characteristics by outperforming a normative model in anticipating actions during a specific event.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI Safety Task Categorization","","","Four general CI tasks were identified in the selected works focused on implementing safety functionalities: control tasks for safety and task requirement compliance, perception tasks for safety monitoring, model checking, and model prediction uncertainty reasoning.","This excerpt directly lists AI capabilities (control, perception, model checking, uncertainty reasoning) implemented for safety, which are key features distinguishing AI systems from conventional automation in high-risk domains.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI methods for safety assessment","","","This section includes a variety of papers that can be included under CI for safety assessment, as they propose AI methods specifically developed to monitor or assess the behavior of a system, essential to guarantee safety in safety-critical applications.","This explicitly mentions AI methods developed for monitoring or assessing system behavior, which is a key AI capability in safety-critical contexts, aligning with the research focus on high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Machine learning for pilot assistance","","","It was demonstrated how the combination of ACT-R and machine learning can improve pilot assistance by selecting sub-models and learning from experience.","This directly mentions machine learning as an AI method used to improve assistance by selecting sub-models and learning from experience, which is a core AI capability.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: adaptation | Feature: Adaptive explanations for policy repair","","","Safety measures should take into account how the workers mental model evolves over long-term interactions and how the created explanations for policy repair should adapt to the worker’s confidence on their mental model.","This describes an adaptive feature where explanations for policy repair should adjust based on the worker's confidence in their mental model, indicating context-aware or adaptive behavior in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: High-accuracy neural network performance metrics","","","The object detector reached test set accuracies above 95%, the two ANNs used for human–robot safety distance assessment reached accuracies above 98%, and the CNN used as a speech recognizer in a preliminary test reached 93% accuracy in the classification of simple answers.","This represents an AI feature because it quantifies the performance of neural network components (object detector, ANNs, CNN) with specific accuracy metrics (95%, 98%, 93%), highlighting their effectiveness in AI-driven tasks for safety monitoring and human-robot communication.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Neural network-based safety monitoring system","","","Ref. [ 108] proposed a complete safety monitoring system for human–robot collaboration workspaces, using neural networks. Four neural networks were used for each component of the system: an object detector using the Faster RCNN ResNet 101 Coco model, safety distance assessors using ANNs, and a speech recognizer for communication between humans and robots using a CNN.","This represents an AI feature because it specifically mentions the use of neural networks (Faster RCNN ResNet 101 Coco model, ANNs, CNN) for various components of a safety monitoring system, demonstrating AI capabilities in object detection, distance assessment, and speech recognition for human-robot interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: control | Feature: supervisor controller for collision prevention","","","To prevent possible collisions between human and collaborative robots, ref. [ 112] developed a supervisor controller that focuses on optimizing stop trajectories.","This represents an AI feature as it involves a controller that optimizes trajectories to prevent collisions, demonstrating adaptive and safety-focused control mechanisms in autonomous systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: supervised assistance","","","The supervised assistance of the robot was able to help the human complete a task with fewer steps and a higher probability of reaching the final state.","This represents an AI feature as it involves a robot providing supervised assistance to enhance human task performance, indicating adaptive or intelligent support capabilities in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic Safety Adaptation","","","The method consists of the online scaling of dynamic safety bounding volumes enclosing the robot and human. The size of the zones is optimized by minimizing the time of potential stop trajectories, considering the robot dynamics, its torque constraints, and its speed with respect to the human.","This represents a context-aware/adaptive AI feature as it involves real-time scaling and optimization of safety zones in response to dynamic interactions between a robot and a human, demonstrating adaptation to environmental changes and variability handling.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Modeling with Markov Decision Processes","","","Here, the robot is modeled by Markov Decision Processes, the human by Partially Observable Markov Decision Processes (to account for their unknown/hidden intents), and the supervisor controller of the robot by a Deterministic Finite Automata.","This excerpt details AI capabilities in modeling systems using Markov Decision Processes and Partially Observable Markov Decision Processes to handle hidden intents, which is a key aspect of AI system design for dynamic and uncertain environments.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Learning algorithm for uncertainty handling","","","More specific supervisor models can be developed, such as the work of [ 107] that proposed the use of a L * learning algorithm [ 118] to ensure safety requirements and task completion in systems with uncertainties, such as human–robot collaboration.","This excerpt explicitly mentions 'systems with uncertainties' and uses a learning algorithm to address them, which relates to non-deterministic/data-driven decision-making where outputs vary with data/model states, leading to uncertainty and unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: control | Feature: Logic interpretation and supervisor learning","","","Probabilistic Computation Tree Logic (PCTL) is used to interpret the logic specifications of the system, and then a deterministic supervisor that satisfies the PCTL specifications can be learned by a L* learning algorithm, through queries and counterexamples.","This excerpt highlights control mechanisms in AI systems, where PCTL interprets logic specifications and a learning algorithm is used to derive a supervisor, demonstrating adaptive control features in handling system requirements.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Reinforcement learning for data generation","","","Reinforcement learning was employed in a first step, through simulation, to maximize the unique state observations of a Markov Decision Process and generate a dataset for invariant mining.","This excerpt explicitly mentions 'reinforcement learning,' which is a specific AI/ML technique used for decision-making and adaptation, distinguishing it from conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Transparency enhancement through uncertainty quantification","","","Uncertainty quantification can also benefit the transparency of the algorithms [5].","Addresses opacity/explainability by linking uncertainty quantification to increased transparency of algorithms, helping mitigate black-box behavior and trust challenges.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Uncertainty quantification and communication","","","Techniques can be developed to estimate the confidence/uncertainty on their outputs and communicate it to the human stakeholders.","Directly addresses non-deterministic/data-driven decision-making by focusing on estimating and communicating uncertainty in AI outputs, which relates to variable outputs and model-dependent decisions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Explainability constraints in AI algorithms","","","The XG-CBS algorithm modifies the Conflict-Based Search (CBS) method for Multi-Agent Path Finding (MAPF) by adding explainability constraints, resulting in a set of non-colliding paths that admit a short-enough explanation that can be visualized as a sequence of images for each time segment where the agent trajectories are disjoint.","This excerpt directly addresses the opacity/explainability characteristic by describing how the XG-CBS algorithm adds explainability constraints to improve transparency and trust in AI decision-making, which is a key feature distinguishing novel AI from conventional automation in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: explainability","","","The results show that the approach is effective in balancing planning time and explainability for the human supervisor.","This directly addresses the opacity/explainability characteristic by discussing how the approach balances explainability for human supervisors, which relates to making AI decisions more transparent and interpretable.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Predictive power for real-time prevention","","","Accident/error prediction is an essential CI task, in which the AI predictive power can be used to prevent human-related critical situations in real-time.","This directly mentions an AI capability (predictive power) used for a specific task (accident/error prediction) in real-time, which is a characteristic of advanced AI systems versus conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic task allocation and risk mitigation decision-making","","","We observed that more recent work displays a higher level of complexity [ 116,117], considering the several components of a HRC system that impact safety, requiring modules for situational assessment, resource optimization (e.g., dynamic task allocation), and decision-making for long-term risk mitigation.","This excerpt highlights AI features such as situational assessment and dynamic task allocation, which are context-aware and adaptive behaviors, allowing the system to respond to changing environments and optimize resources in real-time.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Data-driven neural network for speed prediction","","","using a Non-linear Autoregressive (NAR) Neural Network trained with data from Nclusters of different types of drivers (different driving behavior).","This represents a non-deterministic/data-driven AI feature because it involves a neural network (a data-driven model) trained on varied data from different driver types, leading to variable outputs based on data and model states, which aligns with uncertainty and unpredictable behavior in decision-making.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Context-aware adaptive behavior using Reinforcement Learning","","","Elmalaki et al. (2018) [120] used Reinforcement Learning for adaptive forward collision prediction and warning, taking into account the driver’s context, meaning the human state (specifically attentive or distracted state based on audio streams collected by the driver’s phone) and preferences across drivers and time. The proposed driver-in-the-loop context-aware ADAS system led to an increase of 94.28% in the safety of the driver and a 20.97% improvement in the driving experience.","This directly describes an AI system (Reinforcement Learning) that exhibits context-aware and adaptive behavior by responding to dynamic driver states (attentive/distracted) and preferences over time, which aligns with the research focus on novel AI characteristics like dynamic environment response and variability handling.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Deep-learning model application","","","A common natural language processing deep-learning model, the seq2seq encoder–decoder model, was used to process visual control panel states from synthetic HMI data to detect operator error precursors.","This represents an AI feature as it involves a deep-learning model (seq2seq encoder–decoder) used for data-driven processing and detection tasks, highlighting AI capabilities in analyzing synthetic HMI data for error prevention.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Probabilistic decision-making","","","Risk and accident prediction for a human-in-the-loop decision-support system Perception Bayesian network Probabilistic Maritime industry","This represents non-deterministic/data-driven decision-making as it involves probabilistic modeling (Bayesian network) for risk prediction, indicating variable outputs based on data/model states.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive behavior","","","Adaptive forward collision prediction and warning for an ADAS Perception RL Embodied Intelligence Automotive industry","This represents context-aware/adaptive behavior as it uses Reinforcement Learning for adaptive prediction and warning, indicating dynamic response to the environment.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Black-box behavior","","","Detection of operator error precursors for accident event prevention Perception Deep seq2seq neural network with attention mechanism Machine learning Nuclear and aviation industry","This represents opacity/lack of explainability as it involves a deep neural network with attention mechanism, which can be a black-box model with internal reasoning not easily visible.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Data-driven prediction","","","Prediction of driver’s speed tracking errors for an ASA Perception NAR-NN Machine Learning Automotive industry","This represents non-deterministic/data-driven decision-making as it uses a Non-linear Autoregressive Neural Network for prediction, indicating model-dependent decisions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Human-adaptive systems","","","Human assistance systems were proposed in the collection of selected articles, employing a variety of features that account for human factors or adapt to human preferences and behavior.","This represents context-aware/adaptive behavior as it describes systems that adapt to human factors and behavior, indicating dynamic response to user context.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Learning to imitate human intervention decisions","","","A CNN was then trained with state–action pairs and a corresponding binary label of whether the action was blocked or not, to learn to imitate the human’s intervention decisions (perception of human policy) since human oversight for the total training time of an agent can be infeasible or very costly.","This represents an AI capability where the system learns to replicate human safety decisions, enabling adaptation to human oversight patterns, which is a data-driven approach not typical in conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Trial-and-error learning with potential for dangerous actions","","","RL-based systems that are designed to interact with humans can perform potentially dangerous actions during training. In particular, for model-free agents that can only learn with trial and error, having human intervention is the only way to avoid catastrophes when the agent has not learned yet.","This represents a novel AI characteristic where the system's learning process involves trial and error that can lead to dangerous actions, contrasting with conventional automation that typically operates within predefined safe parameters.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Probabilistic graphical modeling with structure learning","","","data from error messages and warnings (alarm log) are employed as input for a probabilistic graphical model (Bayesian network), having used first a Max-Min Hill-Climbing algorithm to learn the structure of the graph.","This represents an AI capability because it involves a data-driven, probabilistic model (Bayesian network) that uses a specific machine learning algorithm (Max-Min Hill-Climbing) to learn relationships from data, which is characteristic of AI systems rather than conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Model adaptation and confidence measurement for new environments","","","The PORF model showed an upward trend in cumulative reward with increased sampling and training, demonstrating the potential for human–machine collaboration in autonomous driving; however, the model’s confidence should be measured before using the framework in new environments.","This excerpt mentions the model's adaptation through increased sampling and training, showing improved performance, and emphasizes the need to measure confidence in new environments, indicating context-aware and adaptive behavior that responds dynamically to different settings.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Deep RL framework with hybrid DNN and two-stage training","","","The Progressive Optimized Reward Function (PORF) learning model proposed by [125] can be integrated into a Deep RL framework. A hybrid DNN model structure composed of a CNN followed by a conv-LSTM receives as input vehicle front-view sequential images labeled based on the vehicle driving behavior evaluation (the reward). The DNN is trained in two stages: in the pre-training stage, the network is trained with images labeled by a formula that represents the safety of the vehicle–road relationship using data from a virtual environment, and in the progressive optimization stage, the network is trained and optimized continuously with data from human evaluations of the vehicle driving behavior in a real environment.","This excerpt details specific AI capabilities, including a deep reinforcement learning framework, hybrid DNN model structure (CNN and conv-LSTM), and a two-stage training process involving virtual and real-world data, which are advanced AI features compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: interface | Feature: adaptive and responsive user interface","","","The generated graphical causal model is then presented to the operators via an adaptive and responsive user interface, where the users can give feedback to the system by reporting a correct or incorrect inference of the root cause, or by directly editing the generated model.","This represents an AI feature as it involves an adaptive interface that responds to user feedback, enabling human-AI interaction for model refinement, which is a novel characteristic compared to static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: feedback | Feature: iterative model generation with feedback","","","Both the feedback and the changes to the model are used in the next iteration of the model generation.","This represents an AI feature as it involves a feedback mechanism that influences model updates, supporting adaptive behavior and continuous improvement, which is characteristic of data-driven AI systems versus fixed automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Language-based human-AI interaction and adaptive help-seeking","","","the agent can query the human advisor for language sub-goals when it is lost and cannot make progress [132]. To implement such a methodology, it is required to learn a navigation and help policy to constrain the number of help requests that can be made to the advisor and learn at what points it is more effective to make the help requests for the overall progress in the task.","This excerpt describes an AI agent's capability to interact with humans using language, adaptively seek help when stuck, and learn policies to optimize this interaction, which is a novel AI feature compared to conventional automation that typically operates without such dynamic, context-aware human collaboration.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Learning from Demonstrations with Probabilistic Models","","","Ref. [128] used learning from demonstrations (LfD) to perform robot motion synthesis with obstacle avoidance, using probabilistic task representations to estimate human occupancy in a long-term scale and a Hidden Semi-Markov Model (HSMM) to encode the demonstrated motions.","This represents an AI capability involving data-driven learning (LfD) and probabilistic modeling for motion planning, which contrasts with deterministic conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Proactive behavior in response to user uncertainty","","","Proactive behavior was also observed, where the robot carried out the task when the user was unsure of how to perform it, in order to show its intention and how to proceed.","This describes context-aware/adaptive behavior where the AI system dynamically responds to the user's state (uncertainty) by adapting its actions to demonstrate intention and procedure, which is a novel AI characteristic compared to conventional automation that typically follows fixed scripts.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: adaptive behavior through user interaction","","","A probabilistic extension of the method can be used to generate dynamic robot behavior, where the temporal dynamics are adapted by interactions with the user. The Adaptive Duration Hidden Semi-Markov Model [126] allows the robot to not only react to the user but to also behave proactively according to the encoded temporal coherence of the task.","This excerpt directly mentions 'dynamic robot behavior' where 'temporal dynamics are adapted by interactions with the user', aligning with the context-aware/adaptive characteristic of AI systems that respond dynamically to environments and user inputs, contrasting with static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Probabilistic encoding for adaptive replanning","","","The method was evaluated on reproducing air-drawing motions of letters near a moving human arm, with formal safety verification and replanning with a failsafe trajectory to avoid collision, resulting in no safety stops observed for all letters. The trajectories of the robot motion that were replanned to avoid collision still maintained the curvature and shape of the letters since the new trajectory was sampled from the probabilistic encoding of the movement. As the method does not need optimization or real-time collision checking, it computes in deterministic time; however, its validity and effectiveness are dependent on the human model used.","This represents a context-aware/adaptive AI feature as it involves dynamic response to a moving human arm through replanning with a failsafe trajectory, using probabilistic encoding to maintain task performance (letter shape) while adapting to avoid collisions, which is characteristic of novel AI systems in high-risk interactions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic time warping for handling variability in demonstrations","","","Alternatively, dynamic time warping can be employed to process demonstrations with varying operation speeds and align their timelines. Ref. [ 134] used the method to teach a robotic arm assembly motion from human demonstrations.","This represents a context-aware/adaptive AI feature as it involves processing demonstrations with varying operation speeds (handling variability) and aligning timelines to teach robotic motion, which adapts to dynamic inputs from human demonstrations.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Optimization and control for reduced force feedback","","","The method was evaluated with an assembly task, showing lower force feedback compared to the teaching data due to the optimization and smoothing of the GMR model, and the admittance control applied by the controlling system.","This represents an AI feature as it highlights the AI system's capability to optimize and smooth operations using a GMR model and admittance control, leading to measurable performance enhancements like lower force feedback, which is characteristic of advanced AI systems in automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive trajectory optimization with varying speeds","","","The proposed multi-dimensional dynamic time warping (SMMD-STW) method was able to segment and align multiple repetitions of teaching data with varying operation speeds, and a Mixture Gaussian regression (GMR) model was subsequently used to obtain an optimal reference trajectory and expected force field.","This represents an AI feature as it involves context-aware adaptation to varying operation speeds and data-driven optimization using a GMR model, which aligns with novel AI characteristics like dynamic response and variability handling in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Learning from Demonstrations and Interventions","","","Robot Control Learning In the following four papers, robot control was learned from demonstrations and interventions, through the learning of expert policies and rewards. In [131], the policy for an automated driving system was learned using an Interaction Imitation Learning method, a probabilistic variant of the DAGGER algorithm [ 137], to maximize sampling from a novice policy during learning (instead of the human expert), while constraining the probability of failure by a learned safety threshold.","This excerpt directly mentions AI capabilities involving learning from demonstrations and interventions, which is a key characteristic of advanced AI systems compared to conventional automation, as it involves data-driven policy learning and adaptation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: control | Feature: Human-Gated Control Mechanism","","","The Human-Gated DAGGER variant was proposed to give the human expert exclusive control of whether he/she or the novice should be in control at each moment.","This excerpt describes a control mechanism in AI systems where human experts can gate or override AI decisions, highlighting interactive and adaptive control features relevant to human-AI collaboration in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Ensemble-based uncertainty modeling","","","The risk metric was approximated by modeling the novice as an ensemble of neural networks and using the covariance matrix of the ensemble outputs to compute the policy confidence over the state space.","This represents non-deterministic/data-driven decision-making because it explicitly models uncertainty through ensemble methods and covariance matrices, showing how outputs vary with model states rather than being fixed.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptive control transfer learning","","","The threshold to this risk metric at which control should be given back to the expert in unsafe situations was learned by computing the mean of the novice doubt at the time of human intervention","This demonstrates context-aware/adaptive behavior as the system dynamically responds to unsafe situations by learning thresholds for control transfer based on real-time doubt metrics.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Online learning and probabilistic decision-making","","","The authors aimed at learning a shared control strategy for teleoperation in an online fashion from demonstrations and with reinforcement learning. The approach predicts the success probability in selecting one of the robot controllers and requesting human teleoperation if necessary.","This represents an AI feature as it involves learning from data (demonstrations) and reinforcement learning to adapt control strategies dynamically, with probabilistic predictions for decision-making, which contrasts with static, rule-based conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Continuous online learning and improvement","","","The controller’s policies were learned from demonstrations and continuously improved using online learning. The Deep Deterministic Policy Gradients algorithm was used for off-policy, model-free reinforcement learning of the robot policy.","This describes an AI system that adapts and improves its behavior over time through learning from data and experience, which is a key characteristic of context-aware/adaptive AI systems versus static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Algorithmic decision-making with optimization","","","A multi-armed bandit (MAB) algorithm was employed to choose the best controller to use. MAB estimates the reward distribution for each arm from multiple trials, while minimizing the time cost imposed for requesting the human for demonstrations or for needing the human for failure recovery.","This represents an AI feature as it involves a data-driven algorithm (MAB) that makes decisions based on estimated rewards from trials, which is characteristic of AI systems' capability to optimize choices through learning from data, contrasting with conventional automation's fixed rules.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Learning from Demonstration with Behavior Cloning and Q-filter","","","LfD was integrated into the DDPG framework with behavior cloning and a Q-filter. Demonstration of episodes by the controllers was added to a demonstration replay buffer. An additional behavior cloning loss term was then applied only to the samples of this buffer. The Q-filter ensured that the loss was not applied when the learned policy was significantly better than the demonstrated policy.","This represents an AI capability involving data-driven learning from demonstrations, adaptive policy improvement, and mechanisms to handle uncertainty in decision-making, aligning with novel AI characteristics like non-deterministic behavior and context-aware adaptation in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Inverse Reinforcement Learning robustness","","","Two works employed instead Inverse Reinforcement Learning (IRL). IRL can be more robust to changes in the tasks compared to Deep RL and learning from LfD, due to the difficulty of designing a suitable reward function.","This represents a general AI capability, specifically IRL, which is highlighted for its robustness to task changes, indicating adaptive and data-driven decision-making characteristics in AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Online learning from multiple demonstrations","","","a control policy learned online as described above Cl. The empirical results from the comparison with other approaches showed that the method led to a reduction in the total human cost. The results also showed that a better policy was learned using demonstrations from different sources, indicating a potential future application in learning from multiple dissimilar controllers.","This represents context-aware/adaptive behavior as it involves a control policy that learns dynamically (online) and adapts by learning from multiple dissimilar controllers, demonstrating real-time learning and variability handling.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Collaborative Intelligence","","","This type of collaborative intelligence can be likewise important to ensure the systems are functioning properly in production environments, according to the safety requirements, and respecting social and ethical guidelines.","This excerpt describes a key AI capability where human-in-the-loop assistance enables better performance and ensures systems function properly in production environments while adhering to safety, social, and ethical guidelines, which aligns with novel AI characteristics in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Learning variable impedance control and reward functions via adversarial IRL","","","Ref. [ 136] used demonstrations and IRL to learn variable impedance control and reward functions for contact-rich manipulation tasks. The paper investigated the best action space for the reward function: whether rewards are defined for the force or impedance gain. Specifically, adversarial Inverse Reinforcement Learning was used to learn the impedance gain-based expert policy and reward function (the reward function was learned with the discriminator and the variable impedance policy was the generator), which was tested in a simulated and a real industrial robot experiment, achieving better transfer performance than the baselines.","This represents an AI capability involving data-driven learning (IRL) to acquire control policies and reward functions, which is characteristic of AI systems as opposed to conventional automation that relies on pre-programmed rules.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Human-in-the-loop Inverse Reinforcement Learning for complex task learning","","","In [ 129], complex robot tasks were learned using a Human-in-the-loop Inverse Reinforcement Learning (HI-IRL) framework. The approach was more efficient than traditional IRL methods, and involved structured learning from failure experiences, from critical sub-goal information provided by an expert and partial demonstrations of sub-tasks that the agent struggled to learn.","This represents an AI capability that involves adaptive learning from human feedback (expert information, partial demonstrations) and failure experiences, which is a novel AI characteristic enabling more efficient and context-aware learning compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Human-in-the-loop assistance during deployment","","","Systems may also be designed to exploit human capabilities and assistance under normal operating conditions, beyond the learning phase.","This describes an AI system feature where human capabilities are integrated into system operation beyond initial learning, representing a novel interaction characteristic compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Sensor-based emergency detection with human assistance","","","In [138], hands-free detection of emergencies in HRC scenarios was achieved by utilizing the workers’ sensing abilities, where humans indirectly assist in emergency prediction by using data from a mobile Electroencephalogram (EEG) sensor as input.","This represents an AI capability that leverages human physiological data (EEG) as input for emergency prediction in collaborative scenarios, demonstrating a novel data-driven approach to safety monitoring that integrates human sensing abilities.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: AI model-based system for visual grounding","","","The approach used a Faster R-CNN model [141] for object detection, a transformer-based text encoder model for text description embedding, and a text classification model for scoring candidate locations in regard to the text descriptions.","This excerpt directly describes AI capabilities (object detection, text embedding, classification) used in a robotic system for human-robot communication, which aligns with the research focus on AI/autonomous system characteristics in high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Real-time iterative learning through human cooperation","","","The solution proposed by Yang et al. (2020) [ 44], already presented in Section 3.1.1, can be partially categorized as human assistance during the deployment of an ADAS system that learns iteratively in real-time the desired path through repeated cooperation with the driver, but after sufficient iterations, the system takes over to assist the human driver.","This excerpt explicitly mentions an AI system (ADAS) that 'learns iteratively in real-time' and adapts its behavior through 'repeated cooperation with the driver,' demonstrating context-aware/adaptive behavior where the system dynamically responds to the environment and human interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic and flexible adaptation to operational variations","","","Dynamic and flexible approaches, such as the one by Zhu et al. (2019) [ 40], can better deal with variations in the expected operations but are usually harder to implement and test for compliance with safety requirements.","It directly mentions 'Dynamic and flexible approaches' that 'can better deal with variations in the expected operations', aligning with the context-aware/adaptive behavior characteristic where AI systems respond dynamically to environmental changes, though it does not explicitly reference real-time learning or broader AI paradigms.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Quantifying and transmitting model uncertainty","","","the integration of different system modules should account for how errors propagate downstream from the initial perception component to the final decision component, requiring the use of AI methods and metrics to quantify model uncertainty and transmit it through the system.","This directly relates to non-deterministic/data-driven decision-making, as it involves quantifying model uncertainty—a key aspect of AI systems where outputs vary with data/model states, leading to uncertainty and unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Model safety, robustness, and dependability methods","","","the already existing industrial standards for system safety and methods specific for model safety, robustness, and dependability should be used at the different stages of model development.","This represents an AI feature as it discusses specific methods for ensuring safety, robustness, and dependability in AI models, which are key characteristics distinguishing AI systems from conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Continual learning for post-deployment variability","","","Future research should also focus on developing novel methods to deal with unaccounted variability in the data after deployment, such as continual learning.","This represents an AI feature because it describes the need for systems that can adapt to dynamic changes in data after deployment, which is a characteristic of context-aware or adaptive AI systems, as opposed to static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Adaptation and Personalization of Models","","","One possible future work line for human motion intention prediction is the adaptation and personalization of the models to specific subjects when it becomes possible to have access to large amounts of big data that physiological sensors can provide.","This represents an AI feature because it describes the adaptation and personalization of models to specific subjects, which is a characteristic of context-aware and adaptive AI systems that can dynamically respond to individual users or environments, aligning with the research focus on novel AI characteristics such as context-aware/adaptive behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: real-time robot behavior adaptation","","","could add a worker physical state recognition module for real-time robot behavior adaptation or to improve the ergonomic design of collaborative systems.","This represents an AI feature as it involves dynamic response and adaptation to real-time human state data, aligning with context-aware/adaptive behavior characteristics in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: adaptation | Feature: Behavioral indicators for continuous learning","","","behavioral indicators of the state of interest can be used instead (as in [ 96]) for continuous learning or model performance assessment.","This represents an AI feature as it describes using behavioral indicators to enable continuous learning or assess model performance, indicating an adaptive, data-driven system that can evolve based on real-time inputs, which is characteristic of context-aware AI behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Model-derived safety metrics for autonomous policy","","","developing human-inspired safety metrics that can be retrieved from the trained models and serve as a prior for an autonomous driving policy.","This represents an AI feature as it involves using trained models (implying data-driven learning) to generate safety metrics that guide autonomous decision-making in driving, highlighting a non-deterministic, model-dependent approach to safety in AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Generalization and intervention determination","","","learn to generalize to novel scenarios, estimate the suitability to the new scenario, and determine when it needs human assistance/intervention [131,143].","This excerpt mentions 'learn to generalize to novel scenarios' and 'determine when it needs human assistance/intervention', which are adaptive features that handle variability and dynamic decision-making, characteristic of AI systems rather than conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Human behavior estimation and task adjustment","","","estimate and match the human’s expected behavior of the robot. The exploration of this problem will be key to develop systems that can resolve conflicts between humans and machines, reach a common ground, and adjust tasks and goals accordingly [9].","This excerpt describes systems that 'estimate and match the human’s expected behavior' and 'adjust tasks and goals accordingly', which are adaptive behaviors that respond to human context and variability, a key feature of advanced AI systems versus fixed automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Dynamic/adaptive planning and online learning","","","dynamic/adaptive planning methods, online learning for the continuous improvement of the models, personalized assistance to the human collaborator, and more natural communication channels between human and robots.","This excerpt directly mentions 'dynamic/adaptive planning methods' and 'online learning for the continuous improvement of the models', which are context-aware/adaptive behaviors that enable systems to respond dynamically to environments and improve continuously, contrasting with static conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Real-time confidence/doubt estimation","","","by estimating, during real-time deployment, the confidence/doubt of the learned policy model relative to the execution risk, further improving safety [24].","This excerpt involves 'estimating, during real-time deployment, the confidence/doubt of the learned policy model relative to the execution risk', which relates to non-deterministic/data-driven decision-making by addressing uncertainty and model-dependent states in AI systems.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Explainability requirement for self-evolving autonomous systems","","","any machinery, or related product with self-evolving behavior or logic that operates with varying levels of autonomy should communicate its planned actions (such as what it is going to do and why) to operators in a comprehensible manner.","This directly addresses the opacity/explainability characteristic by requiring autonomous systems to communicate their planned actions and reasoning in a comprehensible way to human operators.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Explainability method for AI oversight","","","methods from the explainable AI domain [ 146] and human–machine interaction domain [ 58] should be integrated and adopted. As an example, the presented work of Kottinger et al. (2022) [ 114] provided an explainability method for robot trajectory planning oversight, by generating human-understandable path visualizations.","This directly addresses the opacity/explainability characteristic of AI systems by mentioning 'explainable AI domain' and providing a specific example of an 'explainability method' that generates 'human-understandable path visualizations' for robot trajectory planning oversight, which helps make black-box AI behavior more transparent and interpretable.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: opacity | Feature: Explainable Systems Development","","","Research has mainly focused on collaboration and adaptation, but more work is needed to develop explainable systems that take into account the ethical, legal, and societal responsibilities, as well as the consequences of these systems to better manage conflicts between intelligent systems and human experts.","This excerpt specifically addresses the need for explainable AI systems (addressing opacity/explainability) and considers their broader responsibilities and consequences, which are key novel AI characteristics.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: Hybrid Intelligent System Features","","","The challenges of building Hybrid Intelligent Systems include collaboration between humans and machines, adaptation to humans and the environment, systems that behave responsibly, and systems that can explain and share knowledge and strategies with each other [ 9].","This excerpt directly lists characteristics of AI systems (Hybrid Intelligent Systems) including adaptation and explainability, which are novel AI features compared to conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: non_deterministic | Feature: Uncertainty communication","","","Alternatively, the estimate of the system’s confidence/uncertainty on their outputs can be communicated to the human stakeholders to support informed decision-making [109,110].","This directly addresses non-deterministic/data-driven decision-making by highlighting the system's variable confidence/uncertainty in outputs, which is a key characteristic of AI systems that produce outputs varying with data/model states, leading to uncertainty and unpredictable failures.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: AI_capability | Feature: multi-human multi-context interaction","","","An aspect of collaborative intelligence that was missed in the reviewed works is the fact that in many cyber–physical systems, the intelligent machine might have to interact with many humans in different contexts, and the humans might interact with different system instantiations, sharing the same knowledge base [ 2].","This represents an AI feature as it highlights the capability of intelligent machines to engage in complex, context-aware interactions with multiple humans, which is a novel characteristic compared to conventional automation that typically involves fixed, deterministic interactions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: context_adaptive | Feature: Proactive context-adaptive collaboration","","","true collaboration between humans and AI-based systems that is proactive and involves purposeful interactions, adaptation to context, and an understanding of each other’s actions towards cooperative learning and problem-solving [9].","This excerpt directly mentions 'adaptation to context' as a characteristic of AI-based systems, which aligns with the research focus on context-aware/adaptive behavior in novel AI systems versus conventional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","Category: automation | Feature: Human-Machine Teaming for Flexible Automation and Safety Supervision","","","The review focused on industrial applications that can benefit the most from human–machine teaming to achieve flexible and cost-effective automation solutions, and safety-critical industries that require human supervision as an additional safety layer for highly automated systems.","This excerpt directly discusses AI/automation features: human-machine teaming enables flexible and cost-effective automation solutions, and human supervision serves as an additional safety layer for highly automated systems in safety-critical contexts, which are key characteristics in human-AI interaction for high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: behavioral_changes | Severity: 8","","operator loss of expertise, reduced vigilance and situational awareness, complacency, reduced adaptability, or information overload [ 1].","Severity Justification: These degradations are directly linked to disastrous consequences such as the Air France flight 447 accident, indicating high severity in safety-critical contexts. | Relevance Justification: The excerpt explicitly mentions multiple human performance degradations (loss of expertise, reduced vigilance and situational awareness, complacency, reduced adaptability, information overload) that are central to the research focus on novel AI-related issues versus traditional automation.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: situational_awareness | Severity: 6","","the weak awareness of humans’ mental model and their expected robot behavior, a key ability for trust and reduction in safety risks [ 41].","Severity Justification: The issue is explicitly linked to trust and safety risks, indicating moderate severity as it can impact operational safety and human-AI collaboration. | Relevance Justification: This directly addresses a novel AI-related degradation (context-aware/adaptive behavior leading to unpredictability) by highlighting a gap in human awareness of robot behavior, which is crucial for trust and safety in high-risk interactions.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: trust_issues | Severity: 4","","An explainability metric can be computed or trained to assess the generated cobot motion plans, promoting the situational awareness and trust of the worker.","Severity Justification: The text indirectly suggests trust and situational awareness are concerns that need promoting, but it does not explicitly describe severe degradation or negative outcomes. | Relevance Justification: The text relates to trust and situational awareness in human-AI interaction, which are key aspects of human performance degradation, though it frames it positively as something to promote rather than a direct problem.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: situational_awareness | Severity: 7","","However, the lack of explanation or transparency of the system’s autonomous actions can affect the situational awareness of the human, and cause overtrust or distrust towards the machine [ 5].","Severity Justification: Directly links system opacity to reduced situational awareness and trust problems, which are significant performance degradations in human-AI interaction. | Relevance Justification: Directly addresses human performance degradation (situational awareness and trust) related to novel AI characteristics (opacity/lack of explainability).","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: cognitive_overload | Severity: 7","","Human–machine communication and interaction performance can be impaired by the cognitive biases and expectations underlying the human subjective experience, further impacted by time pressure, fatigue, and stress [ 5].","Severity Justification: The excerpt directly identifies performance impairment due to cognitive biases and stressors, which are significant factors in human performance degradation, especially under time pressure and fatigue in high-risk contexts. | Relevance Justification: This directly addresses human performance degradation in interaction with systems, mentioning cognitive biases and stressors that align with the research focus on novel AI characteristics impacting human performance.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: behavioral_changes | Severity: 6","","Humans can be considered non-stable, non-linear, and complex agents, affected by fatigue and other organizational and personal factors; therefore, the quality of human input should take this into account [5].","Severity Justification: The degradation is explicitly stated as affecting human input quality due to inherent human variability and factors like fatigue, which are significant but not catastrophic performance issues. | Relevance Justification: This directly addresses human performance degradation in the context of human-AI interaction, highlighting factors that impair human input quality, which is highly relevant to the research focus on novel AI characteristics.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: performance_metrics | Severity: 7","","prevent critical situations when the operator/user is not performing at the required level, and can cause harm to himself or others.","Severity Justification: The degradation involves not performing at the required level, which can lead to harm to self or others, indicating a significant safety risk. | Relevance Justification: This directly describes a human performance degradation (not performing at required level) in the context of AI/autonomous systems (human state recognition for robot behavior adaptation), aligning with the research focus on high-risk industries.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","Category: automation_bias | Severity: 6","","when the intelligent system detects human decision-making bias or model disparities between the human and system,","Severity Justification: Detection of bias suggests it is a recognized problem that systems need to address, indicating moderate severity as it can impact decision-making but is being monitored. | Relevance Justification: Directly mentions 'human decision-making bias', which is a key cognitive bias in human-AI interaction, aligning with the research focus on novel AI characteristics like non-deterministic decision-making and opacity.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: improper integration of systems | Evidence Type: direct | Causal Strength: 8 | Performance Effect: operator loss of expertise, reduced vigilance and situational awareness, complacency, reduced adaptability, or information overload","Failure in the proper integration between them has previously led to disastrous consequences (such as the accident of Air France flight 447 in 2009) due to operator loss of expertise, reduced vigilance and situational awareness, complacency, reduced adaptability, or information overload [ 1].","Causal Strength Justification: Direct causal language 'led to... due to' explicitly connects system integration failure to multiple human performance effects. | Relevance Justification: Directly addresses how system integration issues cause human performance degradation, though not specifically tied to novel AI characteristics mentioned in research focus.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: cooperative impedance fuzzy-controller and feedforward neural network | Evidence Type: direct | Causal Strength: 8 | Performance Effect: minimizes human effort/physical stress and maintains trajectory smoothness","Ref. [53] developed a cooperative impedance fuzzy-controller to optimize human–robot cooperation in heavy load lifting tasks, by mapping the interaction force, the derivative of the interaction force, and the end-effector velocity to a fuzzy impedance assistance level, followed by the use of a feedforward neural network to select the best assistance level that minimizes human effort/physical stress and maintains trajectory smoothness.","Causal Strength Justification: Direct causal language: 'to select the best assistance level that minimizes human effort/physical stress' explicitly states the AI feature causes the reduction in human effort/stress. | Relevance Justification: Directly addresses human performance (effort/physical stress) in a cooperative task, though not explicitly linking to the novel AI characteristics (non-deterministic, opacity, adaptive) from the research focus.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: lack of explanation or transparency of the system’s autonomous actions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: affect the situational awareness of the human, and cause overtrust or distrust towards the machine","However, the lack of explanation or transparency of the system’s autonomous actions can affect the situational awareness of the human, and cause overtrust or distrust towards the machine [ 5].","Causal Strength Justification: Direct causal language ('can affect' and 'cause') is used to link the AI feature to specific human performance effects. | Relevance Justification: Directly addresses how opacity (lack of explanation/transparency) in AI systems causally impacts human situational awareness and trust, which are key performance factors in human-AI interaction.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: safety measures in CI systems | Evidence Type: mechanism | Causal Strength: 8 | Performance Effect: assistance of human workers","Within this category of CI systems, the safety measures implemented are mainly focused on the assistance of human workers by reducing the workload and by safety monitoring (as in the case of autonomous collision avoidance).","Causal Strength Justification: Direct causal language using 'by reducing' and 'by safety monitoring' explicitly links the AI feature to the human performance effect. | Relevance Justification: Directly addresses how AI features (safety measures) affect human performance (assistance through workload reduction and monitoring), though not specifically tied to the three novel AI characteristics listed in the research focus.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: HMM modeling of human mental model of reward function | Evidence Type: direct | Causal Strength: 8 | Performance Effect: more successful games and a more positive user experience","Using a HMM to model the human mental model of the reward function, the viability and effectiveness of the method were tested with a joint-execution collaborative game with a robot, leading to more successful games and a more positive user experience than the control condition.","Causal Strength Justification: Direct causation is explicitly stated with 'leading to', linking the AI method to specific human performance outcomes. | Relevance Justification: The excerpt directly connects an AI method (HMM modeling) to human performance effects (game success and user experience), though it does not explicitly tie to the specified novel AI characteristics like non-deterministic, opacity, or adaptive behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: context-aware ADAS system | Evidence Type: direct | Causal Strength: 10 | Performance Effect: increase of 94.28% in safety and 20.97% improvement in driving experience","The proposed driver-in-the-loop context-aware ADAS system led to an increase of 94.28% in the safety of the driver and a 20.97% improvement in the driving experience.","Causal Strength Justification: Direct causation with explicit causal language ('led to') and quantitative outcomes. | Relevance Justification: Directly addresses how an adaptive AI feature (context-aware system) causally affects human performance (safety and experience).","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: Advisory Speed Assistance System (ASA) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduction of 53% in speed error variance in simulated driving and a reduction of 20.00% of speed error variance in real vehicle experiments","The developed Advisory Speed Assistance System (ASA) led to a reduction of 53% in speed error variance in simulated driving and a reduction of 20.00% of speed error variance in real vehicle experiments.","Causal Strength Justification: Direct causal language 'led to' explicitly connects the AI system to the performance effect. | Relevance Justification: The text explicitly shows how an AI system (ASA) causally affects human performance (speed error variance reduction), though it does not directly map to the specific novel AI characteristics listed (non-deterministic, opacity, adaptive).","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: human oversight RL approach | Evidence Type: direct | Causal Strength: 7 | Performance Effect: reduction in catastrophes","Compared to a baseline RL approach with large negative rewards applied to catastrophic actions (which can suffer catastrophic forgetting), the human oversight RL approach performed much better at reducing catastrophes.","Causal Strength Justification: Direct causal language ('performed much better at reducing') explicitly links the human oversight approach to improved outcomes in reducing catastrophes, with a comparative mechanism. | Relevance Justification: Directly addresses how human oversight (a human-AI interaction element) causally affects system performance by reducing catastrophes, though not explicitly tied to novel AI features like non-deterministic or adaptive behavior.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: human feedback through demonstrations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improved efficiency of learning process and increased safety","Human feedback, particularly through demonstrations, can improve the efficiency of the learning process in terms of the number of samples required, compared to classical reinforcement learning, and be safer due to the interventions and corrective actions that the humans can provide in real-time [123].","Causal Strength Justification: Direct causal language ('can improve', 'be safer due to') explicitly links human feedback to specific positive effects on learning and safety. | Relevance Justification: Directly addresses how human involvement (a key aspect of human-AI interaction) causally affects system performance (efficiency and safety), though not specifically tied to non-deterministic, opacity, or adaptive AI features.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: model-free agents learning with trial and error | Evidence Type: direct | Causal Strength: 9 | Performance Effect: avoidance of catastrophes","In particular, for model-free agents that can only learn with trial and error, having human intervention is the only way to avoid catastrophes when the agent has not learned yet.","Causal Strength Justification: Direct causal language ('the only way to avoid') explicitly links human intervention to preventing catastrophes, with a clear mechanism (trial-and-error learning). | Relevance Justification: Directly addresses how a non-deterministic AI feature (trial-and-error learning in model-free agents) necessitates human intervention to prevent negative outcomes, linking AI characteristics to human performance effects.","y"
"Collaborative Intelligence for Safety-Critical Industries: A Literature Review","","","AI Feature: control policy learned online | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduction in total human cost","The empirical results from the comparison with other approaches showed that the method led to a reduction in the total human cost.","Causal Strength Justification: Direct causation with 'led to' linking the method to a specific effect, supported by empirical results. | Relevance Justification: Directly links an AI method (learned control policy) to a human performance effect (reduced cost), though not explicitly tied to non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Explainable AI (XAI) decision support","","","In many existing human + AI systems, decision-making support is typically provided in the form of text explanations (XAI) to help users understand the AI’s reasoning.","This directly describes an AI system feature - the use of XAI text explanations as an interface mechanism to support human decision-making by making AI reasoning more transparent.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI as decision-support tool","","","The emergence of artificial intelligence (AI) systems offers promising new tools to support these frequent and critical decisions [6, 7, 8, 9]. In particular, generative AI shows potential to help people with diabetes in meal planning to better control blood sugar levels [10, 11, 12, 13, 14].","This directly mentions AI systems as tools for decision-making, which is a core AI capability relevant to high-risk industries like healthcare, aligning with the research focus on AI vs. conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Explainable AI (XAI)","","","Even with the growing emphasis on explainable AI (XAI) [31, 32], providing explanations alone has not consistently mitigated trust issues [33].","This directly references explainable AI (XAI), which relates to the opacity/explainability characteristic of novel AI systems, as it involves making AI behavior more transparent and interpretable to address trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Human feedback and AI-driven questions","","","mechanisms like human feedback and AI-driven questions encouraged deeper reflection, but they may have resulted in decreased task performance, likely due to increased cognitive effort and heightened scrutiny, which negatively impacted trust.","This excerpt mentions AI-driven questions and human feedback as mechanisms that affect task performance and trust, representing feedback-related AI features discussed in the chunk.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI reasoning clues provision","","","mechanisms such as AI CLs, text explanations, and performance visualizations significantly enhanced human-AI collaborative task performance, as measured by decision accuracy, and improved trust when AI reasoning clues were provided.","This excerpt describes AI capabilities (confidence levels, explanations, visualizations) that provide reasoning clues to enhance human-AI collaboration and trust, representing specific AI features mentioned in the chunk.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Visual explanations and XAI design","","","Simple mechanisms like visual explanations did not significantly improve trust, highlighting the need for a contextual and balanced approach between CFF and XAI design with interactivity, decision frequency, and task complexity.","This excerpt discusses visual explanations and XAI design as part of AI interface features, addressing how they impact trust and require contextual balancing with other factors.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Explainable AI (XAI) for transparency","","","XAI (Explainable AI) and CFFs (Cognitive Framework Features). XAI focuses on enhancing the transparency of AI systems and is further divided into three categories based on their presentation modality: (1) Textual XAI, (2) Visual XAI, and (3) Multimodal XAI.","This directly addresses the opacity/explainability characteristic by focusing on enhancing transparency in AI systems, which is a novel feature compared to conventional automation that often lacks such mechanisms.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: interpretable model reasoning","","","Explanation mechanisms with a balanced Explanation Information Load (EIL) that provide interpretable model reasoning support for effective human-AI collaboration by enhancing decision-making and facilitating trust calibration without overwhelming users.","This directly addresses opacity/explainability by describing mechanisms that provide interpretable reasoning to enhance transparency and trust in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Interpretability enhancement for AI outputs","","","Complexity-Reduction CFFs: Reduce cognitive load by making AI outputs easier to interpret (e.g., AI confidence levels (CLs) that transparently communicate the model’s uncertainty or performance visualizations that show AI task performance over time).","This addresses opacity/lack of explainability by describing features that make AI outputs easier to interpret, which helps mitigate the black-box nature of AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Human feedback mechanisms for AI validation","","","Decision-Constraint CFFs: Allow users to intervene in decision-making processes (e.g., human feedback mechanisms that enable validation of AI outputs).","This represents a feedback mechanism that enables human oversight and validation of AI outputs, which is a feature relevant to human-AI interaction in high-risk contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: Uncertainty communication via confidence levels","","","AI confidence levels (CLs) that transparently communicate the model’s uncertainty or performance visualizations that show AI task performance over time). Engaging with AI","This directly addresses non-deterministic/data-driven decision-making by explicitly mentioning AI model uncertainty, which is a key characteristic of novel AI systems where outputs vary with data/model states.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-assisted decision-making with explanation mechanisms","","","decision-making w/ AI assistance but w/o any explanation) and between phase 1 (P1: decision-making w/o AI assistance) to phase 3 (P3: decision-making w/ AI assistance accompanied by a decision-support mechanism) using the Wilcoxon signed-rank test [47], a non-parametric test suitable for analyzing paired data and comparing these engagement rates.","This excerpt directly mentions AI assistance in decision-making processes and specifically references a decision-support mechanism, which relates to AI capabilities in supporting human decisions, though it does not explicitly detail novel characteristics like non-determinism or opacity.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI as authoritative figure in decision-making","","","Users show a tendency to rely on AI suggestions, even when the AI is wrong. Such over-reliance can be explained by cognitive bias, where users may perceive the AI system as an authoritative figure, especially in tasks requiring complex decision-making.","This describes a key AI capability where users treat AI as an authoritative source, influencing decision-making behavior and trust dynamics, which is a novel characteristic compared to conventional automation that typically follows predefined rules without such perceived authority.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Trust reinforcement and over-reliance feedback loop","","","When AI is correct, users demonstrate a greater degree of trust, suggesting that accurate AI assistance can reinforce user confidence in the system. However, when AI is incorrect, the lack of significant correction by the users indicates potential risks in over-reliance.","This highlights a feedback mechanism where AI accuracy reinforces user trust, but errors lead to over-reliance without correction, representing a dynamic interaction feature distinct from conventional automation's more predictable feedback loops.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Visual Explanations of AI Attention","","","Condition 2: Visual Explanations. Visual explanations (C2) include a basic visual explanation of the AI’s attention without any additional feedback mechanism. The results do not show any significant engagement improvement ( p= 0.776), suggesting that visual explanations alone may not be sufficient to encourage deeper engagement.","This represents an AI feature related to opacity/explainability, as it discusses visual explanations of the AI's attention, addressing transparency and interpretability in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Confidence levels as transparency mechanism","","","CL is a form of transparency that provides users with direct insight into the AI’s certainty, which encourages users to critically compare their judgment with the AI’s.","This directly addresses opacity/explainability by providing a form of transparency (CL) that mitigates black-box behavior, allowing users to understand the AI's certainty and compare judgments, which is a novel AI characteristic versus conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Trust enhancement through explainability","","","By comparing the pre- and post-questionnaire results, we find a significant improvement in system reliability ( p= 0.048), suggesting that such explanations can improve user’s trust in the AI system.","This shows how addressing opacity (via explanations) enhances trust in AI systems, a feature critical for AI adoption in high-risk industries compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Explainability via textual explanations","","","Condition 1: Textual Explanations. Textual explanations (C1), as our baseline condition, includes detailed textual explanations of the AI’s decision-making process. There is a significant increase in engagement ( p= 0.049), indicating that textual explanations help users to understand AI decisions through a logical structure.","This directly addresses opacity/explainability by describing how textual explanations make AI decision-making more transparent and understandable to users, which is a key feature distinguishing AI from conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Human Feedback Mechanism","","","Condition 4: Human Feedback. Similar to AI CLs (C3), human feedback (C4) allows users to input their CLs. It shows a significant increase in engagement ( p= 0.006), suggesting that allowing users to input their own thinking encourages introspection and deep cognitive involvement in decision-making.","This describes a feedback mechanism where users can input their own confidence levels into an AI system, which is a characteristic of interactive AI systems that involve human-AI collaboration in decision-making processes.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-generated reflective questioning","","","AI-driven questions (C5) include AI-generated questions to prompt users to reflect on AI’s suggestions. The results show a borderline significance ( p= 0.058), indicating that while AI-generated reflective questions encourage some degree of engagement, they are not as effective as other conditions.","This represents an AI capability where the system autonomously generates questions to interact with users, specifically to prompt reflection on its own suggestions, which is a novel feature compared to conventional automation that typically follows fixed scripts or rules without such adaptive prompting.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Performance visualization for AI-human comparison","","","Performance visualization (C6) includes a visualization of the AI and user’s task performance over time. It shows a significant increase in engagement ( p= 0.009), suggesting that the visualized comparison of human and AI performance contributed to a reflective decision-making process. Perhaps it Engaging with AI allows users to contextualize AI suggestions based on empirical evidence, forcing the user to evaluate AI outputs more deliberately.","This represents an AI feature as it involves a visualization interface that displays AI performance alongside human performance, enabling users to engage with and evaluate AI outputs more deliberately, which is a novel characteristic in human-AI interaction compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: explainability mechanisms","","","AI explanation mechanisms are designed to provide transparency and interpretability","This directly addresses the opacity/explainability characteristic by describing AI systems designed to provide transparency and interpretability through explanation mechanisms.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: interpretable reasoning with confidence","","","These mechanisms provide users with interpretable model reasoning that conveys the AI’s rationale and confidence, respectively.","This describes AI features that address opacity by providing interpretable model reasoning that conveys both rationale and confidence, which are key aspects of explainable AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: AI system support interface without explanatory or feedback features","","","In textual explanations (C1), participants interact with the AI system that provides support but without any additional explanatory mechanisms, feedback loops, or engagement-enhancing features.","This describes the interface characteristics of the AI system, specifically its support function and the absence of explanatory and feedback mechanisms, which are key aspects of human-AI interaction design.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Interpretable model reasoning and explanation mechanisms","","","These results suggest that EIL and the presence of interpretable model reasoning are important for effective human-AI collaboration. Mechanisms that balance a manageable EIL with clear reasoning cues can improve collaboration performance. In contrast, mechanisms that lack interpretive support or impose high cognitive demands may hinder performance, highlighting the importance of designing AI explanation mechanisms that offer clear interpretive insights aligned with the user’s cognitive capacity.","This directly addresses the opacity/explainability characteristic by highlighting the need for interpretable reasoning and clear explanation mechanisms to mitigate black-box behavior and enhance trust in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Trust calibration through explainability","","","It shows that explanations can calibrate user trust on AI systems and help them assess AI suggestions more critically.","This relates to opacity/explainability by showing how explanations mitigate trust challenges by allowing users to critically assess AI outputs, addressing the lack of transparency in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Explainability via textual transparency","","","Textual explanation as a form of transparency helps users better understand the rationale behind AI decisions, building users’ confidence in the AI’s outputs.","This directly addresses the opacity/explainability characteristic by describing how textual explanations make AI decision-making more transparent and understandable, contrasting with black-box behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI accuracy perception and variability","","","they do not universally improve user perceptions of the AI’s accuracy. The variability in post-study accuracy ratings (Std= 1.07) indicates a wide range of user experiences, with some participants finding the explanations helpful to assess AI accuracy, while others do not.","This represents an AI feature as it directly addresses user perceptions of AI accuracy, a key aspect of AI system evaluation and trust, with variability indicating potential non-deterministic or context-dependent outcomes in user interactions.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: explanation-mental model alignment","","","We can also find there is a mismatch between the user’s mental model of the AI and the actual usefulness of the explanations provided. If explanations do not align well with user mental models, they may lead to over-reliance or skepticism toward the AI system.","This relates to the opacity/explainability characteristic by highlighting how the effectiveness of explanations in AI systems depends on alignment with user mental models, impacting trust and reliance—critical factors in high-risk applications where unpredictable behavior or lack of transparency can pose risks.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: explainability design trade-off","","","The results highlight a key challenge in AI design: while explanations can increase reliability, they should be presented in a way that does not increase cognitive processing load to the point where trust is negatively affected.","This directly addresses the opacity/explainability characteristic by discussing how explanations (aimed at reducing opacity) in AI systems must be carefully designed to manage cognitive load and trust, which are key challenges in human-AI interaction for high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of transparency/interpretability","","","However, this gain in reliability does not translate into a corresponding increase in trust, as shown in the next subsection, pointing toward potential limitations in how trust is developed in AI systems without transparency or interpretability.","This directly addresses the opacity/explainability characteristic by mentioning that trust is not developed in AI systems without transparency or interpretability, which relates to black-box behavior and trust/regulation challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Transparency and interpretability affecting trust","","","Trust in AI systems depends not only on how reliable the system appears but also on how transparent and interpretable the system is, and how much agency the user has.","This directly addresses the opacity/explainability characteristic by highlighting the importance of transparency and interpretability in AI systems, which are key factors in trust and regulation challenges as mentioned in the research focus.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Visual transparency enhancement","","","In visual explanations (C2), participants interact with an AI system that enhances transparency by highlighting key reasoning areas within the visual input and labeling them, directly mapping the AI’s decision-making process and the visual evidence.","This addresses opacity/explainability by making the AI's internal reasoning visible through visual explanations, which helps users verify and contextualize the AI's decisions, contrasting with conventional automation's typical lack of such transparency features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Interpretability and verification mechanisms","","","Text explanations are widely regarded as a means of improving AI interpretability [49, 50], while visual explanations offer an intuitive way for users to verify and contextualize the AI’s reasoning [51].","This highlights features aimed at reducing opacity and lack of explainability in AI systems, specifically through text and visual explanations that improve interpretability and enable user verification, which are novel characteristics not typically found in conventional automation systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: transparency via certainty levels","","","CL is a form of transparency that provides users with a clear sense of the system’s certainty in its outputs.","This relates to opacity/explainability by introducing a specific transparency mechanism (CLs) that helps mitigate black-box issues by providing certainty information.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of system transparency","","","the lack of system transparency may limit user ability to engage critically with the AI’s outputs with impact on perception of credibility and trust.","This directly addresses opacity/explainability, a novel AI characteristic, by highlighting the absence of transparency as a feature that impacts user trust and critical engagement.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Confidence Level Feedback Mechanism","","","By comparing the CLs of their performance with AI’s, it helps users make more informed decisions and adjust their reliance on the system.","This excerpt directly mentions a feature where AI systems output confidence levels (CLs) that users can compare with their own performance, facilitating informed decision-making and trust calibration, which aligns with feedback mechanisms in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: trust calibration through confidence reflection","","","Trust. Trust in the AI system slightly decreased after participants have to reflect on their own CLs and those of the AI. While this change is not statistically significant ( p= 0.417), the slight decline suggests that the process of reflecting on confidence might lead some users to become more critical of the AI’s suggestions and question the system’s accuracy when they think their confidence is not aligned.","This represents an AI feature related to feedback mechanisms, specifically how user interaction with confidence levels affects trust and perception of AI system accuracy, which is a novel aspect in human-AI interaction compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of explainability in AI logic","","","they may also lead users to question the AI’s recommendations more critically, potentially leading to a loss of trust when the AI’s logic does not align with the user’s expectations.","This excerpt directly addresses the opacity/explainability characteristic by highlighting that users question AI recommendations when the AI's logic isn't transparent or doesn't align with their expectations, which relates to trust challenges due to black-box behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-driven reflective questioning for decision support","","","AI-driven questions (C5) ( p= 0.008). The counterintuitive result suggests that the AI’s reflective questions may help users focus their decision-making process, thereby simplifying the task. By guiding users toward key considerations, the AI-generated questions might reduce cognitive overload, offering a structured framework for evaluating the AI’s recommendations.","This represents an AI capability where the system uses reflective questions to enhance human decision-making, a feature not typical in conventional automation, as it involves adaptive guidance and cognitive support.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI suggestion evaluation through user comparison","","","When users have opportunity to compare their own past performance with AI’s, they may become more critical of the AI’s suggestions, particularly if they identify discrepancies between their judgments and the AI’s.","This represents an AI feature as it involves AI's ability to provide suggestions that users compare with their own judgments, indicating AI's role in decision-making and user interaction, which is a key aspect of human-AI interaction in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of transparency in basic mechanisms","","","textual explanations (C1) demonstrates the highest confidence mean (mean = 6.000,std= 0.553), reflecting the fact that basic mechanisms, while lacking transparency, can still evoke strong confidence, perhaps due to simplicity.","This excerpt directly mentions 'lacking transparency,' which is a key characteristic of opacity/explainability in AI systems, as it refers to the black-box nature where internal reasoning is not visible, affecting user trust and understanding.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Transparency about AI certainty","","","CLs may help users perceive the AI system as more consistent, because CLs provide users with transparency about the AI’s certainty in its decisions.","This directly addresses the opacity/explainability characteristic by mentioning transparency about the AI's decision-making certainty, which helps mitigate black-box behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: Uncertainty exposure from reflective mechanisms","","","This indicates that while reflective mechanisms can prompt users to think critically, they may also expose users to uncertainties, reducing trust in the system.","This directly mentions 'uncertainties' as a characteristic of AI systems, which aligns with the non-deterministic/data-driven decision-making feature where outputs vary and lead to unpredictable outcomes, contrasting with conventional automation's deterministic behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Trust recalibration through AI confidence level comparison","","","AI CLs (C3) shows the highest trust change ( mean = 4.500,std= 1.607), suggesting that users recalibrate their trust based on comparing their CL with the AI’s CL.","This represents an AI-specific feedback mechanism where user trust is dynamically adjusted based on the AI's output (confidence levels), which is a characteristic of interactive AI systems that provide transparency about their certainty, unlike conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-generated explanations for decision-making","","","AI-generated explanations or CFFs were effective in helping improve human decision-making, they did not necessarily increase trust in the AI. This suggests that users may have treated these explanations as learning tools rather than as mechanisms for trust calibration.","This excerpt directly mentions AI-generated explanations, which are a feature of AI systems used to support human decision-making, contrasting with conventional automation that typically lacks such adaptive explanation capabilities.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Textual explanations for interpretability","","","Textual Explanations (C1) improve the interpretability of AI’s decisions by providing detailed reasoning. It significantly improves user performance and reliability, reflecting the value of detailed reasoning in helping users understand AI outputs.","This directly addresses the opacity/explainability characteristic by describing how textual explanations improve the interpretability of AI decisions, which helps mitigate black-box behavior and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Visual explanations for AI decision-making","","","Visual explanations (C2) provide users with an intuitive, graphical breakdown of AI’s decision-making process.","It directly relates to AI opacity/explainability by offering a graphical breakdown to enhance transparency and interpretability of AI decisions, which is a key novel characteristic compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Explainability through visual vs. textual methods","","","visual explanations can provide transparency and improve user satisfaction and trust when well-designed [57, 58, 59]. While other research has highlighted that they can be easily misinterpreted, particularly by lay users, leading to reduced comprehension compared to more detailed textual explanations [60, 61].","This directly addresses the opacity/explainability characteristic of AI systems by comparing visual and textual explanations, highlighting challenges in making AI decisions transparent and understandable to users, which is a key feature distinguishing novel AI from conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of explainability leading to trust issues","","","visual explanations alone may not be sufficient to enhance deeper engagement or trust, despite the task being visual. This is also reflected in user feedback, with one user noting ""I found that when I chose differently than AI, I sometimes was torn on whether or not I should switch."" Users may need more than visual explanations to meaningfully calibrate trust in AI outputs.","This excerpt directly addresses opacity/explainability by showing that visual explanations (a form of explainability) fail to build trust, with user feedback revealing internal reasoning not being transparent enough, causing uncertainty and trust calibration problems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI unreliability and trust issues","","","Another commented, “I felt like the AI was unreliable. ” , reflecting the observed decline in trust. In essence, while feedback mechanisms encourage active reflection, they must be carefully designed to avoid undermining trust or causing unnecessary complexity.","This excerpt directly mentions AI being unreliable, which relates to unpredictable failures and trust challenges, key aspects of novel AI characteristics like non-deterministic behavior and opacity in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Performance visualization for transparency","","","Performance visualization (C6) has been studied as a powerful mechanism for enhancing human-AI interactions. Research shows that visual aids can help users form mental models of AI behavior, influencing their trust and engagement levels. Such mechanisms help users justify their decisions in AI-assisted tasks by making the AI’s behavior more transparent [73, 74, 75, 76, 77, 78].","This represents an AI feature related to interface mechanisms (visual aids) that address opacity/explainability by making AI behavior more transparent, helping users form mental models and justify decisions, which aligns with the research focus on novel AI characteristics like opacity/lack of explainability in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Explicit AI Confidence Levels","","","Recent work emphasizes the critical role of making AI’s confidence explicit to enhance human-AI collaboration. Research shows that providing CLs allows users to better assess when and how much to rely on the AI’s suggestions [62, 63, 26, 27]. In our study, we observe a similar pattern: AI CLs (C3) significantly improve user performance,","This represents an AI feature related to uncertainty and decision-making, as it discusses making AI's confidence explicit to aid user judgment and performance, which is a novel characteristic compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Need for Explanations to Calibrate Trust","","","allowing users to make more informed judgments without feeling cognitively overwhelmed. However, the trust does not increase significantly, indicating perhaps the need for more comprehensive explanations to reasonably calibrate user trust.","This represents an opacity/explainability AI feature, as it indicates that confidence levels alone may not suffice for trust, highlighting a lack of transparency or explainability in AI systems that challenges user reliance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Insufficiency of Confidence Levels for Trust","","","This feedback reflects a broader issue highlighted in the literature: CLs alone may not suffice for deeper trust calibration. Users may require more explanations to decide whether or not to rely on the AI in complex decision-making scenarios [29, 19].","This represents an opacity/explainability AI feature, as it discusses the need for additional explanations beyond confidence levels to address trust issues in AI, pointing to challenges with explainability and transparency in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: Explicit AI Uncertainty","","","showing that making the AI’s uncertainty explicit helps users critically engage with its outputs. While trust does not significantly increase, the improved perceived accuracy demonstrates that CLs can act as effective cognitive scaffolds,","This represents a non-deterministic/data-driven AI feature, as it directly mentions AI's uncertainty, which relates to variable outputs and model-dependent decisions that are characteristic of novel AI systems versus conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-driven engagement with error awareness","","","However, our results show that while AI-driven questions (C5) encourage engagement, they also reduce trust and reliability. This phenomenon may occur because users become more aware of the AI’s potential errors or inconsistencies,","This represents an AI feature as it discusses AI-driven questions affecting user trust and reliability, highlighting AI's potential errors or inconsistencies, which relates to non-deterministic behavior and trust challenges in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: AI errors and unreliability in perception","","","as evidenced by participant feedback. One user remarked, ""It felt a bit unreliable when it kept saying that there was pizza on the plate despite there very clearly being no pizza on the plate."" Similarly, another noted, ""When I saw the AI incorrectly thought one plate had pizza, it made me want to pick the other one instead.""","This represents a non-deterministic AI feature as it describes AI making errors (e.g., incorrectly thinking pizza is present), leading to unpredictable failures and reduced trust, which aligns with variable outputs and uncertainty in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Reflective questions in human-AI interaction","","","Despite the decrease in trust, we observe a significant reduction in system complexity, indicating that reflective questions may streamline the decision-making process by focusing user attention on key aspects. However, it comes at the cost of trust, particularly when AI errors are highlighted during the reflective process.","This represents an interface feature as it discusses reflective questions as a mechanism in human-AI interaction that affects system complexity and trust, relating to feedback mechanisms and AI capabilities in decision-making processes.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: Performance visualization as cognitive aid","","","Performance visualization (C6) in our study demonstrates borderline significance in improving user performance, indicating that this mechanism may positively influence users to critically engage with the task. The improvements suggest that users reflected more deeply on the comparison between their own decisions and the AI’s, as evidenced by remarks like, ""I used it to reflect"" and ""The bar chart helped give me a visual ’faith’ in the AI."" The mechanism serves as a cognitive aid, helping users refine their judgments through the visual representation of performance.","This excerpt describes a specific interface mechanism (performance visualization) that facilitates human-AI interaction by enabling users to compare their decisions with AI outputs, which is a feedback mechanism characteristic of AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Confidence transparency for reliance calibration","","","The transparency of the AI’s confidence allows users to build reasonable reliance, and have a better alignment with the AI when it is correct and reduce reliance when it is incorrect.","This describes an AI-specific capability where the system provides quantifiable measures of its own certainty, which is a novel feature compared to conventional automation that typically lacks such transparency, directly supporting user interaction and trust calibration.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of direct model interpretability","","","Visualizing past performance may introduce additional complexity without direct model interpretability,","This directly references the opacity/explainability characteristic of AI systems, as it highlights the absence of interpretability in models, which is a key feature distinguishing novel AI from conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI-driven questioning for reliance calibration","","","AI-driven questions (C5) led to a significant improvement in reliance calibration from Phase 2 to Phase 3, showing users are more likely to follow the AI’s suggestions when they are correct and less likely to follow them when they are incorrect.","This describes an AI capability where AI-driven questions influence user reliance behavior, a feature of interactive AI systems in human-AI interaction contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: feedback | Feature: Reflective questioning for critical engagement","","","The reflective questions prompt users to think more critically about the AI’s suggestions, engaging them to question the AI’s reasoning [88].","This represents a feedback mechanism where reflective questions engage users to critically evaluate AI suggestions, enhancing human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Reliance calibration mechanisms","","","The results highlight the importance of reliance calibration mechanisms that go beyond surface-level transparency, offering users clear, actionable information or opportunities for active reflection.","This addresses the research focus on opacity/explainability by highlighting AI mechanisms (reliance calibration) that enhance transparency and interpretability beyond superficial levels, which is crucial for mitigating over-reliance and improving human-AI interaction in high-risk contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Transparency and interpretability mechanisms","","","mechanisms aimed at transparency and interpretability (textual explanations (C1) and AI CLs (C3)) and reflective engagement (AI-driven questions (C5)) are the most effective at mitigating over-reliance when the AI is incorrect.","This directly addresses the research focus on opacity/explainability by mentioning specific mechanisms (textual explanations, AI CLs, AI-driven questions) aimed at improving transparency and interpretability in AI systems, which helps users engage critically with AI outputs.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Visual explanation and performance visualization mechanisms","","","mechanisms such as visual explanations and performance visualizations may not provide sufficient guidance, leading to a higher risk of over-reliance when the AI has errors.","This relates to the research focus on opacity/explainability by discussing specific AI mechanisms (visual explanations, performance visualizations) that may fail to adequately address transparency and interpretability issues, potentially leading to over-reliance on erroneous AI outputs.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: transparency mechanisms for AI outputs","","","Conditions involving textual explanations (C1) and AI CLs (C3) show significant improvements in user accuracy and decision-making, as these mechanisms provide transparency and help users engage critically with AI outputs without overwhelming them.","This directly addresses opacity/explainability by describing how textual explanations and AI CLs provide transparency, which helps users understand and critically engage with AI outputs, reducing black-box behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: transparency and interpretability for reliance calibration","","","Previous research has similarly demonstrated that well-calibrated transparency and interpretability can build more effective reliance calibration, allowing users to rely on AI outputs more appropriately without falling into over-reliance or under-reliance traps [85, 90, 28, 91, 46].","This explicitly mentions transparency and interpretability as AI features that address opacity/explainability, enabling users to calibrate their reliance on AI outputs effectively, which is a key challenge in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: Uncertainty Communication","","","AI systems that explicitly communicate uncertainty can help users calibrate their reliance on AI outputs, fostering more accurate decision-making [96].","This directly addresses non-deterministic/data-driven decision-making by highlighting AI systems that communicate uncertainty, which relates to variable outputs and model-dependent decisions that can lead to unpredictable failures.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: Insufficient Transparency","","","The results suggest that surface-level transparency without deeper context may not be enough to guide users in critical decision-making scenarios.","This relates to opacity/explainability by indicating that lack of sufficient transparency (surface-level without deeper context) can hinder user guidance in decision-making, aligning with black-box behavior and trust/regulation challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Trust and reasoning challenges in AI design","","","Users find it more challenging to fully trust the AI’s reasoning [101, 27]. It shows a key tension in AI design: while reflective engagement can promote deeper critical thinking, it must be carefully calibrated to prevent over-scrutiny [102, 103].","This directly addresses AI-specific characteristics related to user trust in AI reasoning and design considerations for reflective engagement, which are novel compared to conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: explanation mechanisms for transparency and interpretability","","","explanation mechanisms that facilitate transparent, interpretable reasoning, such as textual explanations and AI CLs, improve decision-making accuracy by helping users better align their choices with AI suggestions.","This directly addresses the opacity/explainability characteristic by describing mechanisms (textual explanations, AI CLs) that facilitate transparent and interpretable reasoning, which helps mitigate black-box behavior and trust/regulation challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI systems for decision-making and collaboration","","","By advancing our understanding of these dynamics, we can design AI systems that not only improve decision-making accuracy but also support safe and sustainable human-AI collaboration in high-stakes environments.","This represents an AI feature because it explicitly mentions AI systems designed to enhance decision-making accuracy and facilitate human-AI collaboration, indicating AI capabilities beyond conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: context_adaptive | Feature: adaptive interfaces","","","Our future research will explore adaptive interfaces capable of dynamically adjusting explanation complexity based on user behavior and task difficulty.","This represents an AI feature because it describes interfaces that dynamically adapt to user behavior and task context, aligning with context-aware/adaptive behavior characteristics in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: non_deterministic | Feature: Uncertainty-based confidence levels","","","Confidence levels are provided for AI and users. AI’s CL is calculated by the model’s uncertainty in the prediction.","This represents a non-deterministic AI feature as it involves variable outputs dependent on model uncertainty, contrasting with predictable conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: lack of AI explanation","","","However, no explanation was provided for the AI’s decision. This phase was designed to measure the impact of AI assistance on user decision-making accuracy.","This directly addresses opacity/explainability by mentioning that no explanation was provided for the AI's decision, which relates to black-box behavior and trust/regulation challenges in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: AI explanation mechanism","","","3.Phase 3: Condition-Specific AI Explanation Mechanism Participants were shown one of six different AI explanation mechanisms.","This relates to opacity/explainability by discussing AI explanation mechanisms, which are tools or methods aimed at addressing the lack of transparency and interpretability issues in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI decision-support mechanisms","","","Phase 2: Decision-Making w/ AI assistance but w/o any explanation➋ Visual Explanations (C2)➌ AI CLs (C3)➍ Human Feedback (C4)➎ AI-Driven Questions (C5)➏ Performance Visualization (C6)Phase 3: Decision-Making w/ AI Assistance Accompanied by a Decision-Support Mechanism","This text explicitly mentions AI assistance in decision-making and lists specific AI-driven support mechanisms (visual explanations, AI confidence levels, AI-driven questions, performance visualization), which are characteristic features of AI systems in human-AI interaction contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: interface | Feature: AI suggestion presentation and support mechanisms","","","Figure 4: Interface design across different phases: Phase 1, where the user makes an independent decision without AI assistance; Phase 2, where the AI presents its suggestion but without any explanation; and Phase 3, where condition-specific AI decision-making support mechanisms are applied, followed by another chance for users to update their decision.","This text describes how AI presents suggestions and applies condition-specific decision-making support mechanisms, which are interface features that enable AI capabilities in human-AI interaction systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: opacity | Feature: AI explanation methods","","","The right side of the figure illustrates the various explanation methods employed. (1) Textual explanation for why the AI selected a preferred meal, (2) Visual segmentation and labeling of meal items, (3) AI’s confidence level (CL) and an estimation the user’s confidence level (CL), (4) Human feedback on the AI and their own confidence levels (CLs), (5) AI prompted questions for the user, and (6) Performance visualization of the correctness of past questions, for both the AI and the user.","This text directly addresses explainability/opacity concerns by listing specific explanation methods employed by AI systems (textual explanations, visual explanations, confidence levels, etc.), which are features designed to mitigate the black-box nature of AI and improve transparency in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: AI assistance complexity, reliability, and explanation effectiveness","","","the perceived complexity and reliability of the AI’s assistance, and the effectiveness of each AI explanation mechanism.","This directly mentions AI-specific capabilities (assistance, explanation mechanisms) that are being evaluated, which relates to AI features like explainability and reliability in human-AI interaction contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Generative AI model for prediction","","","We utilize a generative AI model to predict the participant’s CL in selecting the correct meal for controlling blood sugar in diabetes management.","This represents an AI feature as it explicitly mentions a 'generative AI model' used for predictive capabilities, which is a novel characteristic compared to conventional automation, involving data-driven decision-making and potential non-deterministic outputs.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Generative AI model for prediction","","","In our experiments, we use GPT-4o-2024-05-13 as the generative AI model. The model processes historical decision data and the nutritional profiles of the current query meals to predict the user’s likelihood of selecting the correct meal.","This represents an AI capability where a generative model (GPT-4o) performs predictive analysis based on data processing, which is a characteristic of advanced AI systems versus conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","Category: AI_capability | Feature: Data-driven prediction","","","Lprocesses historical decisions Dand the nutritional profiles of both current query meals to predict the user’s likelihood of selecting the correct meal;","This describes an AI capability where the system uses historical data (decisions and nutritional profiles) to make predictions about user behavior, which is a core function of AI systems versus conventional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 7","","Since human decision-making often relies on System 1 thinking (fast, intuitive, heuristics driven, prone to cognitive biases), users may ignore or insufficiently engage with the explanations, leading to poor decision-making.","Severity Justification: Poor decision-making directly indicates performance degradation, and the link to cognitive biases suggests a systematic issue. | Relevance Justification: Directly addresses cognitive biases in decision-making with AI systems, which is a key focus area for novel AI-related degradations.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: performance_metrics | Severity: 6","","While mechanisms like human feedback and AI-driven questions encouraged deeper reflection, they may have resulted in decreased task performance, likely due to increased cognitive effort and heightened scrutiny, which negatively impacted trust.","Severity Justification: The degradation is directly mentioned as 'decreased task performance' with a clear causal link to cognitive effort and trust issues, but it is qualified with 'may have resulted in' indicating uncertainty, so it is moderate. | Relevance Justification: This is a direct mention of human performance degradation (decreased task performance) in the context of novel AI-related mechanisms (human feedback and AI-driven questions), making it highly relevant to the research focus on human-AI interaction and performance impacts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","To encourage a shift from System 1 to System 2 thinking, with the goal to help overcome issues of over-trust or under-trust by enhancing engaged task performance, we designed six decision-support mechanisms including a baseline text explanation method.","Severity Justification: Trust issues (over-trust/under-trust) are directly stated as problems needing intervention, indicating moderate severity as they are recognized but not described with extreme consequences. | Relevance Justification: This directly matches the priority search for 'trust issues: over-trust, under-trust' and is explicitly mentioned in the chunk, making it highly relevant to human performance degradation in AI contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Studies suggest that overreliance persists when users are faced with complex tasks or explanations [34, 35, 36].","Severity Justification: Overreliance is a specific form of over-trust that can degrade decision-making, though the text presents it as a studied phenomenon rather than an immediate severe consequence. | Relevance Justification: Directly mentions overreliance, which is related to trust issues and automation bias, key categories for performance degradation in human-AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","Users may over-rely or under-trust an AI system. Over-trust can lead to unquestioned acceptance of AI suggestions, even wrong ones, potentially resulting in incorrect or harmful recommendations [18, 19]. Conversely, under-trust may cause users to disregard valuable AI-generated advice that could potentially improve health outcomes [20].","Severity Justification: The text explicitly links over-trust to potentially harmful recommendations and under-trust to disregarding valuable advice, indicating significant performance impacts, especially in healthcare contexts mentioned earlier. | Relevance Justification: Directly addresses trust issues (over-trust and under-trust) which are priority search items for human performance degradation in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: behavioral_changes | Severity: 6","","Users may rely excessively on their intuitions (System 1) or engage in overly critical analysis (System 2) that can both discount AI contributions.","Severity Justification: The excerpt indicates behavioral changes (excessive reliance on intuition or overly critical analysis) that degrade performance by undervaluing AI inputs, but it does not detail severe consequences like major errors. | Relevance Justification: The excerpt discusses cognitive biases and decision-making issues related to AI interaction, relevant to the research focus on novel AI features like opacity and adaptive behavior affecting human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","Users often weigh the cognitive cost (System 2) of evaluating AI suggestions against perceived benefits, potentially defaulting to uncritical acceptance (System 1) if the effort seems too high [41, 42, 39]. Conversely, under-trust can lead to the dismissal of valuable AI support, limiting the potential benefits of human-AI collaboration.","Severity Justification: The excerpt highlights significant trust issues (under-trust and uncritical acceptance) that directly impair decision-making and collaboration, leading to potential errors or missed benefits, but it does not specify extreme outcomes like accidents. | Relevance Justification: The excerpt explicitly mentions under-trust and uncritical acceptance, which are key trust issues in human-AI interaction, aligning closely with the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 7","","The highly significant p-value ( p <0.001) indicates that a notable proportion of users follow AI suggestions even when they are incorrect, demonstrating a bias toward trusting AI-generated advice. This highlights an over-reliance on AI, even in cases where the suggestions are faulty, something that prior work has also demonstrated [44, 45, 46].","Severity Justification: The text explicitly describes a bias and over-reliance on AI, leading to following incorrect suggestions, which is a direct performance degradation in decision-making. | Relevance Justification: Directly addresses automation bias and trust issues in human-AI interaction, with statistical evidence and references to prior work, aligning with the research focus on novel AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 7","","people tend to follow AI suggestions with limited critical evaluation, often accepting them without thoroughly processing the outcomes. Users show a tendency to rely on AI suggestions, even when the AI is wrong. Such over-reliance can be explained by cognitive bias, where users may perceive the AI system as an authoritative figure, especially in tasks requiring complex decision-making.","Severity Justification: The excerpt describes a clear pattern of over-reliance and limited critical evaluation, which can lead to significant performance degradation when AI is incorrect, though it does not quantify specific error rates. | Relevance Justification: Directly addresses cognitive bias (automation bias) and over-reliance on AI, which are key aspects of human performance degradation in human-AI interaction, aligning with the research focus on novel AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","People tend to trust AI’s answers, even when explanations are minimal, and this can lead to overreliance.","Severity Justification: Overreliance on AI due to minimal explanations can degrade human performance by reducing critical evaluation, potentially leading to errors in high-risk contexts. | Relevance Justification: Directly addresses trust issues and overreliance, which are key human performance degradations in human-AI interaction, as specified in the priority search.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 5","","Explanation mechanisms with a balanced Explanation Information Load (EIL) that provide interpretable model reasoning support for effective human-AI collaboration by enhancing decision-making and facilitating trust calibration without overwhelming users.","Severity Justification: Unbalanced explanations could overwhelm users, leading to cognitive overload and impaired decision-making, though the text focuses on the benefits of balanced mechanisms. | Relevance Justification: Addresses cognitive load and trust calibration issues, which are key degradations in human performance when interacting with AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: behavioral_changes | Severity: 6","","Access to explanation mechanisms makes users more critical of AI suggestions, encouraging deeper evaluation of AI outputs.","Severity Justification: Lack of critical evaluation can lead to degraded performance by increasing reliance on potentially incorrect AI outputs, though the text frames it as a positive effect of explanations. | Relevance Justification: Relates to behavioral changes in how users evaluate AI, which is a human performance factor in decision-making processes.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","However, it is important to note that the system complexity also increased ( p= 0.008), which may have led to increased cognitive overload.","Severity Justification: The text indicates a statistically significant increase in system complexity (p=0.008) that may lead to cognitive overload, suggesting a moderate but measurable impact on human performance. | Relevance Justification: This directly matches the research focus on novel AI characteristics (specifically opacity/lack of explainability leading to complexity) and the priority search category for cognitive load issues. The phrase 'cognitive overload' is explicitly mentioned verbatim.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Users likely accepted the AI’s output without reflecting on its reasoning, which points to potential over-reliance.","Severity Justification: Over-reliance on AI without reflection can lead to uncritical acceptance of potentially flawed outputs, degrading decision-making quality. | Relevance Justification: Directly mentions 'potential over-reliance,' which is a trust issue and human performance degradation in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","However, we observe that there is no significant improvement in system reliability and trust, suggesting that confidence levels alone may not be sufficient to build trust.","Severity Justification: The excerpt explicitly mentions no significant improvement in trust, which is a direct trust issue, but it does not specify severe degradation or failure. | Relevance Justification: This directly addresses trust issues, a priority search category, in the context of AI system reliability, aligning with the research focus on human-AI interaction and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 7","","high information density, without sufficient interpretive support, can lead to cognitive strain, ultimately limiting performance and reducing trust calibration [48].","Severity Justification: Cognitive strain directly limits performance, indicating a moderate to high impact on human capabilities. | Relevance Justification: This directly describes a human performance degradation (limiting performance) related to AI/autonomous system characteristics (information density and interpretive support), aligning with the research focus on cognitive load and trust issues.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: misinterpretation | Severity: 4","","This combination of a manageable EIL and interpretable reasoning support users in making informed decisions without information overload. In contrast, mechanisms that lack clear interpretive reasoning exhibit less impact on performance. Performance visualization (C6), with an EIL of 0.60, demonstrates borderline significance ( p= 0.092), suggesting that while users are encouraged to engage with historical performance data, the lack of explicit interpretive guidance left them to draw their own conclusions from visual information.","Severity Justification: The text indicates a lack of explicit interpretive guidance leads users to draw their own conclusions, which could result in misinterpretation, but it's described as having 'less impact on performance' and 'borderline significance', suggesting moderate rather than severe degradation. | Relevance Justification: This directly addresses interpretation issues (misinterpretation of AI recommendations/understanding AI outputs) from the priority search list, as it discusses how lack of interpretive guidance affects user decision-making with performance data.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 7","","AI-driven questions (C5), with the highest EIL, likely lead to information overload, as users are required to reflect deeply on their decisions.","Severity Justification: Information overload is a direct cognitive overload issue that can degrade decision-making performance, though the text states it 'likely' leads to this rather than confirming it. | Relevance Justification: Directly mentions 'information overload' as a performance degradation linked to AI characteristics (high EIL), aligning with the research focus on novel AI-related degradations.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 8","","In contrast, mechanisms that lack interpretive support or impose high cognitive demands may hinder performance, highlighting the importance of designing AI explanation mechanisms that offer clear interpretive insights aligned with the user’s cognitive capacity.","Severity Justification: Explicitly states that high cognitive demands 'may hinder performance,' indicating a significant potential degradation in human performance due to AI mechanisms. | Relevance Justification: Directly addresses performance hindrance from AI mechanisms with high cognitive demands or lack of interpretive support, relevant to novel AI characteristics like opacity and context-aware behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","The result suggests that textual explanations may introduce additional complexity burden on users, making the system more challenging to use.","Severity Justification: The text indicates a significant increase in system complexity (p=0.008) that burdens users and makes the system more challenging to use, suggesting moderate performance impact. | Relevance Justification: Directly addresses cognitive load/interface complexity from AI explanations, which is a key degradation category in human-AI interaction research.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 3","","Engaging with AI Trust. Interestingly, user trust in the AI system slightly decreased, though this change was not statistically significant (p= 0.368).","Severity Justification: The degradation is described as a slight decrease in trust, which is minor and not statistically significant, suggesting low severity. | Relevance Justification: This directly addresses trust issues, a priority search category, by mentioning decreased user trust in the AI system, which is relevant to human performance degradation in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","However, this gain in reliability does not translate into a corresponding increase in trust, as shown in the next subsection, pointing toward potential limitations in how trust is developed in AI systems without transparency or interpretability.","Severity Justification: The issue is moderate as it directly points to a failure in trust development, which can lead to under-reliance or misuse in high-risk contexts, but it is not explicitly linked to severe outcomes like errors or accidents. | Relevance Justification: Highly relevant as it explicitly discusses trust issues (under-trust or trust mismatch) in AI systems, aligning with the research focus on opacity/lack of explainability and its impact on human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 5","","We can also find there is a mismatch between the user’s mental model of the AI and the actual usefulness of the explanations provided. If explanations do not align well with user mental models, they may lead to over-reliance or skepticism toward the AI system.","Severity Justification: The mismatch can lead to significant behavioral changes like over-reliance or skepticism, suggesting a notable impact on decision-making and trust. | Relevance Justification: This directly relates to trust calibration and misinterpretation in human-AI interaction, core to the research focus on opacity and explainability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 4","","Despite the increase in the reliability, the additional complexity of textual explanations may have increased cognitive load, leading to a minor reduction in overall trust.","Severity Justification: The degradation is described as a 'minor reduction' in trust, suggesting a moderate impact on performance. | Relevance Justification: This directly addresses cognitive load and trust issues, which are key aspects of human performance degradation in AI interactions, aligning with the research focus on opacity and explainability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","Without explanations or feedback, users may begin to feel uncertain about how to critically assess its outputs, leading to a trust decline.","Severity Justification: The degradation involves a direct decline in trust, which is a critical factor in human-AI interaction, especially in high-risk industries where trust affects decision-making and reliance. The uncertainty in critical assessment can lead to under-trust or mis-calibrated trust, impacting performance. | Relevance Justification: This excerpt directly addresses trust issues, specifically trust decline due to lack of transparency and interpretability, which aligns with the research focus on opacity/lack of explainability in novel AI systems. It highlights a human performance degradation related to trust calibration and decision-making under uncertainty.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","the lack of system transparency may limit user ability to engage critically with the AI’s outputs with impact on perception of credibility and trust.","Severity Justification: The degradation is moderate as it directly links transparency issues to reduced critical engagement and trust, but it is not quantified or tied to specific performance metrics. | Relevance Justification: Highly relevant to the research focus on opacity/lack of explainability in AI systems, as it explicitly discusses how this characteristic impacts user trust and critical interaction, aligning with human performance degradation in high-risk contexts.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 3","","Trust. Although trust in the AI slightly decreased after the introduction of CLs, this change is not statistically significant ( p= 0.206). The results indicate a subtle interaction between the reliability and trust of the system.","Severity Justification: The decrease is described as 'slight' and not statistically significant, indicating a minor degradation. | Relevance Justification: Directly addresses trust degradation, a key focus in human performance issues with novel AI systems, aligning with the research focus on trust/regulation challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: behavioral_changes | Severity: 3","","it is possible that users become more critical of the AI suggestions when they can see that the system is not always highly confident or more accurate than a human.","Severity Justification: The text describes a potential negative behavioral change (increased criticism) but does not quantify its impact on performance or specify severe consequences. | Relevance Justification: The text directly addresses a human performance-related behavioral change (increased criticism) in response to AI characteristics (non-deterministic confidence/accuracy), aligning with the research focus on novel AI features versus traditional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 4","","The lack of significance indicates that the added cognitive task of self-reflection does not substantially enhance overall satisfaction, potentially because it may introduce additional cognitive effort.","Severity Justification: The text explicitly mentions 'additional cognitive effort' as a potential negative consequence, but it is framed as a possibility ('potentially because') and not a confirmed severe degradation. | Relevance Justification: The text directly discusses a human performance aspect (satisfaction not enhanced due to added cognitive task/effort) related to human-AI interaction, aligning with the research focus on novel AI characteristics and human feedback mechanisms.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 3","","The results show that while the added interaction may increase the cognitive processing load, users generally find the system manageable.","Severity Justification: The text mentions a potential increase in cognitive processing load, but it is described as manageable and non-significant, indicating low severity. | Relevance Justification: The text directly addresses cognitive processing load, which is a key aspect of cognitive overload in human-AI interaction, aligning with the research focus on novel AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 3","","Trust. Trust in the AI system slightly decreased after participants have to reflect on their own CLs and those of the AI. While this change is not statistically significant ( p= 0.417), the slight decline suggests that the process of reflecting on confidence might lead some users to become more critical of the AI’s suggestions and question the system’s accuracy when they think their confidence is not aligned.","Severity Justification: The decrease is described as slight and not statistically significant, indicating a minor degradation. | Relevance Justification: Directly addresses trust calibration issues, a key focus in human-AI interaction, by describing a decline in trust and questioning of AI accuracy due to reflection on confidence levels.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Trust. We observe a statistically significant decrease in trust after participants interact with AI-driven questions (C5) ( p= 0.032). The result suggests that, while reflective questions are designed to improve engagement, they may also lead users to question the AI’s recommendations more critically, potentially leading to a loss of trust when the AI’s logic does not align with the user’s expectations.","Severity Justification: The decrease in trust is statistically significant (p=0.032), indicating a measurable and reliable negative impact on user trust, which is a critical component of effective human-AI collaboration. | Relevance Justification: This directly addresses the research focus on trust/regulation challenges due to AI opacity/lack of explainability (black-box behavior) and matches the priority search for trust issues (under-trust, trust mismatch).","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: misinterpretation | Severity: 4","","some participants possibly found the reflective questions helpful, while others may have struggled with interpreting the AI’s logic, leading to doubts about its accuracy.","Severity Justification: The degradation is implied through struggles with interpretation leading to doubts, but it is not severe as it is not statistically significant and only affects some participants. | Relevance Justification: This directly relates to misinterpretation of AI outputs, a key aspect of human performance degradation in human-AI interaction, as it involves challenges in understanding AI logic affecting trust and accuracy perception.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 2","","The counterintuitive result suggests that the AI’s reflective questions may help users focus their decision-making process, thereby simplifying the task. By guiding users toward key considerations, the AI-generated questions might reduce cognitive overload, offering a structured framework for evaluating the AI’s recommendations.","Severity Justification: The text describes a reduction in cognitive overload, which is a positive effect on performance, but it is framed in the context of addressing potential degradations, so severity is low as it's mitigating rather than causing degradation. | Relevance Justification: This is relevant to cognitive load (priority 4) in human-AI interaction, as it discusses AI features reducing cognitive overload, which relates to performance metrics and behavioral changes in decision-making processes.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 3","","It is possible that questions may introduce additional cognitive processing load, leading to this slight decrease.","Severity Justification: The text describes a 'slight decrease' in satisfaction linked to 'additional cognitive processing load', indicating a mild but direct performance degradation effect. | Relevance Justification: This directly addresses cognitive load issues (priority 4) in human-AI interaction, as it involves AI-driven questions increasing cognitive processing load, which is a novel AI-related degradation compared to traditional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 3","","without overwhelming them with additional information load.","Severity Justification: The excerpt indirectly addresses cognitive load by stating that visualizations prevent overwhelming users, but it does not explicitly describe severe degradation or negative outcomes. | Relevance Justification: This relates to cognitive load, a priority search category, as it discusses information load in the context of AI decision-making, though it is framed as a mitigation rather than a direct degradation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 3","","Trust. We observe a slight decrease in trust, though this change is not statistically significant ( p= 0.099).","Severity Justification: The decrease is described as 'slight' and not statistically significant, indicating a minor impact on performance. | Relevance Justification: This directly addresses trust issues, a key focus in human performance degradation with novel AI systems, as specified in the priority search.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","The results suggest that the performance visualizations (C6) lead users to question the AI’s recommendations more critically, potentially leading to a loss of trust when the AI’s behavior does not align with the user’s expectations.","Severity Justification: Loss of trust is a moderate degradation as it can undermine effective human-AI collaboration, but the text does not specify severe operational consequences. | Relevance Justification: Directly mentions a loss of trust due to AI behavior not aligning with user expectations, which is a key trust issue in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","It suggests that the increased cognitive load leads users to question AI suggestions more critically.","Severity Justification: The excerpt explicitly links increased cognitive load to questioning AI suggestions, suggesting moderate performance impact as it affects user engagement and trust, but does not specify severe outcomes like errors or failures. | Relevance Justification: This directly relates to cognitive load, a priority search category, and discusses human performance degradation in interacting with AI, aligning with the research focus on novel AI characteristics like opacity and adaptive behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 4","","It may be because the reflective questions introduce extra cognitive steps before making decisions.","Severity Justification: The excerpt implies a moderate degradation due to increased cognitive steps, which could hinder efficiency but is not described as severe or causing errors. | Relevance Justification: Directly relates to cognitive load from AI interaction, aligning with the research focus on novel AI characteristics like opacity and adaptive behavior affecting human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 4","","mechanisms that require deeper cognitive engagement (e.g., Engaging with AI AI-driven questions (C5)) may introduce additional cognitive load without significantly improving user perception of consistency.","Severity Justification: The text explicitly mentions 'additional cognitive load' as a potential negative effect, but it does not quantify its impact or describe severe performance degradation. | Relevance Justification: This directly addresses cognitive load, which is a priority search category for human performance degradation in human-AI interaction, specifically related to novel AI characteristics like context-aware/adaptive behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","Human feedback (C4) and AI-driven questions (C5) both result in significantly lower trust scores compared to the more straightforward AI support provided in Condition 1. This indicates that while reflective mechanisms can prompt users to think critically, they may also expose users to uncertainties, reducing trust in the system.","Severity Justification: The degradation is described as 'significantly lower trust scores' with a direct causal link to reduced trust, indicating a moderate to high impact on human performance due to trust issues. | Relevance Justification: This directly addresses trust issues (under-trust or trust reduction) as a human performance degradation related to novel AI characteristics, specifically reflective mechanisms and uncertainties, which aligns with the research focus on non-deterministic decision-making and opacity.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: performance_metrics | Severity: 4","","For example, in textual explanations (C1), unfamiliar users ( n= 8) had an average accuracy of 69.4% while familiar users ( n= 10 ) achieved 72.5%(p= 0.398). Similarly, in AI CLs (C3) unfamiliar users ( n= 11 ) achieved 71.8%accuracy compared to 77.9%among familiar users ( p= 0.081).","Severity Justification: The degradation is measurable (2.6-6.1% lower accuracy for unfamiliar users) but not statistically significant at conventional thresholds, suggesting a moderate impact. | Relevance Justification: Directly presents human performance metrics (accuracy) comparing unfamiliar vs. familiar users in AI contexts, which aligns with examining performance degradations in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Overall, the trust dynamics across conditions highlight a key trend: participants with higher initial trust were more likely to lower their trust after interacting with the system, especially in conditions where the mechanisms required deeper cognitive engagement (higher EIL). This may have occurred because participants with initially higher trust might have had higher expectations of the system. When those expectations were not fully met, either due to system Engaging with AI performance or in conditions that required more cognitive effort, their trust may have been undermined.","Severity Justification: The text describes a clear trend of trust reduction, especially under conditions requiring more cognitive effort, which could impact performance, but it does not specify severe operational consequences. | Relevance Justification: Directly addresses trust calibration issues and mismatches in human-AI interaction, which are key degradations in the research focus on novel AI characteristics like opacity and adaptive behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","while textual explanations enhance transparency, they also increase cognitive load. Our findings are consistent with these results; although users appreciate the improved clarity, many find the system more complex to use.","Severity Justification: The excerpt directly mentions increased cognitive load and system complexity, which are moderate but clear indicators of performance degradation due to mental strain and usability issues. | Relevance Justification: This is highly relevant to the research focus on novel AI characteristics (opacity/lack of explainability) and human performance degradation, specifically addressing cognitive load and interface complexity as degradations.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: misinterpretation | Severity: 6","","While other research has highlighted that they can be easily misinterpreted, particularly by lay users, leading to reduced comprehension compared to more detailed textual explanations [60, 61].","Severity Justification: Misinterpretation by lay users leading to reduced comprehension indicates a moderate degradation in understanding and decision-making capability. | Relevance Justification: Directly addresses misinterpretation of AI outputs (visual explanations), which is a key human performance degradation in human-AI interaction, aligning with the research focus on opacity/lack of explainability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","the trust does not improve significantly, indicating that visual explanations alone may not be sufficient to enhance deeper engagement or trust, despite the task being visual. This is also reflected in user feedback, with one user noting ""I found that when I chose differently than AI, I sometimes was torn on whether or not I should switch."" Users may need more than visual explanations to meaningfully calibrate trust in AI outputs.","Severity Justification: The excerpt describes a clear trust calibration problem where users are uncertain about trusting AI outputs, which can degrade decision-making performance by causing hesitation and potential errors in high-risk contexts, though it is not explicitly linked to severe outcomes. | Relevance Justification: This directly addresses trust calibration issues, a key aspect of human performance degradation in human-AI interaction, as it involves users struggling to appropriately trust AI recommendations, aligning with the research focus on opacity and explainability challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","While the mechanisms can improve user performance and engagement, they should be carefully designed to avoid overwhelming cognitive load.","Severity Justification: The text explicitly mentions 'overwhelming cognitive load' as a risk that needs to be avoided, indicating a moderate severity as it is a recognized performance degradation issue in human-AI interaction, but it is framed as a preventable design consideration rather than an inevitable outcome. | Relevance Justification: This directly relates to the research focus on novel AI characteristics (e.g., opacity/lack of explainability) and the priority search for cognitive load issues, as it addresses how transparency mechanisms in AI can impact human cognitive workload, which is a key factor in performance degradation in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","Studies have found that self-reported mechanisms can lead to increased cognitive load [64, 29]. When users are encouraged to reflect on their own confidence [65, 66, 67], this mechanism can introduce skepticism [68].","Severity Justification: Increased cognitive load is a moderate degradation as it can impair decision-making and performance, but the text does not specify the extent or consequences. | Relevance Justification: Directly mentions 'increased cognitive load', which is a key performance degradation in human-AI interaction, aligning with the research focus on cognitive load issues.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 4","","Human feedback (C4), especially self-reporting, shows mixed impact on improving trust calibration and decision-making in AI systems.","Severity Justification: Indicates that interventions (human feedback) do not reliably improve trust or decision-making, implying persistent performance issues. | Relevance Justification: Directly discusses trust calibration and decision-making impacts, relevant to performance degradation in human-AI collaboration.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Several users expressed uncertainty regarding the AI’s recommendations, with feedback such as, ""I wasn’t sure if I should trust the confidence level,"" and""..., but most of the time I found it hard to believe the AI suggestions.""","Severity Justification: Direct user feedback shows active distrust and uncertainty, which can degrade decision-making performance by causing hesitation or rejection of AI assistance. | Relevance Justification: Explicitly mentions user uncertainty and distrust of AI recommendations, directly related to trust issues and potential performance degradation in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","In our study, human feedback (C4) similarly shows no significant improvements in performance, satisfaction, or trust. One participant stated, “I had already made my mind up by the third phase, so it didn’t play a huge part in my decision-making.” It highlights how some users may have already discounted AI input when they interact with the feedback mechanism. Another commented, “I felt like the AI was unreliable.”, reflecting the observed decline in trust. In essence, while feedback mechanisms encourage active reflection, they must be carefully designed to avoid undermining trust or causing unnecessary complexity.","Severity Justification: The text explicitly mentions 'no significant improvements in performance' and 'observed decline in trust,' indicating a direct negative impact on human performance and trust, which is a moderate to high severity issue in human-AI interaction. | Relevance Justification: The text directly addresses trust issues (under-trust and trust mismatch) and performance degradation in the context of novel AI systems, as it discusses human feedback showing no improvement in performance and a decline in trust due to AI unreliability and user discounting of AI input.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 6","","shifting their thinking from fast, heuristic-based decisions (System 1) to more deliberate, analytical processes (System 2) [37]. For example, CFFs show that forcing users to slow down and reflect can reduce over-reliance on AI [27, 30, 71, 72].","Severity Justification: Over-reliance on AI is a recognized form of automation bias that can lead to significant performance degradation, as users may uncritically accept AI outputs without proper scrutiny, potentially resulting in errors in high-risk contexts. | Relevance Justification: The excerpt directly addresses over-reliance on AI, which is a key human performance degradation issue in human-AI interaction, particularly relevant to the research focus on novel AI characteristics like opacity and non-deterministic decision-making that can exacerbate such biases.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 7","","However, our results show that while AI-driven questions (C5) encourage engagement, they also reduce trust and reliability. This phenomenon may occur because users become more aware of the AI’s potential errors or inconsistencies,","Severity Justification: The degradation is directly stated as a result of AI interaction, with a clear negative impact on trust and reliability, though it is not described as catastrophic. | Relevance Justification: This directly addresses trust issues, a priority category in the research focus on novel AI characteristics, and is explicitly mentioned in the chunk.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Despite the decrease in trust, we observe a significant reduction in system complexity, indicating that reflective questions may streamline the decision-making process by focusing user attention on key aspects. However, it comes at the cost of trust, particularly when AI errors are highlighted during the reflective process.","Severity Justification: The degradation is repeated and linked to AI errors, indicating a persistent issue, but it is balanced with some benefits like reduced complexity. | Relevance Justification: This is another explicit mention of trust degradation, directly relevant to the trust issues category in the research focus on AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","The results provide insights into how decision support mechanisms influence trust or reliance calibration, highlighting both under-reliance (insufficient reliance on correct AI suggestions) and over-reliance (excessive reliance on incorrect AI suggestions).","Severity Justification: The text explicitly mentions under-reliance and over-reliance, which are trust-related degradations that can lead to errors in decision-making, but it does not quantify their impact or link them to severe outcomes. | Relevance Justification: The text directly addresses trust calibration and reliance issues in AI decision support, aligning with the research focus on human-AI interaction and performance degradations in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: misinterpretation | Severity: 6","","Such a modest increase implies that while users find visual explanations somewhat useful, they are not sufficient to help users identify when Engaging with AI the AI is wrong, as users may rely on the visual aids without critically evaluating whether the AI’s suggestions are accurate [83, 84].","Severity Justification: The degradation involves users failing to identify AI errors due to reliance on visual aids, which could lead to incorrect decisions, but the impact is moderated by the context of visual explanations being somewhat useful. | Relevance Justification: This directly addresses misinterpretation of AI recommendations (priority 6) and trust issues (priority 1), as users may over-trust visual aids and misunderstand AI outputs, aligning with the research focus on opacity/lack of explainability in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 3","","In performance visualization (C6), we find a slight decrease in reliance calibration from Phase 2 to Phase 3, though the change is not statistically significant. The results suggest that while users appreciate the comparison information, it does not necessarily lead to improved reliance calibration.","Severity Justification: The decrease is described as 'slight' and 'not statistically significant', indicating a minor degradation. | Relevance Justification: Reliance calibration is directly related to trust issues, as it involves adjusting trust in AI systems based on performance, which aligns with the research focus on trust/regulation challenges from opacity/lack of explainability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 7","","Users still exhibit a tendency to follow the AI even when it is wrong. Visualizing past performance may introduce additional complexity without direct model interpretability, which likely leads users to rely more on the AI, even when it is incorrect.","Severity Justification: The degradation involves following incorrect AI, which can lead to errors in high-risk industries, but the text doesn't specify the frequency or consequences, so severity is moderate. | Relevance Justification: Directly addresses automation bias (following AI when wrong) and relates to novel AI characteristics like opacity/lack of explainability (without direct model interpretability), making it highly relevant to the research focus.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 5","","However, it is essential to balance the cognitive load, as questions may introduce complexity that could lead to overthinking or under-reliance [89].","Severity Justification: The degradation involves increased cognitive load and complexity leading to negative outcomes like overthinking or under-reliance, but it is presented as a potential risk to be balanced, not a confirmed severe issue. | Relevance Justification: This directly addresses cognitive load (a priority search item) and its potential to degrade performance by causing overthinking or under-reliance, relevant to human-AI interaction challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Reliance calibration in this condition appears to be less effective, as users may struggle to map their confidence with the AI’s accuracy [87], leading to minimal impact.","Severity Justification: The degradation involves a direct struggle in mapping confidence to AI accuracy, which is a core trust calibration issue, but the impact is described as 'minimal', indicating moderate severity. | Relevance Justification: This directly addresses trust calibration (a priority search item) and human performance degradation in aligning with AI outputs, which is central to the research focus on human-AI interaction degradations.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 6","","These mechanisms help users engage critically with the AI’s outputs, reducing the likelihood of blindly following incorrect suggestions. On the other hand, mechanisms such as visual explanations and performance visualizations may not provide sufficient guidance, leading to a higher risk of over-reliance when the AI has errors.","Severity Justification: The text explicitly mentions a 'higher risk of over-reliance when the AI has errors,' which directly indicates a performance degradation due to automation bias, where users may overly trust incorrect AI outputs. | Relevance Justification: This directly addresses over-reliance on AI, which is a key aspect of automation bias and human performance degradation in human-AI interaction, as specified in the research focus on novel AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","the additional cognitive load required to process this information seems to reduce the overall effectiveness of reliance calibration. These findings align with cognitive psychology studies, where increased reflection and mental effort, when not accompanied by clear decision-making guidance, can lead to cognitive overload and hinder performance [92, 93, 94].","Severity Justification: The excerpt directly states that cognitive overload ""hinder[s] performance,"" indicating a moderate degradation effect, though it is framed as a potential outcome rather than a guaranteed severe impact. | Relevance Justification: This excerpt is highly relevant as it explicitly mentions cognitive overload and hindering performance in the context of AI interaction (reliance calibration on AI), aligning with the research focus on novel AI characteristics and human performance degradations.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: automation_bias | Severity: 7","","users find it difficult to disengage from incorrect AI outputs, leading to over-reliance. The results suggest that surface-level transparency without deeper context may not be enough to guide users in critical decision-making scenarios. Users tend to over-rely on automation [97, 41, 36, 98], especially when the decision-making process lacks sufficient transparency or when the cognitive effort to process the decision information is high.","Severity Justification: The excerpt directly describes a performance degradation where users fail to disengage from incorrect AI outputs and over-rely on automation, which can lead to critical errors in decision-making scenarios, particularly in high-risk industries. | Relevance Justification: This excerpt explicitly mentions human performance degradation related to novel AI characteristics (non-deterministic decision-making and opacity), as it discusses over-reliance on AI outputs and the impact of insufficient transparency, which aligns with the research focus on trust and decision-making challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Users find it more challenging to fully trust the AI’s reasoning [101, 27]. It shows a key tension in AI design: while reflective engagement can promote deeper critical thinking, it must be carefully calibrated to prevent over-scrutiny [102, 103].","Severity Justification: The degradation is explicitly stated as users finding it 'more challenging to fully trust the AI’s reasoning,' which directly impacts trust calibration and reliance, but the text does not quantify the severity or link it to specific negative outcomes like errors or failures. | Relevance Justification: This directly addresses trust issues (a priority search category) in the context of novel AI systems, specifically mentioning challenges in trusting AI reasoning and the need for calibration to prevent over-scrutiny, which aligns with the research focus on opacity/lack of explainability and context-aware behavior.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 5","","Users may initially over-rely or under-rely on the AI, but these behaviors may change as they interact with the system over an extended period and depend on how much value the system provides them.","Severity Justification: The severity is moderate because over-reliance and under-reliance can lead to errors or inefficiencies in human-AI collaboration, but the text suggests these behaviors may change and are not necessarily permanent or severe degradations. | Relevance Justification: The relevance is high as it directly addresses trust issues (over-trust and under-trust) in human-AI interaction, which are key performance degradations in the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 6","","However, CFF mechanisms like human feedback and AI-driven questions, while encouraging deeper engagement, sometimes impose additional cognitive demands, thereby reducing performance.","Severity Justification: The degradation is directly stated as 'reducing performance' due to 'additional cognitive demands', indicating a moderate impact on human performance in high-stakes scenarios. | Relevance Justification: This directly addresses human performance degradation related to novel AI characteristics (specifically context-aware/adaptive behavior and opacity/lack of explainability, as CFF mechanisms involve AI-driven questions and feedback), matching the research focus on AI-related degradations vs traditional automation.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: trust_issues | Severity: 6","","Visual explanations and performance visualizations offer straightforward support with low-cognitive-load but are less effective in fostering critical evaluation of AI outputs and therefore have limited impact on trust and reliance.","Severity Justification: The degradation is moderate because it directly links an AI interface characteristic (visual explanations) to reduced effectiveness in critical evaluation and limited impact on trust/reliance, which are key performance factors in human-AI interaction. | Relevance Justification: Highly relevant as it explicitly discusses AI-related degradations (limited impact on trust and reliance due to ineffective critical evaluation) compared to traditional automation, addressing trust issues and interpretation problems from the priority search list.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 7","","In contrast, mechanisms encouraging reflection promote deeper analysis but risk overwhelming users when the cognitive load is too high.","Severity Justification: The degradation is moderately high because it directly states that certain AI mechanisms risk overwhelming users with high cognitive load, which can significantly impair decision-making and task performance. | Relevance Justification: Highly relevant as it explicitly discusses an AI-related degradation (risk of overwhelming users with high cognitive load) that aligns with the cognitive load category from the priority search list, contrasting with traditional automation interfaces.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 4","","• Consistency and Cognitive Load: Perceived consistency in the AI’s outputs and the cognitive effort required to process its explanations.","Severity Justification: The text explicitly references cognitive effort, which can imply performance degradation due to increased mental workload, but it does not specify severity levels or direct negative impacts. | Relevance Justification: It directly addresses cognitive load, a key factor in human performance degradation with AI, as per the research focus on context-aware/adaptive behavior and unpredictability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 5","","Consistency and Cognitive Load (H= 6.48, p= 0.262)Textual Explanations (C1) 4.639 (0.760) NoneVisual Explanations (C2) 4.667 (0.913) AI CLs (C3) 4.694 (1.056) Human Feedback (C4) 4.111 (1.264) AI-Driven Questions (C5) 4.306 (0.974) Performance Visualization (C6) 3.944 (1.212)","Severity Justification: The data shows varying scores for cognitive load, with some conditions like Performance Visualization having lower scores, indicating potential performance issues, but no explicit degradation is stated. | Relevance Justification: It provides empirical measures of cognitive load, relevant to evaluating human performance degradation in AI interactions, aligning with the focus on cognitive aspects.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","Category: cognitive_overload | Severity: 5","","•Mental Demand of Task: I thought the task of picking the right meal was mentally demanding. Scale: 1 (Not at all) to 7 (Extremely demanding)","Severity Justification: The text directly asks about mental demand on a 7-point scale, indicating it is a measured factor, but does not specify the extent of degradation, so a moderate severity is assigned. | Relevance Justification: This is highly relevant as it directly addresses cognitive load, which is a priority search category for human performance degradations in human-AI interaction, specifically in the context of novel AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: human feedback and AI-driven questions | Evidence Type: direct | Causal Strength: 7 | Performance Effect: decreased task performance and negatively impacted trust","While mechanisms like human feedback and AI-driven questions encouraged deeper reflection, they may have resulted in decreased task performance, likely due to increased cognitive effort and heightened scrutiny, which negatively impacted trust.","Causal Strength Justification: Direct causal language ('resulted in', 'due to') is used, though moderated by 'may have', indicating a strong but not absolute causal link. | Relevance Justification: Directly addresses how specific AI features (human feedback, AI-driven questions) causally affect human performance (task performance, trust) through mechanisms like cognitive effort and scrutiny.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: text explanations (XAI) to help users understand the AI’s reasoning | Evidence Type: direct | Causal Strength: 8 | Performance Effect: poor decision-making","Since human decision-making often relies on System 1 thinking (fast, intuitive, heuristics driven, prone to cognitive biases), users may ignore or insufficiently engage with the explanations, leading to poor decision-making.","Causal Strength Justification: Direct causal language 'leading to' explicitly connects the cause (ignoring explanations due to System 1 thinking) to the effect (poor decision-making). | Relevance Justification: Directly addresses how AI features (explanations) interact with human cognitive processes to affect performance, though not specifically tied to non-deterministic, opacity, or adaptive characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: cognitive forcing functions (CFFs) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: shift toward a more deliberate, analytical thought process or System 2 thinking","Among these, cognitive forcing functions (CFFs) are strategically designed interventions that can interrupt automatic, intuitive reasoning at the precise moment of decision making, causing individuals to shift toward a more deliberate, analytical thought process or System 2 thinking [43].","Causal Strength Justification: Direct causation is explicitly stated with the word 'causing'. | Relevance Justification: The causal link is explicit and relates to human cognitive processes, but it does not directly connect to the specified AI features (non-deterministic, opacity, adaptive) or their effects on human performance as outlined in the research focus.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-generated advice | Evidence Type: direct | Causal Strength: 8 | Performance Effect: disregard valuable AI-generated advice that could potentially improve health outcomes","Conversely, under-trust may cause users to disregard valuable AI-generated advice that could potentially improve health outcomes [20].","Causal Strength Justification: Direct causal language ('may cause') explicitly links under-trust to negative outcomes. | Relevance Justification: Directly addresses how trust in AI (a human-AI interaction feature) causally affects human decision-making performance, with explicit cause-effect language.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI suggestions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: unquestioned acceptance of AI suggestions, potentially resulting in incorrect or harmful recommendations","Over-trust can lead to unquestioned acceptance of AI suggestions, even wrong ones, potentially resulting in incorrect or harmful recommendations [18, 19].","Causal Strength Justification: Direct causal language ('can lead to', 'resulting in') explicitly links over-trust to negative outcomes. | Relevance Justification: Directly addresses how trust in AI (a human-AI interaction feature) causally affects human decision-making performance, with explicit cause-effect language.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI suggestions | Evidence Type: direct | Causal Strength: 7 | Performance Effect: over-reliance on AI suggestions","Such over-reliance can be explained by cognitive bias, where users may perceive the AI system as an authoritative figure, especially in tasks requiring complex decision-making.","Causal Strength Justification: Direct causal language 'can be explained by' links cognitive bias as the cause of over-reliance, with specific context about AI authority and complex tasks. | Relevance Justification: Directly addresses how AI features (suggestions) affect human performance (over-reliance) through a causal mechanism, though not explicitly tied to non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: Decision-Constraint CFFs | Evidence Type: direct | Causal Strength: 7 | Performance Effect: allow users to intervene in decision-making processes","Decision-Constraint CFFs: Allow users to intervene in decision-making processes (e.g., human feedback mechanisms that enable validation of AI outputs).","Causal Strength Justification: Direct causal relationship: 'Allow users to intervene...' indicates these features cause specific user actions in decision-making. | Relevance Justification: Shows how AI features affect human decision-making performance by enabling intervention, relevant to control and trust issues, but not explicitly tied to novel AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: CFFs (Cognitive Forcing Functions) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: influence user decision-making","CFFs are designed to influence user decision-making by reducing cognitive biases and encouraging deliberate, thoughtful processes.","Causal Strength Justification: Direct causal language: 'designed to influence... by reducing... and encouraging...' explicitly states purpose and mechanisms. | Relevance Justification: Directly addresses how AI features (CFFs) affect human decision-making performance, though not specifically tied to the novel AI characteristics listed in the research focus.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: Attention-Directing CFFs | Evidence Type: direct | Causal Strength: 7 | Performance Effect: guide user focus and prompt critical thinking","Attention-Directing CFFs: Guide user focus and prompt critical thinking (e.g., AI-driven questions).","Causal Strength Justification: Direct causal relationship: 'Guide... and prompt...' indicates these features cause specific user behaviors. | Relevance Justification: Shows how an AI feature (Attention-Directing CFFs) affects human cognitive performance, but not explicitly linked to novel AI characteristics like non-determinism or opacity.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: Complexity-Reduction CFFs | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduce cognitive load","Complexity-Reduction CFFs: Reduce cognitive load by making AI outputs easier to interpret (e.g., AI confidence levels (CLs) that transparently communicate the model’s uncertainty or performance visualizations that show AI task performance over time).","Causal Strength Justification: Direct causal language: 'Reduce cognitive load by making...' explicitly states the effect and mechanism. | Relevance Justification: Directly addresses how AI features reduce cognitive load (a human performance aspect), with relevance to opacity through transparency mechanisms, though not explicitly naming novel characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: decision support mechanisms | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increase in user engagement","We hypothesize that integrating decision support mechanisms during the decision-making process would lead to an increase in user engagement (H2).","Causal Strength Justification: Direct causal language ('would lead to') is used in a hypothesis, indicating a strong proposed causal link, though it is not empirically confirmed in this chunk. | Relevance Justification: Directly addresses causal relationships between AI features (decision support mechanisms) and human performance (user engagement), aligning with the task focus on AI characteristics affecting human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: balanced Explanation Information Load (EIL) providing interpretable model reasoning | Evidence Type: mechanism | Causal Strength: 8 | Performance Effect: effective human-AI collaboration, enhanced decision-making, facilitated trust calibration","3.Hypothesis 3 (H3): Explanation mechanisms with a balanced Explanation Information Load (EIL) that provide interpretable model reasoning support for effective human-AI collaboration by enhancing decision-making and facilitating trust calibration without overwhelming users.","Causal Strength Justification: Direct causal language 'support for... by enhancing... and facilitating...' explicitly links the cause (balanced explanation mechanisms) to the effects (collaboration, decision-making, trust calibration). | Relevance Justification: Directly addresses how an AI feature (explanation mechanisms) causally affects human performance (collaboration, decision-making, trust calibration), highly relevant to the research focus on opacity and trust.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: explanation mechanisms | Evidence Type: direct | Causal Strength: 9 | Performance Effect: more critical evaluation and deeper evaluation of AI outputs","2.Hypothesis 2 (H2): Access to explanation mechanisms makes users more critical of AI suggestions, encouraging deeper evaluation of AI outputs.","Causal Strength Justification: Direct causal language 'makes' explicitly links the cause (access to explanation mechanisms) to the effect (more critical and deeper evaluation). | Relevance Justification: Directly addresses how an AI feature (explanation mechanisms) causally affects human performance (critical evaluation), relevant to the research focus on opacity and trust.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: minimal explanations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: overreliance","1.Hypothesis 1 (H1): People tend to trust AI’s answers, even when explanations are minimal, and this can lead to overreliance.","Causal Strength Justification: Direct causal language 'can lead to' explicitly links the cause (minimal explanations) to the effect (overreliance). | Relevance Justification: Directly addresses how an AI feature (lack of explainability/opacity) causally affects human performance (overreliance), matching the research focus on opacity and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: textual explanations of AI decision-making process | Evidence Type: direct | Causal Strength: 7 | Performance Effect: increased cognitive overload","However, it is important to note that the system complexity also increased ( p= 0.008), which may have led to increased cognitive overload.","Causal Strength Justification: Direct causal language ('may have led to') with statistical significance (p=0.008) showing strong correlation, though 'may have' indicates some uncertainty. | Relevance Justification: Directly addresses how an AI feature (textual explanations) affects human performance (cognitive overload), though not specifically about non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI confidence levels (CL) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: significant increase in engagement, helps users calibrate trust, encourages critical comparison with AI judgment","The results show a significant increase in engagement ( p= 0.001), indicating that confidence levels may help users to calibrate their trust in the AI system. CL is a form of transparency that provides users with direct insight into the AI’s certainty, which encourages users to critically compare their judgment with the AI’s.","Causal Strength Justification: Direct causal language ('indicating that confidence levels may help users to calibrate their trust', 'which encourages users') with statistical evidence (p=0.001) showing correlation between CL and engagement increase. | Relevance Justification: Directly addresses how an AI feature (confidence levels/transparency) causally affects human performance (engagement, trust calibration, critical thinking) as requested in the task.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: performance visualization / visualized comparison of human and AI performance | Evidence Type: direct | Causal Strength: 7 | Performance Effect: higher engagement","Perhaps it Engaging with AI allows users to contextualize AI suggestions based on empirical evidence, forcing the user to evaluate AI outputs more deliberately. The visualized comparison is intuitive and easy to process by the human, leading to higher engagement.","Causal Strength Justification: Direct causal language is used ('leading to'), and the mechanism ('intuitive and easy to process') is explicitly stated as connecting the AI feature to the human effect. | Relevance Justification: Directly addresses the research focus on human-AI interaction by explicitly linking an AI system feature (visualization) to a measurable human performance outcome (engagement).","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: high information density without sufficient interpretive support | Evidence Type: direct | Causal Strength: 8 | Performance Effect: limiting performance and reducing trust calibration","This aligns with cognitive psychology research, which indicates that high information density, without sufficient interpretive support, can lead to cognitive strain, ultimately limiting performance and reducing trust calibration [48].","Causal Strength Justification: Direct causal language ('can lead to') is used to connect the cause (high information density without support) to the effect (cognitive strain, limiting performance, reducing trust). | Relevance Justification: Directly addresses how an AI-related feature (information density without support) causally affects human performance (limiting performance and trust calibration), aligning with the research focus on AI characteristics impacting human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI CLs (C3) with low EIL | Evidence Type: direct | Causal Strength: 7 | Performance Effect: allows users to judge the certainty of AI suggestions at a glance, serving as an efficient cognitive aid","The low EIL in AI CLs (C3) ( EIL = 0.602) allows users to judge the certainty of AI suggestions at a glance, serving as an efficient cognitive aid.","Causal Strength Justification: Direct causal language 'allows... serving' explicitly links the AI feature to the human effect. | Relevance Justification: Directly addresses opacity by enabling judgment of AI certainty, affecting human cognitive efficiency.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: visual explanations (C2) with moderate EIL | Evidence Type: direct | Causal Strength: 7 | Performance Effect: encouraging critical engagement without overwhelming users","The moderate EIL in visual explanations (C2) ( EIL = 1.749) provides users with comprehensive but understandable insights into AI reasoning, encouraging critical engagement without overwhelming them.","Causal Strength Justification: Direct causal language 'provides... encouraging' explicitly links the AI feature to the human effect. | Relevance Justification: Directly addresses opacity by providing insights into AI reasoning, affecting human cognitive engagement.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: mechanisms with manageable EIL and clear reasoning cues | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improve collaboration performance","Mechanisms that balance a manageable EIL with clear reasoning cues can improve collaboration performance. In contrast, mechanisms that lack interpretive support or impose high cognitive demands may hinder performance, highlighting the importance of designing AI explanation mechanisms that offer clear interpretive insights aligned with the user’s cognitive capacity.","Causal Strength Justification: Direct causal language 'can improve' and 'may hinder' indicates explicit cause-effect relationships, with clear mechanisms described. | Relevance Justification: Directly addresses how AI mechanisms (EIL balance and reasoning cues) causally affect human collaboration performance, highly relevant to the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-driven questions with high EIL | Evidence Type: direct | Causal Strength: 7 | Performance Effect: information overload","AI-driven questions (C5), with the highest EIL, likely lead to information overload, as users are required to reflect deeply on their decisions.","Causal Strength Justification: Direct causal language 'likely lead to' indicates a strong, explicit cause-effect relationship, though 'likely' introduces some uncertainty. | Relevance Justification: Directly addresses how an AI characteristic (high EIL in questions) causally affects human performance (information overload), aligning with the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: textual explanations | Evidence Type: direct | Causal Strength: 7 | Performance Effect: making the system more challenging to use","The result suggests that textual explanations may introduce additional complexity burden on users, making the system more challenging to use. Although textual explanations are designed to clarify the AI’s decision-making process, they can also make the system complex, especially if the explanations are not easily digestible or require additional effort to interpret.","Causal Strength Justification: The text uses causal language ('may introduce', 'making', 'can also make') to directly link textual explanations to increased complexity and usability challenges, though it's framed as a suggestion ('suggests') rather than a definitive statement. | Relevance Justification: This directly addresses how an AI feature (textual explanations aimed at reducing opacity) affects human performance (usability), aligning with the research focus on opacity/lack of explainability and its impact on human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: textual explanations (transparency) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increased perception of reliability, better understanding of AI decisions, building user confidence, calibrated trust, more critical assessment of AI suggestions","Reliability. There is a statistically significant increase in reliability ( p= 0.048) in textual explanations (C1). It indicates that textual explanations have a positive impact on how users perceive the AI system’s reliability. Textual explanation as a form of transparency helps users better understand the rationale behind AI decisions, building users’ confidence in the AI’s outputs. The results suggest that even though explanations may increase system complexity, they also enhance user perception of the system’s reliability. It shows that explanations can calibrate user trust on AI systems and help them assess AI suggestions more critically. Engaging with AI","Causal Strength Justification: Direct causal language: 'have a positive impact on', 'helps users better understand', 'building users’ confidence', 'enhance user perception', 'can calibrate user trust', 'help them assess AI suggestions more critically'. Statistical significance (p=0.048) supports causation. | Relevance Justification: Directly addresses opacity/lack of explainability (black-box behavior) from research focus by showing how textual explanations (transparency) causally affect human perception and trust in AI systems.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: lack of explanations or feedback | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users feel uncertain about how to critically assess AI outputs, leading to trust decline","Without explanations or feedback, users may begin to feel uncertain about how to critically assess its outputs, leading to a trust decline.","Causal Strength Justification: Direct causal language ('leading to') explicitly connects the cause (lack of explanations/feedback) to the effect (trust decline), with a clear intermediate step (uncertainty in assessment). | Relevance Justification: Directly addresses how opacity (lack of explanations/feedback) in AI systems causally affects human performance by reducing trust, which aligns with the research focus on opacity/lack of explainability.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: explanations | Evidence Type: direct | Causal Strength: 6 | Performance Effect: over-reliance or skepticism toward the AI system","If explanations do not align well with user mental models, they may lead to over-reliance or skepticism toward the AI system.","Causal Strength Justification: Conditional causal language ('may lead to') directly connects misalignment of explanations to human behavioral effects, though probabilistic. | Relevance Justification: Directly links AI opacity (via explanation misalignment) to human performance issues like over-reliance or skepticism, highly relevant to trust and interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: textual explanations | Evidence Type: direct | Causal Strength: 7 | Performance Effect: minor reduction in overall trust","Despite the increase in the reliability, the additional complexity of textual explanations may have increased cognitive load, leading to a minor reduction in overall trust.","Causal Strength Justification: Direct causal language ('leading to') links increased cognitive load to reduced trust, with 'may have' indicating probabilistic causation. | Relevance Justification: Directly addresses how AI opacity (via explanations) affects human trust, a key performance metric in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: lack of system transparency | Evidence Type: direct | Causal Strength: 7 | Performance Effect: limit user ability to engage critically with the AI’s outputs, impact on perception of credibility and trust","The results suggest that while the AI is generally perceived as accurate, the lack of system transparency may limit user ability to engage critically with the AI’s outputs with impact on perception of credibility and trust.","Causal Strength Justification: The causal language 'may limit' and 'with impact on' directly links the AI feature to the human performance effects, though it is phrased as a possibility rather than a certainty. | Relevance Justification: Directly addresses how opacity (lack of transparency) in AI affects human performance (critical engagement and trust), which is a core focus of the research.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: confidence levels (CLs) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users make more informed decisions and adjust their reliance on the system","By comparing the CLs of their performance with AI’s, it helps users make more informed decisions and adjust their reliance on the system.","Causal Strength Justification: Direct causal language 'helps' indicates a strong, explicit mechanism linking AI feature (CLs) to human performance effect. | Relevance Justification: Directly addresses how an AI feature (CLs) causally affects human performance (decision-making and reliance), aligning with the research focus on human-AI interaction and transparency.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI system’s performance in supporting decisions | Evidence Type: direct | Causal Strength: 7 | Performance Effect: enhance its perception of reliability","The result suggests that even without detailed textual explanations, the AI system’s performance in supporting decisions can enhance its perception of reliability.","Causal Strength Justification: The text uses 'can enhance' to directly link AI performance to reliability perception, supported by statistical evidence (p=0.005), indicating a strong causal inference. | Relevance Justification: This directly addresses how an AI feature (performance in supporting decisions) affects human performance (perception of reliability), aligning with the research focus on AI characteristics and human interaction, though it does not specify non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI systems without transparency or interpretability | Evidence Type: direct | Causal Strength: 8 | Performance Effect: does not translate into a corresponding increase in trust","However, this gain in reliability does not translate into a corresponding increase in trust, as shown in the next subsection, pointing toward potential limitations in how trust is developed in AI systems without transparency or interpretability.","Causal Strength Justification: The text uses 'does not translate into' to directly link the absence of transparency/interpretability to the lack of trust increase, supported by statistical evidence (p=0.421), indicating a strong causal inference. | Relevance Justification: This directly addresses how an AI feature (lack of transparency/interpretability) affects human performance (trust development), explicitly linking opacity to trust issues, aligning perfectly with the research focus on opacity and human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI system provides reasonably accurate suggestions | Evidence Type: direct | Causal Strength: 6 | Performance Effect: users may find the system to be reliable","The results suggest that users may find the system to be reliable when the AI system provides reasonably accurate suggestions, even without a clear understanding of its decision-making process.","Causal Strength Justification: The text uses 'when' to imply a conditional causal link between AI accuracy and reliability perception, supported by observational results, indicating a moderate causal inference. | Relevance Justification: This directly addresses how an AI feature (reasonably accurate suggestions) affects human performance (perception of reliability), with explicit mention of opacity ('without a clear understanding of its decision-making process'), aligning closely with the research focus on opacity and human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI confidence and user reflection on confidence | Evidence Type: direct | Causal Strength: 5 | Performance Effect: users feel slightly more confident in the system’s reliability","It shows that while reflection on their own confidence and the AI’s confidence may lead users to feel slightly more confident in the system’s reliability, the task of inputting CLs does not produce a major shift in how users perceive the AI’s reliability.","Causal Strength Justification: Direct causal language 'may lead to' is used, but the effect is described as slight and not statistically significant, indicating moderate causal strength. | Relevance Justification: Directly relates to how AI opacity (lack of explainability in confidence) affects human trust and perception of reliability, aligning with the research focus on opacity and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: process of reflecting on confidence and AI suggestions | Evidence Type: direct | Causal Strength: 6 | Performance Effect: users become more critical of the AI’s suggestions and question the system’s accuracy","the slight decline suggests that the process of reflecting on confidence might lead some users to become more critical of the AI’s suggestions and question the system’s accuracy when they think their confidence is not aligned.","Causal Strength Justification: Direct causal language 'might lead to' is used, with a clear mechanism described, indicating a moderate causal strength. | Relevance Justification: Directly addresses how AI opacity (in confidence alignment) causes human trust degradation and critical questioning, highly relevant to the research focus on opacity and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI’s reflective questions | Evidence Type: mechanism | Causal Strength: 4 | Performance Effect: simplifying the task, reducing cognitive overload, decreasing system complexity","The counterintuitive result suggests that the AI’s reflective questions may help users focus their decision-making process, thereby simplifying the task. By guiding users toward key considerations, the AI-generated questions might reduce cognitive overload, offering a structured framework for evaluating the AI’s recommendations.","Causal Strength Justification: Uses causal language like 'thereby' and 'might reduce', but it's framed as a suggestion ('suggests', 'may help') rather than a definitive statement, with some statistical support mentioned earlier. | Relevance Justification: Explicitly links AI features (reflective questions) to human performance effects (task simplification, reduced cognitive overload) through causal mechanisms, with relevance to decision-making processes.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-generated reflective questions | Evidence Type: mechanism | Causal Strength: 3 | Performance Effect: slight decrease in user satisfaction","It is possible that questions may introduce additional cognitive processing load, leading to this slight decrease.","Causal Strength Justification: The causal language is speculative ('It is possible that') and the effect is described as 'slight' and not statistically significant, indicating a weak causal claim. | Relevance Justification: Directly addresses how an AI feature (reflective questions) affects human performance (satisfaction) through a causal mechanism (cognitive load).","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: performance visualizations (C6) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users question AI recommendations more critically and potentially lose trust","The results suggest that the performance visualizations (C6) lead users to question the AI’s recommendations more critically, potentially leading to a loss of trust when the AI’s behavior does not align with the user’s expectations.","Causal Strength Justification: Direct causal language 'lead to' and 'potentially leading to' explicitly connects the AI feature to human effects. | Relevance Justification: Directly addresses how an AI feature (visualizations) causally affects human performance (critical questioning and trust), aligning with the research focus on human-AI interaction effects.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: confidence levels (CLs) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: users perceive the AI system as more consistent","We observe that CLs may help users perceive the AI system as more consistent, because CLs provide users with transparency about the AI’s certainty in its decisions.","Causal Strength Justification: Direct causal language ('because') links the AI feature (CLs) to the human effect (perception of consistency), though it is qualified with 'may' indicating some uncertainty. | Relevance Justification: Directly addresses how an AI feature (transparency via CLs) causally affects human perception (consistency), relevant to trust and performance in human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: loss of trust","The result suggests that, while reflective questions are designed to improve engagement, they may also lead users to question the AI’s recommendations more critically, potentially leading to a loss of trust when the AI’s logic does not align with the user’s expectations.","Causal Strength Justification: Direct causal language ('may also lead' and 'potentially leading to') links the cause (reflective questions) to the effect (loss of trust), with a clear mechanism (questioning recommendations critically). | Relevance Justification: Directly addresses how an AI interaction feature (reflective questions) causally affects human trust, with explicit causal links, strongly aligning with the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions | Evidence Type: direct | Causal Strength: 7 | Performance Effect: impacted perception of system reliability","From the results, we find that the reflective questions encourage deep reflection but may simultaneously reveal errors in the AI’s reasoning, thus impacting how users perceive the system’s reliability.","Causal Strength Justification: Direct causal language ('thus impacting') links the cause (revealing errors in AI reasoning) to the effect (impacting perception of reliability), though it is conditional ('may'). | Relevance Justification: Directly addresses how an AI interaction feature (reflective questions) causally affects human perception (reliability perception), aligning with the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: encouraging critical engagement with AI outputs | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduced trust","This finding highlights a critical challenge in human-AI collaboration: while encouraging users to engage critically with AI outputs can promote better decision-making, it can also expose the limitations of the AI, leading to reduced trust.","Causal Strength Justification: Direct causal language ('leading to') links the cause (exposing AI limitations) to the effect (reduced trust), with a clear mechanism (critical engagement). | Relevance Justification: Directly addresses how an AI interaction feature (critical engagement) causally affects human trust, with explicit causal links, strongly aligning with the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions prompting deep engagement with AI decisions | Evidence Type: direct | Causal Strength: 7 | Performance Effect: undermined confidence in system reliability","When users are prompted to engage more deeply with AI decisions, they may become more aware of potential errors, which can undermine their confidence in the system’s reliability.","Causal Strength Justification: Direct causal language ('which can undermine') links the cause (prompted deep engagement) to the effect (undermined confidence), though it is conditional ('may'). | Relevance Justification: Directly addresses how an AI interaction feature (reflective questions) causally affects human perception (confidence in reliability), aligning with the task focus on AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-driven questions (C5) and human feedback (C4) as reflective mechanisms | Evidence Type: direct | Causal Strength: 8 | Performance Effect: significantly lower trust scores and reduced trust in the system","Human feedback (C4) and AI-driven questions (C5) both result in significantly lower trust scores compared to the more straightforward AI support provided in Condition 1. This indicates that while reflective mechanisms can prompt users to think critically, they may also expose users to uncertainties, reducing trust in the system.","Causal Strength Justification: Direct causal language 'result in' and 'reducing' explicitly links AI features to trust outcomes. | Relevance Justification: Directly addresses how AI features (reflective mechanisms) causally affect human performance (trust), aligning with the research focus on AI characteristics impacting human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: system Engaging with AI performance or conditions that required more cognitive effort | Evidence Type: mechanism | Causal Strength: 6 | Performance Effect: trust may have been undermined","This may have occurred because participants with initially higher trust might have had higher expectations of the system. When those expectations were not fully met, either due to system Engaging with AI performance or in conditions that required more cognitive effort, their trust may have been undermined.","Causal Strength Justification: The text uses explicit causal language ('because', 'due to', 'may have been undermined') to link unmet expectations (caused by system performance or cognitive effort) to trust reduction, though it is phrased as a possibility ('may have occurred', 'may have been undermined'). | Relevance Justification: Directly addresses how AI system performance or interaction conditions (requiring cognitive effort) causally affect human trust, which is a key aspect of human performance in AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: Textual Explanations (C1) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improves user performance and reliability","Textual Explanations (C1) improve the interpretability of AI’s decisions by providing detailed reasoning. It significantly improves user performance and reliability, reflecting the value of detailed reasoning in helping users understand AI outputs.","Causal Strength Justification: Direct causal language ('improve', 'improves') is used to link the AI feature to human performance effects, with a clear mechanism ('by providing detailed reasoning') and explicit mention of outcomes. | Relevance Justification: Directly addresses how an AI feature (textual explanations) causally affects human performance (improved performance and reliability), aligning with the task focus on causal relationships between AI features and human performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI outputs | Evidence Type: direct | Causal Strength: 7 | Performance Effect: shift from fast, heuristic-based decisions (System 1) to more deliberate, analytical processes (System 2) and reduced over-reliance on AI","Reflective questions have been studied as a tool to engage users more critically with AI outputs [69, 70], shifting their thinking from fast, heuristic-based decisions (System 1) to more deliberate, analytical processes (System 2) [37]. For example, CFFs show that forcing users to slow down and reflect can reduce over-reliance on AI [27, 30, 71, 72].","Causal Strength Justification: Direct causal language is present: 'shifting their thinking' and 'can reduce over-reliance on AI', indicating a clear cause-effect relationship, though it is based on studies and examples rather than absolute causation. | Relevance Justification: Directly addresses how AI features (outputs) affect human performance (decision-making processes and reliance), aligning with the research focus on human-AI interaction, though it does not explicitly mention non-deterministic, opacity, or adaptive characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-driven questions (C5) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: reduced trust and reliability","However, our results show that while AI-driven questions (C5) encourage engagement, they also reduce trust and reliability. This phenomenon may occur because users become more aware of the AI’s potential errors or inconsistencies, as evidenced by participant feedback.","Causal Strength Justification: Direct causal language: 'reduce trust and reliability' and 'may occur because' explicitly links AI questions to reduced trust via awareness of errors. | Relevance Justification: Directly addresses how an AI feature (questions) causally affects human performance (trust/reliability), though not explicitly tied to non-deterministic/opacity/adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective process | Evidence Type: direct | Causal Strength: 5 | Performance Effect: cost of trust","However, it comes at the cost of trust, particularly when AI errors are highlighted during the reflective process.","Causal Strength Justification: Direct causal link: 'comes at the cost of trust' and 'when AI errors are highlighted' implies causation between error highlighting and trust reduction. | Relevance Justification: Addresses how AI errors in reflective processes affect human trust, though not explicitly tied to non-deterministic/opacity/adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions | Evidence Type: mechanism | Causal Strength: 6 | Performance Effect: reduction in system complexity, streamlined decision-making","Despite the decrease in trust, we observe a significant reduction in system complexity, indicating that reflective questions may streamline the decision-making process by focusing user attention on key aspects.","Causal Strength Justification: Causal mechanism: 'indicating that' and 'by focusing' show reflective questions cause reduced complexity via attention focus. | Relevance Justification: Shows causal effect of AI feature (reflective questions) on human/system performance (complexity reduction, decision-making), though not explicitly linked to non-deterministic/opacity/adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI confidence levels (CLs) and making AI uncertainty explicit | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users better assess when and how much to rely on AI suggestions, and user performance significantly improves","Research shows that providing CLs allows users to better assess when and how much to rely on the AI’s suggestions [62, 63, 26, 27]. In our study, we observe a similar pattern: AI CLs (C3) significantly improve user performance, showing that making the AI’s uncertainty explicit helps users critically engage with its outputs.","Causal Strength Justification: Direct causal language: 'allows users to better assess' and 'significantly improve user performance' with explicit mechanism 'making the AI’s uncertainty explicit helps users critically engage'. | Relevance Justification: Directly addresses how AI features (CLs and uncertainty) causally affect human performance (assessment and engagement), aligning with the research focus on non-deterministic AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: CLs (confidence levels) | Evidence Type: mechanism | Causal Strength: 7 | Performance Effect: users make more informed judgments without feeling cognitively overwhelmed","the improved perceived accuracy demonstrates that CLs can act as effective cognitive scaffolds, allowing users to make more informed judgments without feeling cognitively overwhelmed.","Causal Strength Justification: Direct causal mechanism: 'CLs can act as effective cognitive scaffolds, allowing users to make more informed judgments' with explicit effect 'without feeling cognitively overwhelmed'. | Relevance Justification: Explicitly links AI feature (CLs) to human performance effect (informed judgments and reduced cognitive overload), relevant to non-deterministic AI characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: CLs and lack of explanations | Evidence Type: indirect | Causal Strength: 6 | Performance Effect: users may not be able to decide whether to rely on AI in complex decision-making scenarios","CLs alone may not suffice for deeper trust calibration. Users may require more explanations to decide whether or not to rely on the AI in complex decision-making scenarios [29, 19].","Causal Strength Justification: Indirect causal relationship: 'may not suffice' and 'may require more explanations' imply that without sufficient explanations, users cannot decide on reliance, linked to trust calibration. | Relevance Justification: Addresses how AI opacity (lack of explanations) affects human performance (trust and decision-making), relevant to the research focus on opacity and trust challenges.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: performance visualization (C6) | Evidence Type: direct | Causal Strength: 6 | Performance Effect: improving user performance, positively influencing users to critically engage with the task, users reflecting more deeply on decisions, and helping users refine judgments","Performance visualization (C6) in our study demonstrates borderline significance in improving user performance, indicating that this mechanism may positively influence users to critically engage with the task. The improvements suggest that users reflected more deeply on the comparison between their own decisions and the AI’s, as evidenced by remarks like, ""I used it to reflect"" and ""The bar chart helped give me a visual ’faith’ in the AI."" The mechanism serves as a cognitive aid, helping users refine their judgments through the visual representation of performance.","Causal Strength Justification: The text uses 'demonstrates borderline significance in improving user performance' and 'indicating that this mechanism may positively influence users,' which suggests a direct but moderate causal link, supported by evidence from user remarks and described mechanisms. | Relevance Justification: This directly addresses how an AI feature (performance visualization) causally affects human performance, including engagement, reflection, and judgment refinement, aligning with the task's focus on AI-human interaction effects.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: visualizing past performance without direct model interpretability | Evidence Type: direct | Causal Strength: 7 | Performance Effect: users rely more on the AI, even when it is incorrect","Visualizing past performance may introduce additional complexity without direct model interpretability, which likely leads users to rely more on the AI, even when it is incorrect.","Causal Strength Justification: Direct causal language 'likely leads to' explicitly connects the AI feature to the human performance effect. | Relevance Justification: Directly addresses how lack of interpretability (opacity) affects human performance by increasing reliance on AI even when incorrect.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users think more critically about AI suggestions and question the AI's reasoning","The reflective questions prompt users to think more critically about the AI’s suggestions, engaging them to question the AI’s reasoning [88].","Causal Strength Justification: Direct causal language 'prompt' explicitly connects reflective questions to users thinking more critically and questioning AI reasoning. | Relevance Justification: Directly addresses how an AI feature (reflective questions) causally affects human performance (critical thinking about AI), though not specifically tied to non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: AI-driven questions (C5) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: significant improvement in reliance calibration; users more likely to follow correct AI suggestions and less likely to follow incorrect ones","AI-driven questions (C5) led to a significant improvement in reliance calibration from Phase 2 to Phase 3, showing users are more likely to follow the AI’s suggestions when they are correct and less likely to follow them when they are incorrect.","Causal Strength Justification: Direct causal language 'led to' explicitly connects AI-driven questions to the improvement in reliance calibration. | Relevance Justification: Directly addresses how an AI feature (AI-driven questions) causally affects human performance (reliance calibration), though not specifically tied to non-deterministic, opacity, or adaptive features.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: transparency and interpretability mechanisms (textual explanations, AI CLs) and reflective engagement mechanisms (AI-driven questions) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduced likelihood of blindly following incorrect suggestions (mitigating over-reliance when AI is incorrect)","These mechanisms help users engage critically with the AI’s outputs, reducing the likelihood of blindly following incorrect suggestions.","Causal Strength Justification: Direct causal language: 'help... reducing' explicitly links the mechanisms to the effect. | Relevance Justification: Directly addresses how AI features (specific mechanisms) affect human performance (reliance on AI), which is the core of the research focus on human-AI interaction and over-reliance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: visual explanations and performance visualizations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: higher risk of over-reliance when AI has errors","On the other hand, mechanisms such as visual explanations and performance visualizations may not provide sufficient guidance, leading to a higher risk of over-reliance when the AI has errors.","Causal Strength Justification: Direct causal language: 'leading to' explicitly links the lack of sufficient guidance to the effect. | Relevance Justification: Directly addresses how AI features (specific mechanisms) affect human performance (over-reliance on AI), which is the core of the research focus on human-AI interaction and over-reliance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: textual explanations (C1) and AI CLs (C3) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: significant improvements in user accuracy and decision-making","Conditions involving textual explanations (C1) and AI CLs (C3) show significant improvements in user accuracy and decision-making, as these mechanisms provide transparency and help users engage critically with AI outputs without overwhelming them.","Causal Strength Justification: Direct causal language 'show significant improvements... as these mechanisms provide...' explicitly links the AI features to the performance effect through a clear mechanism. | Relevance Justification: Directly addresses how specific AI features (textual explanations and AI CLs) causally affect human performance (accuracy and decision-making) in a high-stakes context, aligning with the research focus on human-AI interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: well-calibrated transparency and interpretability | Evidence Type: direct | Causal Strength: 7 | Performance Effect: more effective reliance calibration, allowing users to rely on AI outputs more appropriately without falling into over-reliance or under-reliance traps","Previous research has similarly demonstrated that well-calibrated transparency and interpretability can build more effective reliance calibration, allowing users to rely on AI outputs more appropriately without falling into over-reliance or under-reliance traps [85, 90, 28, 91, 46].","Causal Strength Justification: Direct causal language 'can build more effective reliance calibration' explicitly links the AI features to the performance effect, with a clear subsequent outcome. | Relevance Justification: Directly addresses how AI features (transparency and interpretability) causally affect human performance (reliance calibration and appropriate reliance), which is relevant to the research focus on trust and regulation challenges in high-risk industries.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: performance visualizations/comparison data | Evidence Type: direct | Causal Strength: 7 | Performance Effect: reduced effectiveness of reliance calibration","While users benefit from seeing comparison data, the additional cognitive load required to process this information seems to reduce the overall effectiveness of reliance calibration.","Causal Strength Justification: Direct causal language ('seems to reduce') with clear mechanism (cognitive load from processing information), though moderated by 'seems' which indicates some uncertainty. | Relevance Justification: Directly addresses how an AI feature (performance visualizations providing comparison data) affects human performance (reliance calibration effectiveness) through cognitive mechanisms, aligning with the research focus on human-AI interaction effects.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: increased reflection and mental effort without clear guidance | Evidence Type: direct | Causal Strength: 8 | Performance Effect: cognitive overload and hindered performance","These findings align with cognitive psychology studies, where increased reflection and mental effort, when not accompanied by clear decision-making guidance, can lead to cognitive overload and hinder performance [92, 93, 94].","Causal Strength Justification: Strong direct causal language ('can lead to') with clear outcomes (cognitive overload, hindered performance), supported by reference to established studies. | Relevance Justification: Directly addresses how cognitive processes (increased reflection/mental effort without guidance) affect human performance (hindered performance through cognitive overload), highly relevant to understanding AI interaction effects on human cognition and performance.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: visual explanations (C2) and performance visualizations (C6) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: over-reliance","However, in visual explanations (C2) and performance visualizations (C6), users find it difficult to disengage from incorrect AI outputs, leading to over-reliance.","Causal Strength Justification: Direct causal language 'leading to' explicitly connects the difficulty disengaging from incorrect AI outputs to the effect of over-reliance. | Relevance Justification: Directly addresses how AI features (visual explanations/performance visualizations) affect human performance (over-reliance), which is central to the research focus on AI characteristics and human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: decision-making process lacking sufficient transparency, high cognitive effort to process decision information | Evidence Type: direct | Causal Strength: 7 | Performance Effect: over-rely on automation","Users tend to over-rely on automation [97, 41, 36, 98], especially when the decision-making process lacks sufficient transparency or when the cognitive effort to process the decision information is high.","Causal Strength Justification: Direct causal language 'especially when' explicitly connects the conditions (lack of transparency, high cognitive effort) to the effect of over-reliance on automation. | Relevance Justification: Directly addresses how AI features (lack of transparency in decision-making, high cognitive effort) affect human performance (over-reliance on automation), which is central to the research focus on AI characteristics and human interaction.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: reflective questions (as part of AI design) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increased critical engagement with AI outputs, introduction of skepticism, reduced trust in AI reasoning","The cognitive load imposed by reflective questions may lead users to engage more critically with AI outputs, but it also introduces skepticism. Users find it more challenging to fully trust the AI’s reasoning [101, 27].","Causal Strength Justification: Direct causal language ('may lead to', 'introduces') explicitly connects the cause (cognitive load from reflective questions) to the effects (critical engagement, skepticism, trust challenges). | Relevance Justification: Directly addresses how an AI design feature (reflective questions) causally affects human performance (critical engagement, trust, skepticism), which aligns with the research focus on human-AI interaction effects.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: CFF mechanisms like human feedback and AI-driven questions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reducing performance","However, CFF mechanisms like human feedback and AI-driven questions, while encouraging deeper engagement, sometimes impose additional cognitive demands, thereby reducing performance.","Causal Strength Justification: Direct causal language with mechanism: 'impose... thereby reducing...' explicitly links the AI feature to the performance effect. | Relevance Justification: Directly addresses how an AI feature (CFF mechanisms) causally affects human performance (reducing it) in a high-stakes scenario, though not explicitly tied to the three novel characteristics.","y"
"Engaging With AI: How Interface Design Shapes Human-AI Collaboration in High-Stakes Decision Making","","","AI Feature: explanation mechanisms (textual explanations and AI CLs) that facilitate transparent, interpretable reasoning | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improve decision-making accuracy","Our findings reveal the following dynamics: explanation mechanisms that facilitate transparent, interpretable reasoning, such as textual explanations and AI CLs, improve decision-making accuracy by helping users better align their choices with AI suggestions.","Causal Strength Justification: Direct causal language with mechanism: 'improve... by helping...' explicitly links the AI feature to the performance effect. | Relevance Justification: Directly addresses opacity/explainability (a novel AI characteristic from the research focus) and its causal impact on human decision-making performance in a high-stakes scenario.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: AI explainability challenges","","","Despite advances in AI explainability, challenges persist in fostering appropriate reliance.","This directly references AI explainability, which relates to opacity/lack of explainability as a novel AI characteristic, highlighting trust and regulation issues in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI integration requirements for high-risk contexts","","","The research demonstrates that successful AI integration in high-risk contexts requires domain-specific calibration, integrated sociotechnical design addressing trust calibration and skill preservation simultaneously, and proactive measures to maintain human agency and competencies essential for safety, accountability, and ethical responsibility.","It directly mentions AI integration features such as domain-specific calibration, sociotechnical design, and maintaining human agency, which are novel AI characteristics relevant to high-risk industries compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Data processing and decision support capabilities","","","AI systems are increasingly capable of processing vast datasets, detecting complex patterns, and generating decision support recommendations with a speed and scalability that surpass human capabilities [2].","This excerpt describes AI system features related to processing data, detecting patterns, and generating recommendations, which are key capabilities distinguishing AI from conventional automation in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: Uncertainty communication and interactive features","","","We identified that visual presentations, interactive features, and uncertainty communication significantly influence trust calibration, with simple visual highlights proving more effective than complex presentation and interactive methods in preventing over-reliance.","This excerpt mentions uncertainty communication as a feature that influences trust calibration, which relates to handling uncertainty in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Context-aware task allocation and adaptive designs","","","complementary role architectures that amplify rather than replace human judgment, adaptive user-centered designs tailoring AI support to individual decision-making styles, context-aware task allocation dynamically assigning responsibilities based on situational factors, and autonomous reliance calibration mechanisms empowering users’ control over AI dependence.","This excerpt describes AI systems that adapt to individual decision-making styles and dynamically assign responsibilities based on situational factors, representing context-aware and adaptive behavior.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: intuitive presentation and interaction","","","Effective AI systems should provide insights through intuitive presentation and interaction approaches, fostering a symbiotic relationship that calibrates trust and promotes appropriate reliance on AI recommendations.","This describes an AI system feature related to how it presents information and interacts with users, which is a key aspect of human-AI collaboration and interface design.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Machine learning-based predictive analytics","","","the Targeted Real-time Early Warning System (TREWS) employs machine learning to analyze real-time physiological data, predicting sepsis risk hours in advance and issuing timely alerts.","This directly describes an AI capability (machine learning) performing data-driven analysis and prediction, which is a core feature distinguishing AI from conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Performance enhancement and efficiency improvement","","","AI’s capacity to surpass human performance in specific tasks, enhance workforce efficiency, improve task quality, and narrow skill disparities among workers [4].","This explicitly lists multiple AI capabilities (surpassing human performance, enhancing efficiency, improving quality, narrowing skill gaps) that represent key features of AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: transparency requirement for trust and safety","","","medical diagnostic AI must prioritize transparency to foster clinician trust and patient safety [ 11]","This directly addresses the opacity/explainability characteristic by highlighting the need for transparency in AI systems to foster trust and ensure safety, which is a key challenge in high-risk industries like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Task efficiency and accuracy","","","Existing research has extensively documented AI’s immediate advantages in task efficiency and accuracy [ 4,6]; however, comprehensive analyses investigating the long-term effects of sustained human–AI interaction on human cognitive strategies, decision-making capabilities, and professional competencies remain limited.","This excerpt explicitly mentions AI's capabilities in task efficiency and accuracy, which are key features distinguishing AI from conventional automation, as highlighted in the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI-driven diagnostic tools","","","For instance, research highlights cognitive risks such as automation bias, where over-reliance on AI recommendations can lead to diminished decision-making skills and mental inertia, particularly in healthcare settings where clinicians increasingly depend on AI-driven diagnostic tools [ 7,8].","This excerpt describes AI-driven diagnostic tools as a feature, indicating AI's role in high-risk decision-making scenarios, which relates to the research focus on novel AI characteristics in industries like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI integration in high-risk decision making","","","(3) the ethical and practical implications of AI integration in high-risk decision making, with particular attention to trust calibration, skill degradation, and accountability across different domains.","This describes AI system capabilities for integration into high-risk decision-making contexts, with specific attention to trust calibration, skill degradation, and accountability - features that go beyond conventional automation in complex, high-stakes environments.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: AI presentation and interaction approaches","","","(2) the influence of AI presentation and interaction approaches on trust calibration and reliance behaviors;","This identifies AI system features related to how AI is presented and interacts with humans, specifically affecting trust and reliance - characteristics that distinguish AI from conventional automation interfaces.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Decision-making autonomy","","","(1) design strategies that enable AI systems to support humans’ intuitive capabilities while maintaining decision-making autonomy;","This directly describes an AI system feature where AI maintains its own decision-making authority while supporting human capabilities, contrasting with conventional automation that typically follows predetermined rules without autonomy.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Collaborative partnership augmenting human cognition","","","Contemporary AI systems transcend traditional automation, functioning as collaborative partners that augment human cognitive capabilities [ 18].","This represents a general AI capability that distinguishes novel AI from conventional automation, emphasizing its role as a collaborator rather than just a tool, which aligns with the research focus on human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Adaptive capabilities at cost of interpretability","","","Early systems offered transparent but rigid assistance, while the rise in machine learning and deep learning enabled adaptive capabilities in tasks like image recognition and natural language processing, often at the cost of interpretability [ 16].","This directly addresses the opacity/explainability characteristic from the research focus, highlighting how adaptive AI systems (via machine/deep learning) can lack interpretability, which is a key novel feature versus conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Deterministic nature and lack of intentionality","","","Trust in HAIC differs fundamentally from interpersonal trust due to the deterministic nature of AI systems and their lack of intentionality [ 31].","This directly mentions a key characteristic of AI systems (deterministic nature and lack of intentionality) that distinguishes them from human interactions, which is relevant to understanding AI features in human-AI interaction contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI design minimizing cognitive load","","","Similarly, Cognitive Load Theory distinguishes between intrinsic, extraneous, and germane cognitive load [ 21], suggesting that effective AI design should minimize irrelevant information processing while supporting meaningful pattern construction and respecting human cognitive limitations [ 22].","This excerpt discusses AI design principles that aim to optimize information processing and support human cognition, representing a general AI capability focused on enhancing human-AI interaction through cognitive load management.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: structured outputs for human awareness support","","","For AI systems to support rather than hinder human awareness, their outputs must be structured to enhance all three levels without overwhelming cognitive capacity.","This excerpt describes an AI capability related to designing outputs that enhance human situational awareness, which is a general AI feature relevant to human-AI interaction in decision-making contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: transparent and contextually relevant explanations","","","This balance requires AI systems to deliver not only accurate recommendations but also transparent, contextually relevant explanations that align with human cognitive processes, particularly in high-stakes decision-making environments.","This excerpt directly addresses the need for transparency and explainability in AI systems, which relates to the opacity/explainability characteristic by emphasizing transparent explanations to mitigate black-box behavior and trust challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Collaborative decision-making capability","","","AI has evolved into an integral collaborative partner in complex decision-making processes. This evolution has enabled HAIC to deeply penetrate various vertical domains, revealing distinct collaborative patterns and challenges, particularly in high-risk decision environments where human intuition intersects with AI-driven insights.","This excerpt describes AI's role as a collaborative partner in complex decision-making processes, highlighting its capability to work alongside humans in high-risk environments, which is a key feature distinguishing advanced AI from conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI (XAI) for interpretable outputs","","","AI explainability is critical for intuitive decision making, characterized by rapid, experience-based judgments informed by pattern recognition [26]. Explainable AI (XAI) refers to AI systems’ ability to provide interpretable and understandable outputs that allow users to comprehend how decisions are made [27]. This explainability enables users to validate AI recommendations against their expertise and make informed decisions about when to rely on AI assistance.","This directly addresses the opacity/explainability characteristic by describing AI systems' ability to provide interpretable and understandable outputs, allowing users to comprehend how decisions are made, which contrasts with black-box behavior.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: AI presentation and interaction techniques","","","The AI presentation and interaction techniques include various modalities: visual representations (such as saliency maps in medical imaging or decision paths in diagnostic systems), natural language explanations that articulate reasoning processes, interactive elements allowing users to query specific decisions, and uncertainty visualizations that communicate confidence levels.","This describes specific interface types and feedback mechanisms for AI systems, such as visual representations, natural language explanations, interactive querying, and uncertainty visualizations, which are relevant to human-AI interaction and explainability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Opacity of AI reasoning","","","Yet, challenges persist, including the opacity of AI reasoning, which can erode trust, and ethical dilemmas surrounding autonomous weapon systems, necessitating robust human oversight to align with international norms [38,39].","This directly references the opacity (lack of explainability) of AI reasoning, which is a key novel AI characteristic compared to conventional automation, as it can undermine trust and necessitate oversight.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Interpretability requirement for trust","","","the need for interpretable AI outputs to build clinician trust","This directly addresses the opacity/explainability characteristic by highlighting the need for interpretable AI outputs to overcome black-box challenges and build trust, which is a key issue in human-AI interaction for high-risk industries like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI-enhanced diagnostic precision and collaborative decision-making","","","AI algorithms enhance diagnostic precision by analyzing medical imaging and patient data, collaborating with clinicians who validate and contextualize findings [ 11]. Decision patterns often feature AI proposing evidence-based options while humans retain final authority to account for patient-specific factors.","This excerpt describes AI capabilities in analyzing data to enhance precision and its role in a hybrid decision-making model, highlighting AI's data-driven and collaborative features in healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Adaptive content and feedback delivery","","","AI delivering adaptive content and feedback while educators interpret analytics to refine teaching strategies [13].","This directly describes AI's context-aware and adaptive behavior in dynamically responding to individual student needs by delivering personalized content and feedback, which is a novel characteristic compared to static conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Lack of transparency and trust issues","","","Transparency and trust are the most significant issues.","This directly addresses the opacity/explainability category by highlighting transparency as a critical issue for AI systems, which relates to black-box behavior and trust challenges mentioned in the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Data-driven reliance","","","AI systems often rely on vast personal datasets, raising concerns about consent and security, particularly in sensitive areas like medical diagnostics and intelligence analysis [49].","This represents an AI feature as it highlights AI systems' capability to process large datasets, which is a key characteristic distinguishing them from conventional automation, especially in high-risk contexts like healthcare and intelligence.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Adapting to dynamic environments","","","Challenges include ensuring safe human–robot interactions, adapting to dynamic environments, and addressing ethical issues tied to workforce displacement and automation biases.","This mentions the need for AI systems to adapt to dynamic environments, which is a context-aware/adaptive behavior characteristic of novel AI systems, as they must respond to changing conditions unlike static conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Real-time decision making and adaptability to unforeseen changes","","","Collaborative robots (cobots) assist in real-time decision making on factory floors, with humans guiding adaptability to unforeseen changes [ 45].","This directly describes context-aware/adaptive behavior where AI systems (cobots) respond dynamically to the environment (factory floors) and adapt to unforeseen changes in real-time, which is a novel AI characteristic compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Rapid data processing","","","Decision patterns here blend AI’s rapid data processing with human oversight for safety and innovation.","This highlights AI's computational capability (rapid data processing) as part of decision-making patterns, which is a general AI feature that enables enhanced performance compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI as a collaborative partner","","","Rather than viewing ethics as a secondary consideration, integrating these concerns into the design process is essential for fostering appropriate reliance, where humans leverage AI as a partner without losing agency or ethical grounding.","This excerpt explicitly mentions 'AI as a partner', which represents a novel AI characteristic involving human-AI collaboration and shared decision-making, contrasting with conventional automation that typically operates independently or under strict human control.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Trust and accountability erosion","","","In domains such as healthcare and defense, where decisions carry ethical weight, opaque AI can erode trust and complicate accountability, leaving decision-makers uncertain about how to balance AI recommendations with their own expertise [ 19].","This highlights the consequences of AI opacity in critical industries, specifically how it challenges trust, regulation, and human-AI interaction by making accountability difficult.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Black-box opacity","","","The “black box” nature of many AI systems obscures their decision-making processes, making it difficult for users to assess the validity of outputs or integrate them with their own intuition [ 48].","This directly describes the opacity/lack of explainability characteristic of AI systems, where internal reasoning is not visible, creating trust and interpretability issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: Data-driven bias and skewed outcomes","","","Furthermore, biases embedded in training data—whether from societal inequalities or developer assumptions—can lead to skewed outcomes, amplifying ethical risks and undermining fairness [ 5].","This relates to non-deterministic/data-driven decision-making, as biases in data lead to variable and unpredictable outputs that can result in unfair or uncertain failures.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI systems supporting human intuition and autonomy","","","RQ1: What design strategies enable AI systems to support humans’ intuitive capabilities while maintaining decision-making autonomy?","This represents an AI feature as it directly addresses AI systems' capabilities in human-AI interaction, focusing on design strategies that enhance human intuition and maintain autonomy, which aligns with novel AI characteristics like context-aware behavior and adaptive support in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI as research assistant","","","In this stage, AI functioned as a “research assistant”, providing comprehensive background information and preliminary analysis without making final judgments.","This describes a specific AI capability (functioning as a research assistant) that involves providing structured information extraction and preliminary analysis, which relates to AI's role in processing and organizing data without autonomous decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: structured information provision for human augmentation","","","the structured information provided by AI helps human researchers to focus more efficiently on core issues requiring professional judgment, rather than spending substantial amounts of time on basic information extraction.","This describes an AI capability (providing structured information) that enhances human efficiency by handling basic tasks, which is a characteristic of AI systems versus conventional automation in human-AI interaction contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI computational efficiency with human decision priority","","","This approach leverages the computational efficiency of AI for structured data extraction while prioritizing the nuanced judgment of human researchers in final decision making [55,56].","This excerpt describes an AI feature where AI enhances efficiency in structured tasks, but human judgment is central to decision-making, reflecting a collaborative framework that balances AI capabilities with human expertise in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI-assisted structured processing with human oversight","","","AI handles the efficient processing of structured information extraction tasks, while human researchers retain the ultimate authority over judgments concerning research relevance and scholarly value.","This represents an AI feature where AI performs specific computational tasks (structured information extraction) efficiently, highlighting its role in augmenting human capabilities rather than replacing them, which is a key characteristic in human-AI interaction frameworks.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Large-scale processing and summarization","","","This method leverages AI’s ability to process and summarize large volumes of literature efficiently [57,58]. This collaborative workflow illustrates a complementary role architecture: AI handles large-scale, structured reading and drafting, while human experts retain control over all evaluative and interpretive judgments.","This directly describes AI's ability to handle large-scale, structured tasks (reading and drafting) efficiently, which is a key AI capability compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Trust and Cognitive Mechanisms in AI Interaction","","","The research landscape consequently shifted toward HAIC, evidenced by post-2023 studies emphasizing trust dynamics, cognitive mechanisms, and practical applications—reflecting both scholarly interest and growing societal demand (shown in Figure 3).","This excerpt highlights AI features related to trust dynamics and cognitive mechanisms, which are key aspects of human-AI interaction, reflecting AI's advanced capabilities in decision-making and adaptability compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: adaptive system design","","","This stage focused on (1) the increased application of generative AI in collaborative settings, (2) greater emphasis on interaction models and adaptive system design, (3) deeper explorations of ethical considerations and trust mechanisms, and (4) expansion into diverse professional fields and real-world applications.","This excerpt mentions 'adaptive system design' as a key focus in recent HAIC trends, representing a context-aware/adaptive AI feature that responds dynamically to environments, aligning with novel AI characteristics versus conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: Adaptation to Individual Decision-Making Styles","","","effective HAIC requires systems that amplify rather than replace human judgment, adapt to individual decision-making styles, and provide users with meaningful control over the decision process.","This describes an AI system feature where the system adapts to individual user styles, which relates to context-aware/adaptive behavior by responding dynamically to user-specific needs.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: explainable AI and trust dynamics","","","Trust and Explainability (27 papers, 32.9%) explores trust dynamics and explainable AI, with recent work (2024–2025) addressing trust calibration and cognitive biases.","This directly mentions 'explainable AI', which relates to the opacity/explainability characteristic of AI systems, addressing challenges in transparency and trust.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: adaptive systems for collaboration","","","Interaction Patterns and Collaboration Boon Frameworks (14 papers, 17.1%) designs adaptive systems for seamless collaboration, emphasizing communicative agents in healthcare and security.","This mentions 'adaptive systems' designed for collaboration, which aligns with the context-aware/adaptive behavior characteristic, involving dynamic response and environment adaptation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Human-AI complementarity via Bayesian belief revision","","","Three theoretical positions have emerged regarding how human–AI complementarity should be structured. The first, represented by Reverberi et al. [ 1], frames complementarity as Bayesian belief revision; endoscopists in their study integrated AI recommendations with clinical assessments while retaining diagnostic control.","This excerpt directly discusses an AI capability—specifically, how AI recommendations are integrated with human assessments in a complementary role architecture, which relates to AI decision-making and control mechanisms in high-risk settings like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Predictive capability for average outcomes","","","Scholes’s research reinforced this concern, noting that while AI effectively predicts average outcomes, humans remain critical for rare high-stakes events— yet often fail to recognize when override is appropriate [ 61].","This excerpt describes an AI capability (predicting average outcomes) and highlights a limitation in human-AI interaction (failure to recognize override needs), which relates to AI decision-making features in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: feedback | Feature: AI feedback mechanism","","","AI feedback increased conversational empathy by 19.6% among peer supporters while preserving response autonomy [47].","This represents an AI feature as it describes AI providing feedback to improve human interaction (conversational empathy), which is a novel capability compared to conventional automation, involving data-driven or adaptive elements in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI data analysis capability","","","AI handles data analysis while humans retain content creation authority [60].","This represents an AI feature as it highlights AI's role in handling data analysis, a key capability in human-AI complementarity, distinguishing it from traditional automation by focusing on cognitive tasks.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: Computational adaptation for AI support adjustment","","","Research on adaptive design divides between computational adaptation and interactive co-creation approaches. Computational adaptation aims to automatically adjust AI support based on inferred user states.","This directly mentions an AI feature: computational adaptation that automatically adjusts AI support, which relates to context-aware/adaptive behavior in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: AI capabilities and limitations information requirement","","","Cai et al. found that pathologists required comprehensive information about AI capabilities and limitations to determine effective partnership strategies—suggesting that metacognitive support may mediate complementarity’s success [ 62].","It directly mentions 'AI capabilities and limitations,' which relates to opacity/explainability as it involves understanding AI's internal workings for effective human-AI interaction, addressing trust and regulation challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Progressive collaboration levels from transparent AI","","","The Kase et al. framework for military decision making proposed progressive levels of collaboration from transparent AI to theory-of-mind teaming, but remains unvalidated empirically [36].","It mentions 'transparent AI,' which directly addresses opacity/explainability by proposing levels of collaboration that aim to make AI behavior more understandable and trustworthy in high-risk decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: adaptive autonomy","","","Hauptman et al. showed that adaptive autonomy in cybersecurity—higher automation for predictable tasks, lower for uncertain ones—improved collaboration by matching workflow patterns [ 65].","It represents a context-aware/adaptive AI feature by dynamically responding to task difficulty (predictable vs. uncertain) to match workflow patterns, highlighting variability handling and real-time adaptation in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Simplified AI with static predictions","","","However, Gomez et al. used a simplified AI with static predictions and academically sophisticated participants;","This directly mentions an AI system feature (simplified AI with static predictions), which contrasts with more advanced or dynamic AI characteristics, indicating a specific capability or limitation in the context of human-AI interaction research.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: automation | Feature: Automation Levels Testing","","","Meske and Ünal tested five automation levels in face recognition and found no universal optimum—preferences varied significantly across individuals [72].","This excerpt directly mentions 'automation levels' being tested in an AI system (face recognition), which relates to AI capabilities and how they are implemented and perceived, aligning with the research focus on novel AI characteristics versus conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: Interactive co-creation and user participation in AI prediction generation","","","Interactive co-creation takes a different approach, involving users in shaping AI assistance. Gomez et al. found that user participation in AI prediction generation for bird classification increased recommendation acceptance and teamwork perceptions [ 66].","This excerpt directly mentions AI assistance and AI prediction generation, which are AI capabilities, and describes user involvement in shaping them, representing an interface feature for human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: User modification of feature weights in AI models","","","Muijlwijk et al. showed that allowing marathon coaches to modify feature weights improved both model acceptance ( β= 0.266, p< 0.001) and prediction accuracy (error reduced from 3.14% to 2.33%) [ 67].","This excerpt describes a feature where users can modify feature weights in an AI model, which is an interface mechanism for human-AI interaction that enhances model acceptance and accuracy, directly relating to AI capabilities.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: LLM-generated content for sensemaking","","","Liu et al. developed Selenite, using LLM-generated overviews and questions to accelerate interdisciplinary sensemaking while preserving researcher autonomy [68].","This represents an AI feature as it involves LLM-generated content (overviews and questions) to enhance human-AI interaction in sensemaking tasks, showcasing AI's capability to assist in complex cognitive processes.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI-assisted molecular deconstruction with static pathways","","","Shi et al. showed that chemists using RetroLens for molecular deconstruction experienced reduced cognitive load, though the system’s static recommendation pathways limited flexibility [70].","This represents an AI feature as it involves an AI system (RetroLens) that assists chemists in molecular deconstruction, reducing cognitive load, though it has limitations in flexibility due to static recommendation pathways.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: Human-AI co-exploration interface","","","Zheng et al. demonstrated DiscipLink’s human–AI co-exploration for information seeking [69].","This represents an AI feature as it describes a human-AI co-exploration interface for information seeking, highlighting AI's role in collaborative interaction and support for user tasks.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: automation | Feature: human-centered automation framework","","","Choudari et al. proposed human-centered automation for data science, preserving intuitive decision making, but their framework awaits empirical validation [60].","It mentions 'human-centered automation for data science', which is an AI capability involving automation levels and decision-making, aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: Computational adaptation for personalization","","","Computational adaptation assumes that user states can be accurately inferred from behavioral signals and that optimal support configurations can be derived from these inferences—yet the evidence that self-reported measures fail to align with behavioral measures [ 74] suggests that even users themselves may not have accurate insight into their support needs [ 74].","This describes an AI feature where computational adaptation uses behavioral signals to infer user states and derive support configurations, highlighting data-driven decision-making and potential uncertainty in user insight, aligning with novel AI characteristics like non-deterministic behavior and context-aware adaptation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Context-aware task allocation","","","Context-aware allocation addresses how decision-making responsibilities should be distributed based on task characteristics and situational factors. Research divides between taxonomic approaches specifying predetermined allocations and dynamic approaches adjusting in real-time.","This represents a context-aware/adaptive AI feature because it involves dynamic response to task characteristics and situational factors, with real-time adjustments, aligning with the research focus on context-aware/adaptive behavior in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI performance in goal-directed planning","","","Lin et al. found GPT-3 underperformed compared to human assistants in goal-directed planning despite generating longer dialogs, suggesting humans should retain strategic decision control [ 81].","This excerpt discusses an AI system's (GPT-3) capability in goal-directed planning, highlighting its underperformance relative to humans, which relates to AI decision-making features in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: adaptive frameworks for alert fatigue mitigation","","","Chen et al. proposed adaptive frameworks for security operations centers to mitigate alert fatigue [80].","This excerpt directly mentions 'adaptive frameworks,' which aligns with the context-aware/adaptive behavior category, indicating AI systems that can adjust to dynamic environments like security operations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: control | Feature: human-AI teaming with human control over novel threats","","","Jalalvand et al. showed that human–AI teaming in alert prioritization improved performance through the automation of routine tasks, with human control over novel threats [79].","This excerpt explicitly mentions 'human–AI teaming' and specifies a control mechanism where AI handles routine automation while humans manage novel situations, representing a key AI feature in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: Dynamic AI influence adaptation","","","Temporal dynamics add another dimension. Flathmann et al. found that decreasing AI influence over time enhanced human performance, while sustained high influence increased cognitive workload [ 83]. This suggests allocation should not be static but should evolve as collaboration develops.","This represents an AI feature because it describes how AI influence changes over time (decreasing or sustained high) in response to collaboration dynamics, which relates to adaptive behavior in human-AI interaction, a key characteristic of novel AI systems versus static conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI capability model overestimation in complex conditions","","","The finding from Lin et al. [81] that GPT-3 underperformed human assistants in strategic planning—despite being allocated precisely the kind of reasoning-intensive task that taxonomies would suggest favoring AI—indicates that current models of AI capability may systematically overestimate performance in complex real-world conditions.","This excerpt directly discusses AI capability models and their potential to overestimate performance in complex real-world conditions, which relates to AI system features and limitations compared to human performance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI recommendation rejection mechanisms","","","Research has identified specific mechanisms through which users appropriately reject AI recommendations. Chen et al. documented three intuition-driven override pathways: strong outcome intuition, discrediting AI through feature analysis, and recognizing AI limitations [ 29]. All pathways improved outcomes when users detected AI unreliability.","This represents an AI feature related to non-deterministic behavior and opacity, as it discusses AI unreliability and user mechanisms to override AI recommendations, which aligns with unpredictable failures and trust/regulation challenges in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: feedback | Feature: AI-mediated communication with human oversight","","","Kreps and Jakesch demonstrated that AI-mediated communication with human oversight increased trust when AI performed well, but poor performance eroded confidence [ 88].","This represents an AI feature as it involves AI-mediated communication, a capability of AI systems that influences trust through performance feedback, relevant to human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: explainability and error detection","","","leading to undetected errors [ 87]. This occurred specifically through “False Confirmation” errors where clinicians failed to identify AI mistakes.","This excerpt addresses the opacity/explainability feature by describing how explainable AI can result in undetected errors, specifically 'False Confirmation' errors, indicating challenges in AI transparency and user interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: explainability and trust","","","The relationship of explainability to calibration is more complex than initially assumed. Rosenbacke found that explainable AI increased trust but risked promoting over-reliance,","This directly discusses explainability as an AI feature, highlighting its impact on trust and potential negative consequences like over-reliance, which relates to opacity and explainability challenges in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: Uncertainty communication and reliance","","","Recent works have explored uncertainty communication for calibration. Xu et al. examined how LLM-verbalized uncertainty influences reliance, but findings were limited to U.S. participants familiar with AI and to specific uncertainty framings [ 91].","This represents a non-deterministic AI feature as it involves uncertainty in AI outputs (LLM-verbalized uncertainty) and its impact on human reliance, which relates to variable outputs and unpredictable interactions in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Adaptive agents in augmented reality","","","Syiem et al. found that adaptive agents in augmented reality mitigated attentional issues technically but produced no significant overall task performance improvement—benefits were limited to receptive users [ 93].","It explicitly mentions 'adaptive agents,' which aligns with the context-aware/adaptive behavior characteristic of novel AI systems, involving dynamic response and environment adaptation in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI and false assurance","","","Rosenbacke’s [ 87] finding that explainable AI can promote over-reliance through “False Confirmation” errors suggests that making AI reasoning visible does not straightforwardly enable users to identify when AI is wrong—it may instead provide false assurance that errors have been checked for and ruled out.","This directly addresses the opacity/explainability characteristic by discussing how making AI reasoning visible (explainable AI) does not necessarily solve trust/regulation challenges and can instead create false assurance about error identification.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Limitations of explanations for calibration","","","The field’s assumption that well-designed explanations will enable appropriate calibration appears increasingly untenable in light of this evidence.","This relates to opacity/explainability by challenging the assumption that explanations (which address black-box behavior) effectively enable users to calibrate trust appropriately, highlighting a key limitation in AI system design.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: Autonomous calibration mechanisms","","","Autonomous calibration mechanisms face the troubling finding that transparency may promote rather than prevent over-reliance.","This represents an AI feature because it involves autonomous calibration, which relates to adaptive behavior and feedback mechanisms in AI systems, as part of context-aware/adaptive characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Context-aware allocation","","","Context-aware allocation research relies predominantly on virtual environments and gaming contexts that cannot establish ecological validity for safety-critical applications.","This represents an AI feature because it involves context-aware behavior, where AI systems dynamically respond to and adapt based on environmental factors, as specified in the research focus on context-aware/adaptive behavior.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Interpretability enhancement via visual explanations","","","Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) visualizations enhanced interpretability and increased human trust in AI decisions for malware detection tasks [101].","This directly addresses the opacity/explainability characteristic by showing how visual explanations (LIME and SHAP) enhance interpretability of AI decisions, which helps mitigate black-box behavior and trust challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: AI presentation and interaction approaches","","","3.3. RQ2: How Do AI Presentation and Interaction Approaches Influence Trust Calibration and Reliance Behaviors in HAIC? This section presents findings from 34 studies examining how different AI presentation and interaction approaches affect trust calibration and reliance behaviors in HAIC systems.","This directly addresses AI system features related to how AI presents information and interacts with users, which is a key aspect of human-AI interaction systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: Visual presentation techniques","","","3.3.1. How AI Systems Present Information to Users Method 1: Visual Presentations-Showing Users What AI “Sees” Visual presentation techniques emerged as the most extensively studied approach for influencing trust calibration and reliance behaviors.","This explicitly mentions an AI system feature: visual presentation techniques that show users what the AI ""sees,"" which is a specific method for information presentation in AI systems to affect user trust and reliance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: visual presentation complexity in AI systems","","","Simple visual highlights consistently outperformed more complex presentation in reducing over-reliance on AI systems during difficult tasks [ 102].","It directly mentions 'AI systems' and describes how interface design (visual highlights) influences user behavior (over-reliance), which is a key aspect of human-AI interaction features.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: AI unreliability and failure identification","","","example-based presentations provided clearer signals of AI unreliability, supporting appropriate reliance behaviors by helping users identify when AI systems might fail.","This excerpt directly mentions 'AI unreliability' and 'when AI systems might fail,' which aligns with the non-deterministic/data-driven characteristic of AI systems having unpredictable failures, as specified in the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: collaborative human-AI interaction","","","Notably, this interactive approach also improved the model’s prediction accuracy, suggesting mutual benefits from collaborative human–AI interaction. Similar collaborative benefits were observed in financial forecasting tasks, where interactive policy modification led to significant team development and outperformed static approaches to promoting appropriate reliance behaviors [ 86].","This represents an AI feature related to human-AI interaction, specifically collaborative interfaces that enable mutual benefits and improved prediction accuracy through interactive approaches, which is a key aspect of novel AI systems compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: control | Feature: User-modifiable feature weights in prediction models","","","When coaches could modify the importance weights of previous races in marathon finish time predictions, both the acceptance of the model’s recommendations and perceived model competence improved substantially [ 67].","This illustrates a control mechanism where users can adjust AI model parameters (feature weights), which is a specific AI capability for enhancing user trust and acceptance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: Interactive frameworks and prediction models with user control","","","Interactive frameworks that enabled user engagement with AI recommendations demonstrated particularly strong effects upon trust calibration and reliance behaviors. Interactive prediction models allowing users to adjust feature weights showed significant benefits for both user perception and behavioral outcomes.","This describes interactive AI system features that enable user engagement and control over AI recommendations, which is a key aspect of human-AI interaction interfaces.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: Uncertainty Communication in AI Decision-Making","","","The communication of AI system confidence and uncertainty emerged as a crucial factor in trust calibration and reliance decision making. The explicit display of correct likelihood information proved effective in promoting appropriate trust behaviors. Research comparing three strategies based on estimated human and AI correctness likelihood—Direct Display, Adaptive Workflow, and Adaptive Recommendation—found that all three approaches promoted more appropriate human trust in AI, particularly reducing over-trust when AI provided incorrect recommendations [105].","This excerpt directly mentions AI system confidence and uncertainty, which relates to non-deterministic/data-driven decision-making as it involves variable outputs and uncertainty in AI recommendations, impacting trust and reliance in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: explanatory AI systems with justifications and AR-based visual guidance","","","AI systems providing justifications and AR-based visual guidance demonstrated improved task performance, shared awareness, and reduced errors compared to systems without these explanatory elements [107].","This directly describes AI system features (justifications and AR-based visual guidance) that enhance human-AI interaction by improving performance, awareness, and reducing errors, which relates to explainability and interface characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI for Decision-Making Transparency","","","Method 3: Explaining the Process: How AI Describes Its Decision Making Contextual and process-based interaction approaches focused on explaining how and why AI decisions were made, influencing both trust and reliance through enhanced understanding. The concept of Shared Mental Models (SMMs) provided a theoretical foundation for these approaches, with explainable AI serving as a key enabler for establishing SMMs in human–AI teams by allowing humans to form accurate mental models of AI teammates [106].","This represents an AI feature related to opacity/explainability, as it addresses the lack of transparency in AI systems by focusing on how AI describes its decision-making, which helps mitigate black-box behavior and improves trust and understanding in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainability through visual methods","","","Feature-based visual methods like Grad-CAM in medical imaging increased physicians’ diagnostic trust by aligning with familiar cognitive models, while LIME/SHAP approaches in cybersecurity applications bolstered experts’ confidence through clear decision rationales [101]. These approaches achieved trust enhancement by making AI reasoning visible and connected to domain knowledge.","This directly addresses the opacity/explainability characteristic by describing methods that make AI reasoning visible, which is a key feature for building trust in AI systems, especially in high-risk domains like healthcare and cybersecurity.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI error patterns affecting human delegation","","","humans were less likely to delegate complex predictions to AI when it made rare but large errors, driven by higher self-confidence rather than lower confidence in the model [111].","It represents an AI feature related to non-deterministic behavior, as AI outputs (predictions) can vary with errors, impacting human trust and interaction, which is a novel characteristic compared to conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Trust calibration through uncertainty and correctness information","","","Confidence and uncertainty information proved particularly effective at guiding appropriate reliance behaviors. Providing accurate information about AI recommendations led to better calibrated trust, with participants deviating more from low-quality recommendations and less from high-quality ones [74]. Similarly, correctness likelihood strategies effectively promoted appropriate trust in AI, especially in reducing over-trust when AI provided incorrect recommendations [105].","This represents an AI capability related to human-AI interaction where the AI system provides information about its own confidence, uncertainty, and likelihood of correctness to help users make appropriate reliance decisions, which is particularly relevant for novel AI systems with non-deterministic outputs.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI and trust dynamics","","","the potential for explainable AI to sometimes lead to over-trust [87], and the significant effect of confirmation bias on trust development when AI recommendations aligned with users’ initial judgments [108].","This highlights explainability issues in AI, specifically how explainable AI can inadvertently lead to over-trust and how user biases interact with AI recommendations, addressing opacity and trust challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Transparency mechanisms for trust calibration","","","Transparency mechanisms, such as revealing the top five AI recognitions and showing correct likelihood information, calibrated trust by revealing AI capabilities and often reduced workload perceptions and uncertainty [76,105].","This directly addresses opacity/explainability by discussing transparency mechanisms that reveal AI capabilities to mitigate trust and uncertainty issues, which are key challenges of black-box AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explanatory visualizations in clinical AI","","","In clinical settings, explanatory visualizations increased clinicians’ perceptions of AI usefulness and their confidence in AI’s decisions yet did not significantly affect binary concordance with AI recommendations, suggesting that trust calibration effects may manifest in subtle ways beyond simple compliance measures [109].","This discusses explainability through visualizations in AI, showing how they affect user perceptions and trust in a high-risk industry (healthcare), directly related to opacity and interpretability issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Imperfect explainability in XAI systems","","","the imperfect explanations real XAI systems produce, and Morrison et al.’s [ 104] demonstration that imperfect example-based explanations are more deceptive than their alternatives suggests that findings from idealized conditions may reverse in deployment.","This directly addresses the opacity/explainability characteristic by highlighting the imperfect nature of explanations from real XAI systems, which relates to black-box behavior and trust/regulation challenges in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI decision support systems","","","The integration of AI decision support systems into high-risk human decision-making contexts generates complex ethical and practical implications that fundamentally challenge traditional models of human agency, accountability, and competence.","This excerpt explicitly mentions 'AI decision support systems' as a feature integrated into high-risk contexts, highlighting their role in challenging traditional human decision-making models, which aligns with the research focus on novel AI characteristics in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Adaptive Explanation Mechanisms","","","The theoretical foundation suggests that effective trust calibration requires adaptive explanation mechanisms that respond to user needs and system performance variations.","This excerpt directly mentions 'adaptive explanation mechanisms that respond to user needs and system performance variations,' which aligns with the context-aware/adaptive behavior characteristic of novel AI systems, as they dynamically adjust based on user context and system states.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Capability-Morality Trust Paradox","","","Users consistently perceive AI systems as “capable but amoral”, creating a paradoxical situation where they view AI as technically superior but morally deficient compared to human experts [ 117].","This excerpt describes a novel AI characteristic where users view AI as technically superior but lacking moral judgment, highlighting a trust and perception issue distinct from conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Trust-shaping system attributes","","","system attributes such as reliability, predictability, and transparency, along with human factors such as expertise, workload, and individual differences [113].","This excerpt directly mentions AI system features (reliability, predictability, transparency) that are critical for trust calibration in human-AI interaction, addressing the research focus on novel AI characteristics versus conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: XAI transparency and interpretability","","","XAI theory provides mechanisms for trust calibration through transparency and interpretability [115].","This excerpt explicitly discusses transparency and interpretability as mechanisms for trust calibration in AI systems, directly relating to the research focus on opacity/explainability (black-box behavior) in novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: High learning capacity configurations","","","High control configurations risk over-reliance and the progressive de-skilling of human decision-makers, while high learning capacity configurations create vulnerabilities to automation bias and implementation failures.","The text explicitly mentions 'high learning capacity configurations' as an AI system feature that creates vulnerabilities to automation bias and implementation failures, distinguishing it from conventional automation through its adaptive learning capability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: adaptation | Feature: adaptive influence management","","","AI teammates that decrease their influence over time enable humans to improve their performance, while highly influential AI teammates can increase perceived cognitive workload and potentially inhibit skill development [ 83].","This describes AI systems that dynamically adjust their influence over time, which is a form of context-aware or adaptive behavior, as they respond to temporal dynamics to impact human skill development and maintenance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Adaptive and dynamic AI systems","","","adaptive AI systems that learn and evolve over time. New frameworks emphasizing human agency, oversight, transparency, and accountability are being developed to address these shortcomings, but their effectiveness in high-risk contexts remains largely untested. The challenge lies in creating certification approaches that can accommodate the dynamic nature of AI systems while maintaining the","This excerpt directly mentions 'adaptive AI systems that learn and evolve over time' and 'the dynamic nature of AI systems', which aligns with the context-aware/adaptive behavior category in the research focus, highlighting AI's ability to change and respond dynamically, unlike static conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: adaptability to user preferences","","","Transparency mechanisms intended to improve accountability can paradoxically increase bias and create new risks in some contexts, particularly as systems become more adaptable to user preferences [ 120].","This excerpt directly mentions systems becoming 'more adaptable to user preferences,' which is a key characteristic of context-aware/adaptive AI systems that respond dynamically to user inputs, aligning with the research focus on novel AI features like context-aware/adaptive behavior in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI vs. Black-Box Systems","","","explainable AI increases clinicians’ trust compared to black-box systems, but can lead to dangerous over-reliance, resulting in undetected diagnostic errors [ 87]. This demonstrates that transparency mechanisms can create new vulnerabilities rather than simply enhancing decision quality, particularly in high-risk contexts where the stakes of misplaced trust are severe.","This directly addresses the opacity/explainability characteristic from the research focus, contrasting explainable AI with black-box systems and discussing their impact on trust and decision quality in high-risk environments.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI integration challenges","","","AI integration presents unique challenges including over-reliance, data quality issues, and cybersecurity concerns [ 77].","This excerpt directly mentions AI integration and its associated challenges, highlighting features like over-reliance and data quality issues that are relevant to AI capabilities in safety-critical systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Varied engagement patterns with AI recommendations","","","Healthcare professionals demonstrate varied engagement patterns with AI recommendations—ignoring, negotiating, considering, or relying on system advice [109]. The “negotiation” pattern, where clinicians selectively adopt recommendation aspects rather than accepting or rejecting them outright, highlights the nuanced nature of HAIC in medical decision-making and may represent a critical skill preservation strategy.","This represents an AI feature as it directly discusses how humans interact with AI system outputs (recommendations), highlighting the dynamic and nuanced nature of human-AI collaboration in decision-making contexts, which is a key aspect of AI capabilities in high-risk industries like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainability vs. black-box nature","","","Explainable AI systems increase clinicians’ trust compared to black-box alternatives but can paradoxically lead to dangerous over-reliance, resulting in undetected diagnostic errors [ 87].","This directly addresses the opacity/explainability characteristic by comparing 'explainable AI systems' to 'black-box alternatives', which relates to transparency, trust, and the challenges of interpretability in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Transparency mechanisms and vulnerabilities","","","This demonstrates that transparency mechanisms designed to enhance decision quality can create new vulnerabilities in healthcare settings.","This excerpt discusses 'transparency mechanisms', which are a key aspect of explainability in AI systems, highlighting their potential to create vulnerabilities despite being designed to enhance decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Transparency mechanisms and defined roles","","","These environments require human-centric design approaches, sophisticated transparency mechanisms, and clearly defined human–AI roles to mitigate catastrophic risks.","This represents an AI feature because it discusses transparency mechanisms (related to explainability/opacity) and clearly defined human-AI roles, which are key aspects of AI system design for safety and interaction in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Transparency in AI use for public communication","","","Off-topic and repetitive responses significantly reduce public trust, underscoring the importance of transparency regarding AI use in public communication.","This represents an AI feature as it discusses the impact of AI-generated responses (off-topic and repetitive) on public trust and the need for transparency, which relates to AI capabilities in communication and trust-building.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI-mediated communication with human oversight","","","AI-mediated communication with human oversight can increase constituent trust compared to generic auto-responses in legislative settings, but poorly performing AI language technologies risk damaging constituent confidence and democratic legitimacy [ 88].","This represents an AI feature as it describes AI language technologies used in communication, highlighting their capability to mediate interactions with human oversight, which relates to AI capabilities in public institutional contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI systems affecting governance engagement","","","AI systems that reduce public servants’ engagement with constituents or decision-making processes could undermine democratic responsiveness and institutional legitimacy.","This directly mentions AI systems and their potential impact on decision-making processes, which relates to AI capabilities in governance contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI systems in military accountability contexts","","","Military applications present perhaps the most complex accountability challenges, where current certification frameworks often prove inadequate for AI systems, especially in contexts where command authority and rules of engagement are paramount [119].","This explicitly discusses AI systems in military applications and their challenges with certification frameworks, directly addressing AI capabilities in high-risk domains.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Tripartite Intelligence framework for human-AI interaction","","","The “Tripartite Intelligence” framework demonstrates how combining deep neural networks, large language models, and human intelligence can balance AI scalability with human oversight [121]. This approach offers a model for maintaining human agency while leveraging AI capabilities in high-risk contexts.","This excerpt directly mentions AI capabilities (deep neural networks, large language models) and their integration with human intelligence in a framework designed for high-risk contexts, aligning with the research focus on novel AI characteristics in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: adaptive AI systems","","","traditional certification frameworks that assume static system behavior are structurally incapable of addressing adaptive AI systems, and no validated alternatives exist.","This directly mentions 'adaptive AI systems' as a feature that differs from static systems, aligning with the context-aware/adaptive behavior category in the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: Performance variability and adaptation uncertainty","","","Enhanced interpretability Increased expert trustMalware detection tasksPerformance varies with datasets; emerging threat adaptation unclear","This excerpt directly mentions 'Performance varies with datasets' and 'emerging threat adaptation unclear', which relate to non-deterministic/data-driven decision-making (outputs vary with data) and context-aware/adaptive behavior (dynamic response to threats), key novel AI characteristics in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: interface | Feature: AI presentation methods and feedback mechanisms","","","Table 2 examines the relationship between AI presentation methods and user performance outcomes, highlighting the critical role of presentation design in shaping collaboration effectiveness. The evidence shows that presentation methods significantly influence both performance and trust outcomes, with visual presentations and contextual feedback demonstrating particularly strong effects across different domains.","This excerpt directly mentions AI presentation methods, visual presentations, and contextual feedback, which are interface and feedback mechanisms that influence user interaction and trust with AI systems, aligning with the research focus on human-AI interaction features.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Tailoring systems to high-risk contexts","","","This matrix approach enables the tailoring of systems to specific high-risk contexts while preserving human competence and maintaining accountability.","This describes an AI system capability to be tailored to specific high-risk contexts, which is a feature of AI systems in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Ethical management addressing autonomy shifts and transparency","","","The ethical management of human–AI interaction requires integrating duty and virtue ethics within sociotechnical systems to address issues like autonomy shifts, distributive justice, and transparency [ 122].","This mentions transparency as an issue to address in human-AI interaction, which relates to the opacity/explainability feature of AI systems, though it is discussed in the context of ethical management rather than as a direct system characteristic.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: AI decision support systems in high-risk contexts","","","The cumulative evidence suggests that the successful integration of AI decision support systems in high-risk contexts depends not simply on technical capabilities but on deliberately designed sociotechnical systems that account for human cognition, organizational contexts, ethical principles, and domain-specific safety requirements.","This explicitly mentions AI decision support systems as a type of AI system integrated into high-risk contexts, which is a direct AI feature relevant to the research focus on high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: control | Feature: Control and feedback dimensions with context-specific configurations","","","Effective HAIC systems should be designed along both control and feedback dimensions, allowing for context-specific configurations that mitigate the ethical risks associated with different collaborative arrangements [ 118].","This describes design features of HAIC systems, specifically control and feedback mechanisms with context-specific configurations, which are AI system features relevant to human-AI interaction in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: AI_capability | Feature: Technological Sophistication of AI Systems","","","Studies examined AI systems with varying technological sophistication, from rule-based systems to advanced machine learning models, making it difficult to draw conclusions about specific AI approaches.","This excerpt directly discusses AI system features by describing the range of technological sophistication, from rule-based to advanced machine learning models, which are key characteristics of AI systems in human-AI interaction contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: control | Feature: Interactive control mechanisms","","","Interactive control mechanisms show similar conflicting patterns. Several studies demonstrated that user control improved outcomes, yet other research found that interactive methods may increase over-trust through co-creation effects [ 66,67,86].","This represents an AI feature as it discusses 'interactive control mechanisms' and 'user control', which are key aspects of human-AI interaction and control mechanisms in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: opacity | Feature: Explainable AI and transparency mechanisms","","","transparency mechanisms improve trust calibration, while other research found that explainable AI led to dangerous over-reliance in healthcare contexts [ 76,87,105]. This contradiction suggests context-dependent transparency effects requiring domain-specific analysis.","This directly addresses opacity/explainability by mentioning 'explainable AI' and 'transparency mechanisms', which relate to black-box behavior and interpretability issues in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: non_deterministic | Feature: data-driven bias reflection","","","Third, the AI models used for information extraction may reflect biases present in their training datasets, particularly in how they interpret and categorize research contributions.","This represents a non-deterministic/data-driven AI feature because it highlights how AI outputs (interpretation and categorization) vary based on data/model states (training datasets), introducing uncertainty and potential unpredictable failures due to biases.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","Category: context_adaptive | Feature: Context-aware task allocation","","","context-aware task allocation that dynamically assigns responsibilities based on situational factors","This represents a context-aware/adaptive AI feature as it describes dynamic response to environmental factors, aligning with the research focus on context-aware/adaptive behavior in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","The research demonstrates that successful AI integration in high-risk contexts requires domain-specific calibration, integrated sociotechnical design addressing trust calibration and skill preservation simultaneously, and proactive measures to maintain human agency and competencies essential for safety, accountability, and ethical responsibility.","Severity Justification: The text explicitly mentions 'skill preservation' as a requirement, indicating that skill degradation is a significant concern in high-risk AI contexts, warranting a high severity score. | Relevance Justification: This directly addresses skill degradation as a human performance issue related to novel AI systems, aligning with the research focus on non-deterministic and adaptive AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","We identified that visual presentations, interactive features, and uncertainty communication significantly influence trust calibration, with simple visual highlights proving more effective than complex presentation and interactive methods in preventing over-reliance.","Severity Justification: Over-reliance is a specific trust issue that can degrade human performance by causing excessive dependence on AI, potentially leading to errors or reduced vigilance. | Relevance Justification: The excerpt directly mentions 'over-reliance' as a performance degradation related to trust calibration, aligning with the priority search for trust issues like over-trust.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","What ethical and practical implications arise from integrating AI decision support systems into high-risk human decision making, particularly regarding trust calibration, skill degradation, and accountability across different domains?","Severity Justification: Skill degradation is directly identified as a significant concern in high-risk decision-making contexts, implying potential serious consequences for human performance and safety. | Relevance Justification: The excerpt directly addresses 'skill degradation' as a core implication of AI integration, perfectly matching the research focus on human performance degradations in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 7","","research highlights cognitive risks such as automation bias, where over-reliance on AI recommendations can lead to diminished decision-making skills and mental inertia, particularly in healthcare settings where clinicians increasingly depend on AI-driven diagnostic tools [ 7,8].","Severity Justification: The degradation involves diminished decision-making skills and mental inertia, which are significant cognitive impairments, especially in high-risk healthcare settings where errors have severe consequences. | Relevance Justification: This directly addresses human performance degradation related to novel AI characteristics (non-deterministic/data-driven decision-making and opacity), focusing on automation bias as a cognitive risk leading to skill and decision-making degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","the ethical and practical implications of AI integration in high-risk decision making, with particular attention to trust calibration, skill degradation, and accountability across different domains.","Severity Justification: Skill degradation is directly named as a critical implication in high-risk contexts, indicating significant concern. | Relevance Justification: The excerpt verbatim includes 'skill degradation', which is a core human performance degradation issue related to AI integration.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 5","","However, effective HAIC hinges on achieving appropriate reliance, wherein decision-makers neither over-rely on AI outputs nor dismiss them outright [ 19].","Severity Justification: The text implies a moderate risk of performance degradation due to trust calibration issues, as improper reliance (over- or under-trust) can lead to errors, but it does not specify severe outcomes. | Relevance Justification: This directly addresses trust issues (over-trust and under-trust) in human-AI interaction, which are key to performance degradation, aligning with the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 6","","For AI systems to support rather than hinder human awareness, their outputs must be structured to enhance all three levels without overwhelming cognitive capacity. Similarly, Cognitive Load Theory distinguishes between intrinsic, extraneous, and germane cognitive load [ 21], suggesting that effective AI design should minimize irrelevant information processing while supporting meaningful pattern construction and respecting human cognitive limitations [ 22].","Severity Justification: The excerpt explicitly mentions avoiding overwhelming cognitive capacity and minimizing irrelevant information processing, indicating a moderate risk of cognitive overload that could impair decision-making and situational awareness in high-stakes environments. | Relevance Justification: This directly addresses cognitive load issues related to AI systems, which are a key aspect of human performance degradation in human-AI interaction, as per the research focus on context-aware/adaptive behavior and the priority search for cognitive load.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 6","","These techniques enhance intuitive decision making by aligning AI outputs with human cognitive processes, reducing cognitive friction, and mitigating risks like automation bias [ 29].","Severity Justification: Automation bias is explicitly mentioned as a risk being mitigated, indicating it is a recognized performance degradation issue, but the context is about mitigation rather than active occurrence, so severity is moderate. | Relevance Justification: Automation bias is directly listed in the priority search under cognitive biases and is explicitly mentioned in the chunk, making it highly relevant to human performance degradation in AI interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Trust calibration is the alignment between a person’s level of trust in an automated system and the system’s actual trustworthiness or capabilities and the existing trust framework identifies three trust calibration states: appropriate trust (aligned with system capabilities), over-trust (exceeding system reliability), and under-trust (below system capabilities) [ 31].","Severity Justification: Over-trust and under-trust directly indicate performance degradation risks: over-trust can lead to complacency and errors when the system fails, while under-trust can cause underutilization of beneficial AI, both impairing decision-making in high-stakes contexts. | Relevance Justification: This excerpt explicitly mentions over-trust and under-trust as trust calibration states, which are directly listed as priority search terms for trust issues related to human performance degradation in AI interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Yet, challenges persist, including the opacity of AI reasoning, which can erode trust, and ethical dilemmas surrounding autonomous weapon systems, necessitating robust human oversight to align with international norms [38,39].","Severity Justification: Erosion of trust due to AI opacity can significantly impair human decision-making and reliance on AI systems, especially in high-stakes military contexts, but the text does not specify catastrophic outcomes. | Relevance Justification: Directly mentions trust erosion as a challenge from AI opacity, aligning with the research focus on novel AI characteristics like opacity/lack of explainability and its impact on trust.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","However, challenges persist in data complexity, the need for interpretable AI outputs to build clinician trust, and ethical concerns over accountability in life-critical decisions [42,43].","Severity Justification: The issue is explicitly stated as a challenge in a life-critical context (diagnostic imaging), indicating moderate severity due to potential impact on decision-making and trust, but it is not described as causing direct degradation. | Relevance Justification: Directly mentions trust building with clinicians in relation to AI outputs, aligning with the research focus on trust issues in high-risk industries and the priority search for trust issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 3","","these collaborative systems face common challenges, including insufficient transparency, difficulties in establishing trust, and ethical conflicts [ 17,47]. Given the profound socioeconomic implications of these domains, continued in-depth research is essential to develop HAIC systems with domain adaptability, ethical safeguards, and appropriate human dependency mechanisms.","Severity Justification: The text indirectly implies potential performance issues through trust challenges, but does not specify degradation, resulting in low severity. | Relevance Justification: The text discusses trust and transparency challenges relevant to human-AI interaction, but lacks direct mention of performance degradation, making it moderately relevant.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","opaque AI can erode trust and complicate accountability, leaving decision-makers uncertain about how to balance AI recommendations with their own expertise [ 19].","Severity Justification: Eroding trust and complicating accountability directly impact decision-making effectiveness, especially in high-stakes domains, indicating moderate to high severity. | Relevance Justification: Directly addresses trust issues (over-trust/under-trust/trust calibration) and decision-making uncertainty related to novel AI characteristics like opacity, aligning closely with the research focus on human performance degradations in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 7","","may miss how those same explanations inadvertently trigger cognitive overload or over-reliance in high-stress environments.","Severity Justification: Cognitive overload and over-reliance in high-stress environments can significantly impair decision-making and performance, especially in critical situations. | Relevance Justification: Directly mentions cognitive overload and over-reliance as performance issues triggered by AI explanations in high-stress environments, aligning with the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","how to design systems that actively sustain, rather than gradually erode, the expert’s intuitive judgment over time. There is a lack of unified frameworks that connect design features directly to long-term ethical outcomes like skill degradation and accountability gaps, necessitating a more critical and holistic investigation.","Severity Justification: Skill degradation is explicitly mentioned as a long-term ethical outcome, indicating it is a recognized and significant issue in human-AI interaction, though the severity is implied rather than quantified. | Relevance Justification: The excerpt directly mentions 'skill degradation' as a key outcome related to human performance in the context of AI system design, aligning perfectly with the research focus on human performance degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","What ethical and practical implications arise from integrating AI decision support systems into high-risk human decision making, particularly regarding trust calibration, skill degradation, and accountability across different domains?","Severity Justification: Skill degradation is directly named as a key implication in high-risk contexts, indicating significant concern for human performance. | Relevance Justification: The excerpt verbatim includes 'skill degradation', which is a priority search term and directly addresses human performance degradation related to AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","If humans systematically mis-calibrate their reliance, over-relying when AI errs and under-relying when AI excels, then complementarity’s benefits may be inherently limited regardless of role architecture design.","Severity Justification: Systematic mis-calibration of reliance directly impacts performance by causing inappropriate trust levels (over-reliance on erroneous AI and under-reliance on excellent AI), inherently limiting collaboration benefits regardless of design, indicating a significant degradation. | Relevance Justification: Directly addresses trust calibration issues (over-trust and under-trust) in human-AI interaction, which is a key performance degradation in the research focus on novel AI characteristics like non-deterministic decision-making and opacity.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","human–AI combinations underperformed the best individual agent (Hedges’ g = −0.23), though they exceeded human-only performance (g = 0.64 ) [18]. Task type moderated this effect: decision tasks showed losses while creative tasks showed gains. Scholes’s research reinforced this concern, noting that while AI effectively predicts average outcomes, humans remain critical for rare high-stakes events— yet often fail to recognize when override is appropriate [ 61].","Severity Justification: The excerpt shows a measurable performance degradation (Hedges' g = -0.23) in human-AI combinations compared to the best individual agent, specifically in decision tasks, and highlights a critical failure in override recognition for high-stakes events, indicating significant operational risks. | Relevance Justification: This directly addresses human performance degradation in novel AI contexts, showing underperformance in decision tasks and failure in override recognition, which are key issues in human-AI interaction for high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 6","","Both approaches share a critical limitation: Ding et al.’s model assumes rational decision making, neglecting cognitive biases; Hauptman et al.’s hypothetical scenarios may not reflect operational behavior.","Severity Justification: The excerpt explicitly mentions 'neglecting cognitive biases,' which can lead to automation bias or confirmation bias, degrading decision-making performance, but it does not specify severe outcomes like errors or accidents. | Relevance Justification: This directly relates to cognitive biases in decision-making, a key focus area for human performance degradation in AI interactions, as it highlights how models may fail to account for human biases.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 3","","experienced reduced cognitive load, though the system’s static recommendation pathways limited flexibility","Severity Justification: The excerpt explicitly states 'reduced cognitive load', which is a positive effect, but it is contrasted with 'limited flexibility' due to static pathways, suggesting a mild trade-off or degradation in adaptability. | Relevance Justification: This directly relates to cognitive load, which is a priority search category, and involves human-AI interaction in a high-risk industry (chemistry), though it is framed as a mixed outcome rather than a pure degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Gomez et al.’s [ 66] finding that interaction may inadvertently increase over-trust suggests that involvement does not guarantee appropriate calibration.","Severity Justification: Over-trust is a significant human performance degradation in high-risk contexts as it can lead to complacency and failure to intervene when needed, though the text does not specify direct harm outcomes. | Relevance Justification: Directly mentions 'over-trust' and 'calibration', which are key trust issues in human-AI interaction, aligning with the research focus on novel AI characteristics like opacity and adaptive behavior.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 6","","raises serious questions about whether findings would replicate with technology-naive populations or in domains where users lack the baseline expertise to evaluate AI capabilities.","Severity Justification: The text suggests a significant gap in user expertise that could lead to poor evaluation of AI capabilities, which is a moderate-severe risk for performance degradation in high-risk domains. | Relevance Justification: Directly addresses human performance issues related to evaluating AI capabilities, which aligns with skill/knowledge degradation in the priority search list.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 6","","Flathmann et al. found that decreasing AI influence over time enhanced human performance, while sustained high influence increased cognitive workload [ 83].","Severity Justification: Increased cognitive workload is a moderate degradation as it can impair decision-making and situational awareness, but the text does not specify extreme outcomes. | Relevance Justification: Directly mentions cognitive workload increase due to AI influence, aligning with the research focus on AI-related degradations vs. traditional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 6","","Lin et al. found GPT-3 underperformed compared to human assistants in goal-directed planning despite generating longer dialogs, suggesting humans should retain strategic decision control [ 81].","Severity Justification: The degradation is moderate as it shows AI underperformance in a specific task (goal-directed planning) compared to humans, but it does not specify catastrophic failures or widespread impacts. | Relevance Justification: Highly relevant as it directly compares AI performance to human performance in a high-risk context (strategic decision control), aligning with the research focus on novel AI characteristics and human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 4","","Rastogi al. examined cognitive biases in AI-assisted decision making, finding that non-expert participants limited generalizability to expert domains [ 85].","Severity Justification: The excerpt directly identifies cognitive biases in AI-assisted decision making as a studied issue, but it does not explicitly describe severe performance degradation; it primarily notes a limitation in generalizability. | Relevance Justification: This is highly relevant as it explicitly mentions cognitive biases in AI-assisted decision making, which aligns with the research focus on novel AI characteristics and the priority search for cognitive biases.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 6","","More troubling are the temporal dynamics findings from Flathmann et al. [ 83], which suggest that even within a single task context, optimal allocation may shift as users develop expertise or experience cognitive fatigue—a complexity that neither taxonomic nor current dynamic approaches adequately address.","Severity Justification: Cognitive fatigue is explicitly mentioned as a factor affecting optimal allocation, suggesting moderate severity as it impacts performance but is not described as catastrophic. | Relevance Justification: This directly relates to human performance degradation in the context of task allocation and AI/autonomous systems, addressing cognitive load issues relevant to the research focus on adaptive behavior and unpredictability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: behavioral_changes | Severity: 9","","The predominance of virtual environments and gaming contexts in this literature raises additional concerns: allocation strategies effective in simplified low-stakes settings may fail catastrophically in safety-critical operational contexts where errors carry severe consequences.","Severity Justification: The mention of 'fail catastrophically' and 'severe consequences' indicates high severity, as it suggests significant performance degradation in safety-critical contexts. | Relevance Justification: This is highly relevant to the research focus on high-risk industries and novel AI characteristics, addressing the unpredictability and potential for severe errors in operational settings.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 6","","The finding from Lin et al. [ 81] that GPT-3 underperformed human assistants in strategic planning—despite being allocated precisely the kind of reasoning-intensive task that taxonomies would suggest favoring AI—indicates that current models of AI capability may systematically overestimate performance in complex real-world conditions.","Severity Justification: The severity is moderate because it highlights a systematic overestimation of AI performance, which could lead to degraded human decision-making if AI is incorrectly relied upon, but it does not explicitly detail severe consequences. | Relevance Justification: The relevance is high as it directly mentions AI underperforming humans in a reasoning-intensive task, indicating potential human performance issues when AI is misallocated, aligning with the focus on novel AI-related degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 6","","Schemmer et al. found that deception detection tasks with low human performance limited appropriate reliance development [ 89].","Severity Justification: Low human performance directly indicates a degradation in task execution, which is a measurable performance issue, though the specific impact is described as limiting reliance development rather than causing severe operational failures. | Relevance Justification: This directly mentions 'low human performance' as a factor affecting reliance, which is a key performance degradation in human-AI interaction, aligning with the research focus on novel AI characteristics like non-deterministic decision-making and opacity.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","Kreps and Jakesch demonstrated that AI-mediated communication with human oversight increased trust when AI performed well, but poor performance eroded confidence [ 88].","Severity Justification: The degradation involves trust erosion due to poor AI performance, which can impact decision-making and reliance in high-risk contexts, but it is described as a specific effect rather than a severe or widespread failure. | Relevance Justification: This directly addresses trust calibration, a key aspect of human performance degradation in AI interactions, as poor AI performance undermines confidence, aligning with the research focus on trust and regulation challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Rosenbacke found that explainable AI increased trust but risked promoting over-reliance, leading to undetected errors [ 87]. This occurred specifically through “False Confirmation” errors where clinicians failed to identify AI mistakes.","Severity Justification: The degradation involves over-reliance leading to undetected errors in a clinical setting, which could have serious consequences, but the study's small sample and limited focus moderate the severity. | Relevance Justification: This directly addresses trust issues (over-trust/over-reliance) and performance degradation (undetected errors, failure to identify AI mistakes) in the context of novel AI characteristics like explainability, matching the research focus on human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: behavioral_changes | Severity: 6","","Schmutz et al. reported that AI implementation often reduces team coordination effectiveness, with most findings based on laboratory rather than organizational settings [94].","Severity Justification: Reduced team coordination effectiveness is a significant performance degradation in collaborative environments, but the text notes it is based on laboratory settings, which may limit real-world severity. | Relevance Justification: Directly mentions AI implementation reducing team coordination effectiveness, which aligns with human performance degradation in human-AI interaction contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 7","","Rosenbacke’s [ 87] finding that explainable AI can promote over-reliance through “False Confirmation” errors suggests that making AI reasoning visible does not straightforwardly enable users to identify when AI is wrong—it may instead provide false assurance that errors have been checked for and ruled out.","Severity Justification: The text describes a specific degradation where explainable AI promotes over-reliance through 'False Confirmation' errors, indicating a moderate to high severity as it directly impairs users' ability to identify AI errors, potentially leading to incorrect decisions in high-risk contexts. | Relevance Justification: This excerpt is highly relevant as it explicitly mentions 'over-reliance' and 'False Confirmation' errors, which are key aspects of automation bias and human performance degradation in AI interaction, aligning with the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","transparency may promote rather than prevent over-reliance.","Severity Justification: Over-reliance can lead to complacency and errors in safety-critical applications, posing significant risks. | Relevance Justification: Directly mentions over-reliance as a potential outcome of transparency, aligning with trust calibration issues in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","The performance paradox documented by Vaccaro et al. [ 18] indicates that human–AI combinations frequently fail to achieve complementarity’s theoretical benefits, underperforming the best individual agent in decision tasks.","Severity Justification: The degradation is explicitly stated as underperformance relative to the best individual agent, which is a significant issue in decision-making contexts, though the exact magnitude isn't quantified. | Relevance Justification: This directly addresses human performance degradation in the context of novel AI systems, specifically highlighting that human-AI combinations fail to achieve theoretical benefits and underperform, which aligns with the research focus on non-deterministic and adaptive AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 3","","suggesting that domain-appropriate visual design is crucial for successful trust calibration . In cybersecurity contexts, Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) visualizations enhanced interpretability and increased human trust in AI decisions for malware detection tasks [ 101].","Severity Justification: The text implies trust calibration is crucial, suggesting mild degradation risk if visual design is inadequate, but no explicit performance problems are described. | Relevance Justification: Directly mentions 'trust calibration' and 'increased human trust', aligning with the research focus on trust issues in human-AI interaction, though it does not explicitly state degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","Simple visual highlights consistently outperformed more complex presentation in reducing over-reliance on AI systems during difficult tasks [ 102]. This finding indicates that excessive presentation complexity can increase perceived task difficulty, potentially impairing user trust calibration and compliance [103].","Severity Justification: The degradation involves impaired trust calibration and compliance, which can lead to errors in high-risk industries, but the text does not specify severe outcomes. | Relevance Justification: Directly addresses trust calibration issues related to AI systems, which is a key focus in human performance degradation for novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 5","","Example-based presentation methods demonstrated distinct effects on trust calibration and reliance behaviors compared to feature-based approaches.","Severity Justification: The excerpt mentions effects on trust calibration and reliance behaviors, which can degrade human performance if mismanaged, but it does not specify severe negative outcomes. | Relevance Justification: This directly relates to trust calibration, a priority search item, and involves human performance in interacting with AI systems, aligning with the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 5","","This finding highlights the critical importance of presentation accuracy in maintaining appropriate trust calibration.","Severity Justification: The text implies a potential degradation in trust calibration if presentation accuracy is not maintained, but it does not specify severe consequences, so severity is moderate. | Relevance Justification: This directly addresses trust calibration, a key aspect of human performance degradation in AI interactions, as per the research focus on trust issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","However, the quality of examples significantly influenced trust calibration outcomes. When example-based presentations contained errors, they proved more deceptive than natural language alternatives, affecting reliance behaviors differently across expertise levels [ 104].","Severity Justification: The excerpt directly addresses trust calibration issues and deceptive presentations that affect reliance behaviors, indicating moderate performance degradation due to trust mismatches. | Relevance Justification: This is highly relevant to the research focus on human-AI interaction, specifically addressing trust calibration and reliance behaviors in the context of AI system presentations, which aligns with examining novel AI characteristics vs. conventional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","Research comparing three strategies based on estimated human and AI correctness likelihood—Direct Display, Adaptive Workflow, and Adaptive Recommendation—found that all three approaches promoted more appropriate human trust in AI, particularly reducing over-trust when AI provided incorrect recommendations [105].","Severity Justification: Over-trust in AI when it provides incorrect recommendations can lead to significant performance degradation, as humans may rely on faulty AI outputs without proper verification, potentially causing errors in high-risk industries. | Relevance Justification: This directly addresses trust calibration issues (over-trust) related to novel AI characteristics like non-deterministic decision-making and opacity, which are key to the research focus on human performance degradations in AI interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","The various presentation and interaction approaches demonstrated profound but varied effects on trust calibration.","Severity Justification: The text explicitly mentions 'effects on trust calibration' which indicates a potential degradation area in human performance related to trust mismatches, though it doesn't specify negative outcomes. | Relevance Justification: Directly addresses trust calibration which is a priority search item under trust issues in the research focus on human-AI interaction degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","the potential for explainable AI to sometimes lead to over-trust [ 87], and the significant effect of confirmation bias on trust development when AI recommendations aligned with users’ initial judgments [ 108].","Severity Justification: Over-trust and confirmation bias can lead to uncritical acceptance of AI recommendations, potentially causing errors in high-risk decision-making contexts. | Relevance Justification: Directly addresses trust calibration issues (over-trust and confirmation bias) which are priority search items for human performance degradation in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","While user engagement increased, certain interactive methods may inadvertently increase over-trust by boosting confidence in co-created outputs [ 66]. This finding highlights a critical design challenge in balancing user engagement with appropriate skepticism.","Severity Justification: Over-trust is a moderate performance degradation as it can lead to inappropriate reliance on AI outputs, but the text frames it as a design challenge rather than a severe failure. | Relevance Justification: Directly mentions 'over-trust' as a human performance issue related to AI interaction, specifically addressing trust calibration problems in human-AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 8","","Example-based presentations provided stronger signals of AI unreliability, potentially preventing the blind acceptance of incorrect recommendations [29].","Severity Justification: Blind acceptance of incorrect recommendations implies a severe degradation in critical thinking and decision-making, as it can directly cause mistakes in tasks, especially in high-risk contexts. | Relevance Justification: Directly mentions 'blind acceptance of incorrect recommendations,' which is a clear human performance degradation related to trust issues and automation bias in AI interactions, aligning with the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 7","","Visual presentation techniques, particularly simple visual highlights, demonstrated effectiveness in reducing over-reliance on AI systems during difficult tasks compared to prediction-only or written presentation techniques [ 102].","Severity Justification: Over-reliance on AI systems during difficult tasks indicates a significant bias where human judgment is compromised, potentially leading to errors in high-stakes situations. The text presents it as a problem needing mitigation. | Relevance Justification: Directly mentions 'over-reliance on AI systems,' which is a key human performance degradation related to automation bias in human-AI interaction, matching the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 3","","Providing accurate information about AI recommendations led to better calibrated trust, with participants deviating more from low-quality recommendations and less from high-quality ones [ 74].","Severity Justification: The excerpt describes a positive effect (better calibrated trust) rather than a direct degradation, but it implies potential issues with trust calibration in the absence of accurate information, suggesting mild severity. | Relevance Justification: The excerpt directly addresses trust calibration and reliance behaviors in human-AI interaction, which are key aspects of human performance degradation related to trust issues, aligning with the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 4","","Similarly, correctness likelihood strategies effectively promoted appropriate trust in AI, especially in reducing over-trust when AI provided incorrect recommendations [105].","Severity Justification: The excerpt highlights over-trust as a specific degradation when AI provides incorrect recommendations, indicating a moderate severity as it can lead to poor decision-making. | Relevance Justification: This excerpt explicitly mentions over-trust, a key trust issue in human-AI interaction, and relates to performance degradation by showing how strategies mitigate it, making it highly relevant to the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: behavioral_changes | Severity: 4","","The relationship was influenced by user expertise levels, with the impact of different presentation types varying between novices and experts, as each group utilized different aspects of the presentations in their decision making [29].","Severity Justification: The excerpt indirectly suggests potential performance differences or degradations based on expertise, as novices and experts utilize presentations differently in decision making, but it does not explicitly state severe degradation. | Relevance Justification: It is relevant to human performance in AI interaction as it discusses factors (expertise levels) moderating trust calibration and reliance behaviors, which can impact decision-making performance, though it is not a direct mention of degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: behavioral_changes | Severity: 6","","humans were less likely to delegate complex predictions to AI when it made rare but large errors, driven by higher self-confidence rather than lower confidence in the model [ 111]. Additionally, humans violated choice independence in HAIC, with errors in one domain affecting delegation decisions in others.","Severity Justification: The degradation involves reduced reliance on AI and biased decision-making, which could impair performance in high-risk settings, but it is described as a behavioral pattern without explicit severe outcomes. | Relevance Justification: Directly mentions human performance issues in HAIC, including reduced delegation and violation of choice independence, aligning with the focus on novel AI-related degradations vs traditional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","The integration of AI decision support systems into high-risk human decision-making contexts generates complex ethical and practical implications that fundamentally challenge traditional models of human agency, accountability, and competence.","Severity Justification: The text indicates a fundamental challenge to human competence in high-risk decision-making contexts, which suggests a significant potential for performance degradation, though the exact severity is not quantified. | Relevance Justification: This directly addresses skill degradation (reduced competence) as a key implication of AI integration in high-risk human decision-making, matching the research focus on novel AI-related degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","skill degradation risks, and accountability gaps, each with distinct manifestations across high-risk domains.","Severity Justification: Skill degradation is listed as a primary concern alongside trust calibration and accountability gaps, suggesting it is a significant risk in high-risk domains, though the excerpt does not detail specific impacts. | Relevance Justification: The excerpt directly mentions 'skill degradation risks' in the context of novel AI/autonomous systems vs. traditional automation, aligning perfectly with the research focus on human performance degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 6","","The confirmation bias findings from Bashkirova and Krpan [ 108] further complicate interpretation, suggesting that apparent trust improvements may reflect motivated reasoning rather than genuine calibration.","Severity Justification: Confirmation bias is identified as complicating interpretation and potentially misleading about trust improvements, suggesting moderate impact on decision-making. | Relevance Justification: Directly addresses cognitive biases (confirmation bias) in the context of trust and calibration, relevant to human performance in AI interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","The trust–behavior dissociation documented by Sivaraman et al. [ 109] and Eisbach et al. [ 74] poses a fundamental challenge to intervention evaluation: if self-reported trust does not predict behavioral reliance, then studies relying on trust self-reports—which constitute the majority of this literature—may systematically misestimate intervention effectiveness.","Severity Justification: The dissociation between trust and behavior is described as a fundamental challenge, indicating significant impact on evaluation and potential misestimation of effectiveness. | Relevance Justification: Directly addresses trust issues (trust-behavior dissociation) related to human-AI interaction, which is a key focus area for performance degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","However, explanation quality must be tailored to user expertise and context, as inappropriate explanations can either promote dangerous over-trust or create counterproductive under-trust [ 115,116].","Severity Justification: The text explicitly describes 'dangerous over-trust' and 'counterproductive under-trust' as direct consequences of inappropriate explanations, indicating significant trust calibration issues that could lead to serious performance problems in high-risk contexts. | Relevance Justification: This directly addresses trust issues (over-trust and under-trust) which are priority search items for human performance degradation in human-AI interaction, specifically related to novel AI characteristics like opacity/lack of explainability mentioned in the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 8","","The cognitive burden of this generalization manifests as inefficient monitoring behaviors following negative experiences, diverting cognitive resources from primary decision-making tasks—a critical concern in high-stakes environments where cognitive load management is essential for safety and effectiveness.","Severity Justification: The excerpt directly links cognitive burden to inefficient monitoring and diversion of cognitive resources from primary tasks, which is described as a critical concern in high-stakes environments affecting safety and effectiveness, indicating a significant impact. | Relevance Justification: The excerpt explicitly discusses human performance degradation in the form of inefficient monitoring behaviors and diverted cognitive resources from decision-making tasks, directly related to cognitive load management in high-risk contexts, matching the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Despite this perceived moral deficiency, users demonstrate increasing reliance on AI recommendations as they gain experience, suggesting a dangerous disconnect between perceived trustworthiness and actual reliance behaviors.","Severity Justification: This indicates a significant trust calibration issue where reliance increases despite perceived moral deficiencies, which could lead to over-reliance and degraded decision-making in high-risk contexts. | Relevance Justification: Directly addresses trust mismatch and reliance behaviors in human-AI interaction, aligning with the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Trust calibration is significantly influenced by error patterns, with critical implications for high-risk decision making.","Severity Justification: The excerpt highlights critical implications for high-risk decision making, suggesting significant risk of performance degradation due to trust calibration issues, but does not specify catastrophic outcomes directly. | Relevance Justification: Directly mentions trust calibration, which is a priority search term, and relates to human performance in high-risk contexts, aligning with the research focus on novel AI characteristics and human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","Performance Paradox in HAIC Empirical evidence reveals a concerning performance paradox: human–AI combinations often perform worse than the best solo performer (human or AI) but better than humans alone [ 18].","Severity Justification: The paradox directly indicates degraded performance in human-AI teams compared to optimal solo performance, which is a significant issue in high-stakes contexts. | Relevance Justification: This explicitly mentions a performance degradation (worse than best solo performer) related to human-AI interaction, directly matching the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","High control configurations risk over-reliance and the progressive de-skilling of human decision-makers, while high learning capacity configurations create vulnerabilities to automation bias and implementation failures.","Severity Justification: Progressive de-skilling represents a gradual but significant erosion of human capabilities, while automation bias can lead to systematic errors in decision-making, both with serious consequences in high-risk industries. | Relevance Justification: Directly addresses skill degradation (progressive de-skilling) and cognitive biases (automation bias) in the context of novel AI system characteristics, specifically mentioning both high control and high learning capacity configurations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","Task type significantly moderates this relationship, with decision tasks typically showing performance losses while creation tasks demonstrate gains. This finding challenges fundamental assumptions about the benefits of HAIC in high-risk decision-making contexts and highlights the need for careful task-specific implementation strategies.","Severity Justification: Performance losses in decision tasks within high-risk contexts indicate significant degradation, as it directly impacts safety-critical outcomes. | Relevance Justification: Directly mentions 'performance losses' in relation to decision tasks and human-AI interaction in high-risk contexts, matching the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 6","","The preservation of human competence and agency represents a critical concern in high-risk AI integration.","Severity Justification: Highlighting competence preservation as a critical concern suggests moderate severity, as it indicates recognized risks but not explicit degradation events. | Relevance Justification: Directly relates to skill/knowledge degradation by mentioning 'human competence' as a concern in high-risk AI integration, aligning with the research focus on novel AI features.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 6","","AI teammates that decrease their influence over time enable humans to improve their performance, while highly influential AI teammates can increase perceived cognitive workload and potentially inhibit skill development [ 83].","Severity Justification: The excerpt explicitly mentions 'potentially inhibit skill development,' which directly indicates a risk of skill degradation, though it is phrased as a potential rather than a certainty. | Relevance Justification: This directly addresses human performance degradation in the context of novel AI characteristics (adaptive influence management), focusing on skill development issues, which is a key aspect of the research focus on human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 6","","This ambiguity can undermine both learning from failures and incentive structures for maintaining human competence.","Severity Justification: The excerpt directly links ambiguity to undermining human competence, suggesting a moderate impact on skill maintenance in high-risk domains. | Relevance Justification: The excerpt explicitly mentions 'maintaining human competence,' which aligns with the skill/knowledge category of human performance degradation, though it focuses on incentive structures rather than direct degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 8","","Domain-Specific Skill Degradation Risks In healthcare contexts, specific patterns of skill degradation emerge through error types that reflect diminished human judgment. “False Confirmation” errors occur when clinicians fail to identify AI mistakes due to over-reliance, while “False Conflict” errors arise from cognitive biases like commitment and confirmation bias [ 87]. These error patterns suggest the systematic degradation of critical diagnostic skills when AI serves as a decision support tool.","Severity Justification: The text describes 'systematic degradation of critical diagnostic skills' which indicates a serious, structured decline in essential professional capabilities with direct implications for patient safety and healthcare quality. | Relevance Justification: This directly addresses human performance degradation related to novel AI systems (specifically AI decision support tools) in a high-risk industry (healthcare), mentioning specific degradation patterns, error types, and cognitive biases.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","The manifestation of trust calibration, skill degradation, and accountability challenges varies significantly across different high-risk domains, with each presenting unique ethical considerations and implementation requirements.","Severity Justification: Skill degradation is directly named as a challenge in high-risk environments, indicating a significant performance issue, though the severity is not quantified. | Relevance Justification: The excerpt directly mentions 'skill degradation', which is a key human performance degradation related to novel AI systems in high-risk industries, matching the research focus on human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 8","","Skill degradation in healthcare contexts is particularly concerning given the life-or-death nature of many decisions.","Severity Justification: Skill degradation is directly stated as 'particularly concerning' in life-or-death contexts, indicating high severity. | Relevance Justification: The excerpt directly addresses human performance degradation (skill degradation) in the context of AI/autonomous systems in healthcare, matching the research focus on high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Explainable AI systems increase clinicians’ trust compared to black-box alternatives but can paradoxically lead to dangerous over-reliance, resulting in undetected diagnostic errors [ 87].","Severity Justification: The excerpt explicitly mentions 'dangerous over-reliance' and 'undetected diagnostic errors,' indicating a significant risk to patient safety and clinical performance. | Relevance Justification: This directly addresses trust issues (over-trust) and performance degradation (undetected errors) related to novel AI characteristics like explainability, aligning with the research focus on human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 9","","Aviation and other safety-critical systems present distinct challenges where the consequences of trust miscalibration or skill degradation can be catastrophic and immediate.","Severity Justification: Skill degradation is described as having 'catastrophic and immediate' consequences in high-risk industries, indicating a high severity level. | Relevance Justification: This excerpt directly matches the research focus on human performance degradations, specifically skill degradation, in the context of AI integration in safety-critical systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","AI systems that undermine these competencies pose not only performance risks but also regulatory compliance challenges.","Severity Justification: The degradation is linked to AI systems undermining competencies, which are strictly regulated and critical for safety in high-risk aviation, posing significant performance risks. | Relevance Justification: This directly addresses human performance degradation related to novel AI characteristics (non-deterministic, opaque, adaptive) by highlighting how AI can degrade competencies, aligning with the research focus on skill/knowledge issues in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","AI-mediated communication with human oversight can increase constituent trust compared to generic auto-responses in legislative settings, but poorly performing AI language technologies risk damaging constituent confidence and democratic legitimacy [ 88]. Off-topic and repetitive responses significantly reduce public trust, underscoring the importance of transparency regarding AI use in public communication.","Severity Justification: The text explicitly states that poorly performing AI technologies 'risk damaging constituent confidence and democratic legitimacy' and that off-topic/repetitive responses 'significantly reduce public trust', indicating substantial negative impact on trust and legitimacy. | Relevance Justification: This directly addresses trust issues (over-trust/under-trust/trust calibration) in the context of novel AI characteristics (non-deterministic/data-driven decision-making and opacity/lack of explainability), specifically examining how AI performance problems affect human trust in public institutional contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","In this domain, the skill degradation concern extends beyond individual competency to institutional capacity for democratic governance.","Severity Justification: The degradation is framed as a 'concern' that extends beyond individuals to institutional governance, suggesting significant systemic impact. | Relevance Justification: Directly mentions 'skill degradation' which is a core human performance degradation category in the priority search list.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","The unique characteristics of military decision making—including time pressure, incomplete information, and life-or-death consequences—create distinct requirements for trust calibration and skill preservation.","Severity Justification: The mention of 'skill preservation' directly implies that skills are at risk of degradation in this high-pressure context, which is a core human performance issue. | Relevance Justification: This directly addresses skill degradation, which is a key human performance issue in human-AI interaction, especially in high-risk military applications where decision-making is critical.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 5","","trust calibration, skill degradation, and accountability challenges manifest.","Severity Justification: Skill degradation is explicitly mentioned as a challenge that manifests, but no details on its extent or impact are provided, so severity is moderate. | Relevance Justification: Skill degradation is directly listed as one of the challenges influenced by domain-specific factors, making it highly relevant to human performance degradation in the context of AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 8","","The skill degradation risks in military contexts are particularly concerning because they affect not only individual decision-makers but also command structures and military effectiveness.","Severity Justification: The degradation is described as 'particularly concerning' and affects critical military functions, suggesting high severity due to potential operational and ethical implications. | Relevance Justification: The excerpt directly addresses 'skill degradation risks,' which is a core aspect of human performance degradation in the context of AI/autonomous systems, making it highly relevant to the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 8","","The skill degradation concern is particularly urgent given the asymmetric consequences of error in high-risk domains, yet the evidence base consists primarily of cross-sectional observations from healthcare that cannot distinguish temporary adaptation effects from permanent competence loss.","Severity Justification: The text describes skill degradation as 'particularly urgent' in high-risk domains with 'asymmetric consequences of error', indicating high severity due to potential permanent competence loss. | Relevance Justification: This directly addresses human performance degradation (skill degradation) in the context of high-risk domains, matching the research focus on novel AI/autonomous systems versus traditional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","Trust calibration in these contexts faces unique challenges—the generalization phenomenon documented by Duan et al. [ 114] and the capability–morality tension identified by Tolmeijer et al. [ 117]—that general HAIC research does not address and that existing interventions may be inadequate to resolve.","Severity Justification: The excerpt highlights significant trust calibration challenges that are unique and inadequately addressed, suggesting potential for human performance degradation due to trust mismatches in high-risk settings. | Relevance Justification: Directly mentions trust calibration, a key trust issue from the priority search, in the context of AI integration, making it highly relevant to human performance degradation in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 3","","preserving human competence and maintaining accountability. Informatics 2025 ,12, 135 27 of 36 Ethical Framework Integration The ethical management of human–AI interaction requires integrating duty and virtue ethics within sociotechnical systems to address issues like autonomy shifts, distributive justice, and transparency [ 122]. As AI systems become more deeply integrated into high-risk decision processes, these ethical considerations must remain central to implementation strategies. The cumulative evidence suggests that the successful integration of AI decision support systems in high-risk contexts depends not simply on technical capabilities but on deliberately designed sociotechnical systems that account for human cognition, organizational contexts, ethical principles, and domain-specific safety requirements.","Severity Justification: The text mentions 'preserving human competence' as a goal, implying a risk of competence degradation if not addressed, but it does not explicitly describe active degradation or its severity. | Relevance Justification: The excerpt directly addresses human competence in the context of AI integration, which relates to skill degradation, but it focuses more on preservation and ethical management rather than detailing specific degradation issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 3","","Trust calibration studies provide measurable effect sizes, including meta-analytic evidence demonstrating that human–AI combinations underperformed the best individual agent (Hedges’ g = −0.23, indicating a small negative effect)","Severity Justification: The effect size is described as 'small negative', indicating a measurable but not severe degradation in performance when comparing human-AI combinations to the best individual agent. | Relevance Justification: Directly addresses human performance degradation in the context of human-AI interaction, using quantitative metrics (effect sizes) to show underperformance relative to the best agent, which aligns with the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 3","","Maintained performance Reduced over-relianceOutperformed complex explanationsLow-stakes maze task; ‘perfect’ explanations; and crowd-workers","Severity Justification: The severity is low because the text describes a positive outcome (reduced over-reliance) that addresses a potential degradation, rather than an active degradation itself, suggesting it is a controlled or mitigated issue. | Relevance Justification: High relevance as it directly relates to trust issues (over-reliance) in human-AI interaction, which is a key focus area for performance degradations compared to traditional automation, and it is explicitly mentioned in the chunk.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 6","","found human–AI combinations often underperformed individual agents, numerous studies report significant performance gains [ 18]. Task-type moderation appears critical, with decision tasks showing performance losses and creative tasks demonstrating gains, suggesting domain-specific optimization requirements.","Severity Justification: The excerpt explicitly states 'performance losses' for decision tasks, indicating a direct degradation, but it is presented as part of a broader performance paradox with contradictory evidence, moderating the severity. | Relevance Justification: The excerpt directly mentions human-AI combinations underperforming and performance losses in decision tasks, which aligns with the research focus on novel AI characteristics and human performance degradation in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: cognitive_overload | Severity: 4","","Reduced perceived workload Only 2 transparency levels; n= 20","Severity Justification: Reduced perceived workload has low to moderate severity as it might improve performance by lowering cognitive strain, but it's not explicitly linked to degradation. | Relevance Justification: This is somewhat relevant as it touches on cognitive load, a factor in performance, though it does not directly state degradation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","Decision tasks showed losses Publication bias; high heterogeneity; and limited delegation research","Severity Justification: Decision tasks showing losses indicates significant performance degradation, as it directly affects outcomes in collaborative settings, though context limitations may moderate the impact. | Relevance Justification: This is highly relevant as it explicitly mentions performance losses in decision tasks, aligning with the focus on human performance degradations in AI interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 6","","Reduced over-trust in incorrect AI Non-expert users; low-stake tasks; and small complementarity","Severity Justification: Reduced over-trust in incorrect AI suggests a moderate severity as it can cause errors in low-stake tasks, but the impact is limited to non-expert users and specific contexts. | Relevance Justification: This directly relates to trust issues, a key focus in human-AI interaction, by highlighting over-trust as a performance degradation factor in AI systems.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: trust_issues | Severity: 7","","explainable AI led to dangerous over-reliance in healthcare contexts [ 76,87,105]. This contradiction suggests context-dependent transparency effects requiring domain-specific analysis. Interactive control mechanisms show similar conflicting patterns. Several studies demonstrated that user control improved outcomes, yet other research found that interactive methods may increase over-trust through co-creation effects [ 66,67,86].","Severity Justification: The excerpt mentions 'dangerous over-reliance' and 'increase over-trust,' which imply significant risks to human performance, especially in high-risk healthcare contexts, warranting a moderate-high severity score. | Relevance Justification: This directly addresses trust issues (over-trust) and performance degradations (over-reliance) in novel AI systems, aligning closely with the research focus on human-AI interaction and the priority search for trust issues.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: automation_bias | Severity: 6","","Second, the “AI information first, human judgment second” design may create anchoring effects, potentially influencing human researchers toward AI-suggested interpretations. Although human researchers retained final decision authority and achieved high agreement rates, we cannot entirely rule out such cognitive influences.","Severity Justification: The text explicitly mentions a cognitive bias (anchoring effects) that may influence human decision-making, but notes that human researchers retain final authority and achieve high agreement rates, suggesting a moderate rather than severe impact. | Relevance Justification: This directly addresses cognitive biases (specifically anchoring effects) in human-AI interaction, which is a key focus area for human performance degradation in the context of novel AI systems versus traditional automation.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: performance_metrics | Severity: 7","","However, a concerning performance paradox emerges: human–AI combinations often underperform the best individual agent while consistently surpassing human-only performance.","Severity Justification: The paradox highlights a significant degradation as human-AI combinations fail to match the best agent's performance, suggesting inefficiencies or issues in collaboration. | Relevance Justification: Directly mentions human performance degradation in the context of AI interactions, focusing on underperformance relative to the best agent, which aligns with the research focus on novel AI-related degradations.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","Category: skill_degradation | Severity: 7","","Third, human agency preservation requires proactive measures to prevent skill degradation while maintaining meaningful human control over critical decisions.","Severity Justification: Skill degradation is directly identified as a risk that requires proactive prevention, indicating it is a significant concern in human-AI interaction contexts, though the severity is tempered by the focus on prevention rather than describing actual degradation. | Relevance Justification: This excerpt directly addresses skill degradation, which is a key human performance degradation issue related to novel AI systems, as specified in the priority search list under skill/knowledge.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: visual presentations, interactive features, uncertainty communication, simple visual highlights | Evidence Type: direct | Causal Strength: 8 | Performance Effect: influence trust calibration, preventing over-reliance","We identified that visual presentations, interactive features, and uncertainty communication significantly influence trust calibration, with simple visual highlights proving more effective than complex presentation and interactive methods in preventing over-reliance.","Causal Strength Justification: Direct causal language: 'significantly influence' and 'proving more effective...in preventing' explicitly link AI features to human trust and reliance outcomes. | Relevance Justification: Directly addresses the research focus on how AI presentation and interaction approaches influence trust calibration and reliance behaviors, which are key aspects of human performance in human-AI collaboration.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI recommendations and AI-driven diagnostic tools | Evidence Type: direct | Causal Strength: 8 | Performance Effect: diminished decision-making skills and mental inertia","research highlights cognitive risks such as automation bias, where over-reliance on AI recommendations can lead to diminished decision-making skills and mental inertia, particularly in healthcare settings where clinicians increasingly depend on AI-driven diagnostic tools [ 7,8].","Causal Strength Justification: The text uses direct causal language 'can lead to' to connect over-reliance on AI to specific cognitive degradations, providing a clear cause-effect relationship. | Relevance Justification: Directly addresses how AI features (recommendations and diagnostic tools) causally affect human cognitive strategies and decision-making capabilities, aligning with the research focus on human-AI interaction in high-risk domains.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: opacity of AI reasoning | Evidence Type: direct | Causal Strength: 7 | Performance Effect: erode trust","Yet, challenges persist, including the opacity of AI reasoning, which can erode trust, and ethical dilemmas surrounding autonomous weapon systems, necessitating robust human oversight to align with international norms [38,39].","Causal Strength Justification: Direct causal language 'can erode' explicitly links AI opacity to trust erosion, though it's conditional ('can') rather than definitive. | Relevance Justification: Directly addresses how opacity (a novel AI characteristic) causally affects human trust, aligning with the research focus on human-AI interaction in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI techniques (e.g., explainability and presentation methods) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Enhance intuitive decision making, reduce cognitive friction, and mitigate risks like automation bias","These techniques enhance intuitive decision making by aligning AI outputs with human cognitive processes, reducing cognitive friction, and mitigating risks like automation bias [ 29].","Causal Strength Justification: Direct causal language 'enhance... by' and 'reducing' indicates a mechanism where AI features improve human decision-making and reduce negative effects. | Relevance Justification: Directly links AI features to enhanced human decision-making and mitigation of automation bias, relevant to human performance in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Explainable AI (XAI) systems' ability to provide interpretable and understandable outputs | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Users can validate AI recommendations against their expertise and make informed decisions about when to rely on AI assistance","Explainable AI (XAI) refers to AI systems’ ability to provide interpretable and understandable outputs that allow users to comprehend how decisions are made [ 27]. This explainability enables users to validate AI recommendations against their expertise and make informed decisions about when to rely on AI assistance.","Causal Strength Justification: Direct causal language 'enables' shows a clear mechanism linking AI explainability to human performance outcomes. | Relevance Justification: Directly addresses how opacity (lack of explainability) affects human performance by enabling validation and informed reliance, aligning with the research focus on opacity and trust/regulation challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: opaque AI (lack of explainability/black-box nature) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: erode trust, complicate accountability, leave decision-makers uncertain about balancing AI recommendations with their own expertise","opaque AI can erode trust and complicate accountability, leaving decision-makers uncertain about how to balance AI recommendations with their own expertise [ 19].","Causal Strength Justification: Direct causal language ('can erode', 'complicate', 'leaving') explicitly links AI opacity to negative human performance outcomes in decision-making contexts. | Relevance Justification: Directly addresses how opacity (a novel AI characteristic) affects human performance (trust, accountability, decision-making uncertainty) in high-risk domains like healthcare and defense.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: biases embedded in training data | Evidence Type: direct | Causal Strength: 8 | Performance Effect: lead to skewed outcomes, amplifying ethical risks and undermining fairness","biases embedded in training data—whether from societal inequalities or developer assumptions—can lead to skewed outcomes, amplifying ethical risks and undermining fairness [ 5].","Causal Strength Justification: Direct causal language ('can lead to', 'amplifying', 'undermining') explicitly links training data biases to negative outcomes affecting ethical risks and fairness. | Relevance Justification: Addresses how data-driven aspects (related to non-deterministic/data-driven decision-making) can lead to outcomes that affect ethical considerations and fairness, though human performance impact is less explicit than in the first link.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: adaptive autonomy in cybersecurity | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improved collaboration","Hauptman et al. showed that adaptive autonomy in cybersecurity—higher automation for predictable tasks, lower for uncertain ones—improved collaboration by matching workflow patterns [ 65].","Causal Strength Justification: Direct causal language: 'improved collaboration by matching workflow patterns' explicitly links the adaptive autonomy feature to the performance effect through a mechanism. | Relevance Justification: Directly addresses how adaptive behavior (a novel AI characteristic) affects human performance (collaboration) in a high-risk domain (cybersecurity).","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: human–AI teaming in alert prioritization | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improved performance","Jalalvand et al. showed that human–AI teaming in alert prioritization improved performance through the automation of routine tasks, with human control over novel threats [ 79].","Causal Strength Justification: Direct causation is indicated by 'showed that... improved performance through', explicitly linking the AI feature to the performance effect with a clear mechanism. | Relevance Justification: Directly addresses how an AI feature (teaming in alert prioritization) causally affects human performance, though it does not specifically mention non-deterministic, opacity, or adaptive characteristics from the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI influence over time | Evidence Type: direct | Causal Strength: 8 | Performance Effect: enhanced human performance and increased cognitive workload","Flathmann et al. found that decreasing AI influence over time enhanced human performance, while sustained high influence increased cognitive workload [ 83]. This suggests allocation should not be static but should evolve as collaboration develops.","Causal Strength Justification: Direct causation is indicated by 'enhanced' and 'increased' linking AI influence changes to performance effects, supported by a reference to empirical findings. | Relevance Justification: Directly addresses how AI characteristics (influence dynamics) affect human performance, aligning with the research focus on novel AI features in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explainable AI | Evidence Type: direct | Causal Strength: 8 | Performance Effect: undetected errors due to over-reliance and failure to identify AI mistakes","Rosenbacke found that explainable AI increased trust but risked promoting over-reliance, leading to undetected errors [ 87]. This occurred specifically through “False Confirmation” errors where clinicians failed to identify AI mistakes.","Causal Strength Justification: Direct causal language ('risk promoting over-reliance, leading to undetected errors') and a specific mechanism ('through False Confirmation errors') are explicitly stated in the text. | Relevance Justification: Directly addresses how opacity (lack of explainability) in AI affects human performance by causing over-reliance and errors, aligning with the research focus on novel AI characteristics.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: deception detection tasks | Evidence Type: direct | Causal Strength: 8 | Performance Effect: limited appropriate reliance development","Schemmer et al. found that deception detection tasks with low human performance limited appropriate reliance development [ 89].","Causal Strength Justification: Direct causation is indicated by 'limited', which implies a cause-effect relationship where low human performance restricts reliance development. | Relevance Justification: This relates to human performance in AI interaction, specifically trust and reliance, which is relevant to the research focus on AI characteristics affecting human outcomes, though it does not directly mention non-deterministic, opacity, or adaptive features.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explainable AI | Evidence Type: direct | Causal Strength: 8 | Performance Effect: over-reliance and false assurance","Rosenbacke’s [ 87] finding that explainable AI can promote over-reliance through “False Confirmation” errors suggests that making AI reasoning visible does not straightforwardly enable users to identify when AI is wrong—it may instead provide false assurance that errors have been checked for and ruled out.","Causal Strength Justification: Direct causal language ('can promote through') and explicit mechanism linking AI feature to human effect. | Relevance Justification: Directly addresses how AI opacity/explainability affects human performance in high-risk contexts, aligning with research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explanation-based calibration support | Evidence Type: mechanism | Causal Strength: 7 | Performance Effect: reduced future learning opportunities","This paradox is compounded by Jang et al.’s explanation–action tradeoff [ 76], which indicates that calibration support through explanation may come at the cost of future learning opportunities.","Causal Strength Justification: Explicit causal mechanism ('may come at the cost of') linking AI feature to human performance effect. | Relevance Justification: Relates to AI opacity/explainability affecting human learning and calibration, relevant to high-risk industry performance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: adaptive agents in augmented reality | Evidence Type: direct | Causal Strength: 7 | Performance Effect: mitigated attentional issues technically but produced no significant overall task performance improvement","Syiem et al. found that adaptive agents in augmented reality mitigated attentional issues technically but produced no significant overall task performance improvement—benefits were limited to receptive users [ 93].","Causal Strength Justification: Direct causal language 'produced' links the AI feature to the performance effect, though the effect is mixed (mitigation of attentional issues but no overall improvement). | Relevance Justification: Directly addresses how adaptive AI features affect human performance (task performance and attentional issues), relevant to the research focus on context-aware/adaptive behavior.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI implementation | Evidence Type: direct | Causal Strength: 6 | Performance Effect: reduces team coordination effectiveness","Schmutz et al. reported that AI implementation often reduces team coordination effectiveness, with most findings based on laboratory rather than organizational settings [94].","Causal Strength Justification: Direct causal language 'reduces' links AI implementation to reduced team coordination effectiveness, though the evidence is qualified as 'often' and based on laboratory settings. | Relevance Justification: Directly addresses how AI implementation affects human performance (team coordination), relevant to the research focus on AI characteristics in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: transparency | Evidence Type: direct | Causal Strength: 8 | Performance Effect: over-reliance","Autonomous calibration mechanisms face the troubling finding that transparency may promote rather than prevent over-reliance.","Causal Strength Justification: Direct causal language is used ('promote'), and the relationship is explicitly stated as a finding, indicating a strong causal claim. | Relevance Justification: Directly addresses how an AI feature (transparency, related to opacity/explainability) causally affects human performance (over-reliance), which is a core aspect of the research focus on human-AI interaction in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: example-based presentations | Evidence Type: mechanism | Causal Strength: 9 | Performance Effect: supporting appropriate reliance behaviors by helping users identify when AI systems might fail","Importantly, example-based presentations provided clearer signals of AI unreliability, supporting appropriate reliance behaviors by helping users identify when AI systems might fail.","Causal Strength Justification: Direct causal mechanism 'supporting... by helping' explicitly links the AI feature (clearer signals) to the human performance effect (appropriate reliance behaviors) through a defined causal pathway. | Relevance Justification: Directly addresses how an AI feature (presentation method) causally affects human reliance behaviors through a specific mechanism, highly relevant to performance in high-risk interactions.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: example-based presentation methods | Evidence Type: direct | Causal Strength: 8 | Performance Effect: less disruptive to users’ natural intuition and promoted inductive reasoning patterns","These presentations provided concrete instances illustrating AI reasoning, which were found to be less disruptive to users’ natural intuition and promoted inductive reasoning patterns [ 29].","Causal Strength Justification: Direct causal language 'were found to be less disruptive' and 'promoted' explicitly links the AI feature to the human performance effect without inference. | Relevance Justification: Directly addresses how an AI presentation method (example-based) causally affects human intuition and reasoning patterns, relevant to human performance in interaction contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Feature-based visual presentations, particularly Class Activation Maps (CAMs) with traditional red-blue coloring schemes | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Improved diagnostic accuracy and physician confidence in medical imaging applications for detecting thoracolumbar fractures; enhanced effectiveness leading to successful trust calibration","significantly improved both diagnostic accuracy and physician confidence in medical imaging applications for detecting thoracolumbar fractures [ 100]. The effectiveness of these visual presentations was enhanced when they aligned with users’ existing cognitive models, suggesting that domain-appropriate visual design is crucial for successful trust calibration.","Causal Strength Justification: Direct causal language is present ('significantly improved', 'effectiveness was enhanced', 'suggesting that...is crucial for'), though the strongest causal terms like 'caused' or 'led to' are not used. The relationship is explicitly stated with clear cause (visual presentations) and effect (improved accuracy/confidence and trust calibration). | Relevance Justification: Directly addresses how an AI feature (visual presentations/explanations) affects human performance (diagnostic accuracy, physician confidence, trust calibration), which aligns with the research focus on human-AI interaction and trust/regulation challenges related to opacity/lack of explainability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) visualizations | Evidence Type: direct | Causal Strength: 6 | Performance Effect: Enhanced interpretability and increased human trust in AI decisions for malware detection tasks","In cybersecurity contexts, Local Interpretable Model-agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) visualizations enhanced interpretability and increased human trust in AI decisions for malware detection tasks [ 101].","Causal Strength Justification: Direct causal language is present ('enhanced', 'increased'), though not the strongest terms like 'caused'. The relationship is explicit with clear cause (visualizations) and effect (enhanced interpretability and increased trust). | Relevance Justification: Directly addresses how AI features (LIME and SHAP visualizations) affect human performance (interpretability and trust in AI decisions), which relates to the research focus on human-AI interaction and trust/regulation challenges from opacity/lack of explainability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Interactive frameworks enabling user engagement with AI recommendations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Strong effects upon trust calibration and reliance behaviors","Interactive frameworks that enabled user engagement with AI recommendations demonstrated particularly strong effects upon trust calibration and reliance behaviors.","Causal Strength Justification: Direct causal language using 'demonstrated particularly strong effects upon' explicitly connects the AI feature to human performance outcomes. | Relevance Justification: Directly addresses how AI system features (interactive frameworks) causally affect human performance (trust calibration and reliance behaviors), which aligns with the research focus on human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Ability to modify importance weights of previous races in marathon finish time predictions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Acceptance of the model’s recommendations and perceived model competence improved substantially","When coaches could modify the importance weights of previous races in marathon finish time predictions, both the acceptance of the model’s recommendations and perceived model competence improved substantially [ 67].","Causal Strength Justification: Direct causal relationship where the condition (coaches modifying weights) leads to improvement in human performance outcomes, with explicit language showing the effect. | Relevance Justification: Directly addresses how a specific AI feature (user-modifiable weights in predictions) causally affects human performance (acceptance and perceived competence), which aligns with the research focus on human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Interactive prediction models allowing users to adjust feature weights | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Significant benefits for both user perception and behavioral outcomes","Interactive prediction models allowing users to adjust feature weights showed significant benefits for both user perception and behavioral outcomes.","Causal Strength Justification: Direct causal language using 'showed significant benefits for' explicitly connects the AI feature to human performance outcomes. | Relevance Justification: Directly addresses how AI system features (interactive prediction models with user-adjustable weights) causally affect human performance (user perception and behavioral outcomes), which aligns with the research focus on human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explicit display of correct likelihood information | Evidence Type: direct | Causal Strength: 8 | Performance Effect: promoting appropriate trust behaviors","The explicit display of correct likelihood information proved effective in promoting appropriate trust behaviors.","Causal Strength Justification: Direct causal language 'proved effective in promoting' indicates a strong, demonstrated cause-effect relationship. | Relevance Justification: Directly addresses how an AI feature (communication of confidence/uncertainty) causally affects human performance (trust behaviors), which is the core research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: three strategies based on estimated human and AI correctness likelihood (Direct Display, Adaptive Workflow, Adaptive Recommendation) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: promoted more appropriate human trust in AI, particularly reducing over-trust when AI provided incorrect recommendations","Research comparing three strategies based on estimated human and AI correctness likelihood—Direct Display, Adaptive Workflow, and Adaptive Recommendation—found that all three approaches promoted more appropriate human trust in AI, particularly reducing over-trust when AI provided incorrect recommendations [105].","Causal Strength Justification: Direct causal language 'found that all three approaches promoted' indicates a strong, research-backed cause-effect relationship. | Relevance Justification: Directly addresses how AI features (communication strategies) causally affect human performance (trust calibration and reduction of over-trust), which is the core research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explaining how and why AI decisions were made | Evidence Type: direct | Causal Strength: 7 | Performance Effect: influencing both trust and reliance","Contextual and process-based interaction approaches focused on explaining how and why AI decisions were made, influencing both trust and reliance through enhanced understanding.","Causal Strength Justification: Direct causal language ('influencing') explicitly links the AI feature to human performance effects, with a mechanism ('through enhanced understanding'). | Relevance Justification: Directly addresses opacity/lack of explainability by showing how explaining AI decisions (countering opacity) causally affects human trust and reliance through understanding.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: transparency through revealing top AI recognitions | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increased understandings of and trust in AI capability, reduced perceived workload, better calibrated reliance behaviors","Higher transparency through revealing top AI recognitions increased understandings of and trust in AI capability while reducing perceived workload, contributing to better calibrated reliance behaviors [76].","Causal Strength Justification: Direct causal language ('increased', 'reducing', 'contributing to') explicitly links the AI feature to human performance effects. | Relevance Justification: Directly addresses opacity/lack of explainability by showing how transparency (countering opacity) causally affects human trust, understanding, workload, and reliance behaviors.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explanatory dialogs, justifications, AR-based visual guidance | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improved collaboration effectiveness, influenced reliance behaviors, improved task performance, shared awareness, reduced errors","Explanatory dialogs and justifications improved collaboration effectiveness and influenced reliance behaviors. AI systems providing justifications and AR-based visual guidance demonstrated improved task performance, shared awareness, and reduced errors compared to systems without these explanatory elements [107]. These approaches were particularly effective when combined with prescriptive and descriptive guidance that contextualized AI recommendations within the user’s task environment.","Causal Strength Justification: Direct causal language ('improved', 'demonstrated improved') is used to link AI features to human performance outcomes, with explicit comparison to systems without these features. | Relevance Justification: Directly addresses how AI features (explanatory elements) causally affect human performance metrics such as collaboration, reliance, task performance, awareness, and errors, aligning with the research focus on AI characteristics in high-risk industries.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Feature-based visual methods (Grad-CAM, LIME/SHAP) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Increased physicians' diagnostic trust; bolstered experts' confidence","Feature-based visual methods like Grad-CAM in medical imaging increased physicians’ diagnostic trust by aligning with familiar cognitive models, while LIME/SHAP approaches in cybersecurity applications bolstered experts’ confidence through clear decision rationales [ 101]. These approaches achieved trust enhancement by making AI reasoning visible and connected to domain knowledge.","Causal Strength Justification: Direct causal language: 'increased... by aligning', 'bolstered... through', 'achieved trust enhancement by making' | Relevance Justification: Directly addresses how AI opacity (lack of explainability) affects human trust through visualization methods that make reasoning visible","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Interactive presentation frameworks | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Boosted trust calibration; enhanced perceived reliability","Interactive presentation frameworks significantly boosted trust calibration by providing users with agency in the AI process. The co-creation aspect of these interactions appeared to enhance perceived reliability through the shared ownership of the outcomes [ 67].","Causal Strength Justification: Direct causal language: 'boosted... by providing', 'enhance... through' | Relevance Justification: Addresses how AI interaction approaches affect human trust through agency and co-creation mechanisms","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: example-based presentations | Evidence Type: direct | Causal Strength: 7 | Performance Effect: potentially preventing the blind acceptance of incorrect recommendations","Example-based presentations provided stronger signals of AI unreliability, potentially preventing the blind acceptance of incorrect recommendations [29].","Causal Strength Justification: Direct causal chain: example-based presentations 'provided stronger signals' (cause) which 'potentially preventing' (effect) blind acceptance. The word 'potentially' slightly weakens the certainty. | Relevance Justification: Directly links an AI presentation feature to preventing a specific human performance issue (blind acceptance of incorrect AI outputs), which is highly relevant to human-AI interaction in decision-making.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: visual presentation techniques (simple visual highlights) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reducing over-reliance on AI systems during difficult tasks","Visual presentation techniques, particularly simple visual highlights, demonstrated effectiveness in reducing over-reliance on AI systems during difficult tasks compared to prediction-only or written presentation techniques [ 102].","Causal Strength Justification: Direct causal language 'demonstrated effectiveness in reducing' explicitly links the AI feature (visual presentation) to the human performance effect (reduced over-reliance). | Relevance Justification: Directly addresses human-AI interaction by showing how an AI presentation feature causally affects human reliance behavior, which is a core aspect of performance in high-risk contexts.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: accurate information about AI recommendations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: better calibrated trust, with participants deviating more from low-quality recommendations and less from high-quality ones","Providing accurate information about AI recommendations led to better calibrated trust, with participants deviating more from low-quality recommendations and less from high-quality ones [ 74].","Causal Strength Justification: Direct causal language 'led to' explicitly connects the cause (providing accurate information) to the effect (better calibrated trust and specific behavioral changes). | Relevance Justification: Directly addresses how an AI feature (information provision) causally affects human performance (trust calibration and reliance behavior), though not specifically tied to the novel characteristics listed (non-deterministic, opacity, adaptive).","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Transparency mechanisms (revealing top AI recognitions, showing correct likelihood information) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Calibrated trust, reduced workload perceptions, reduced uncertainty","Transparency mechanisms, such as revealing the top five AI recognitions and showing correct likelihood information, calibrated trust by revealing AI capabilities and often reduced workload perceptions and uncertainty [76,105].","Causal Strength Justification: Direct causal language: 'calibrated trust by revealing' and 'reduced workload perceptions and uncertainty' explicitly state cause-effect relationships. | Relevance Justification: Directly addresses how an AI feature (transparency mechanisms) causally affects human performance (trust calibration, workload perceptions, uncertainty).","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Explanatory visualizations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Increased perceptions of AI usefulness, increased confidence in AI decisions, no significant effect on binary concordance","In clinical settings, explanatory visualizations increased clinicians’ perceptions of AI usefulness and their confidence in AI’s decisions yet did not significantly affect binary concordance with AI recommendations, suggesting that trust calibration effects may manifest in subtle ways beyond simple compliance measures [109].","Causal Strength Justification: Direct causal language: 'increased clinicians’ perceptions' and 'increased...their confidence' explicitly state cause-effect relationships. | Relevance Justification: Directly addresses how an AI feature (explanatory visualizations) causally affects human performance (perceptions, confidence, concordance), linking to opacity/explainability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Presentation and interaction approaches | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Shaped user reliance patterns, with implications for decision quality and human-AI complementarity","Beyond trust calibration, presentation and interaction approaches directly shaped user reliance patterns with significant implications for decision quality and human–AI complementarity.","Causal Strength Justification: Direct causal language: 'directly shaped user reliance patterns' explicitly states a cause-effect relationship. | Relevance Justification: Directly addresses how AI features (presentation and interaction approaches) causally affect human performance (user reliance patterns, decision quality, complementarity).","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Explainable AI, AI recommendations aligning with users' initial judgments | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Over-trust, trust development","Several studies identified contextual complications, including the potential for explainable AI to sometimes lead to over-trust [87], and the significant effect of confirmation bias on trust development when AI recommendations aligned with users’ initial judgments [108].","Causal Strength Justification: Direct causal language: 'lead to over-trust' and 'significant effect on trust development' explicitly state cause-effect relationships. | Relevance Justification: Directly addresses how AI features (explainable AI, AI recommendations aligning with user judgments) causally affect human performance (over-trust, trust development), linking to opacity/explainability.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: errors in one domain | Evidence Type: direct | Causal Strength: 7 | Performance Effect: delegation decisions affected in other domains","Additionally, humans violated choice independence in HAIC, with errors in one domain affecting delegation decisions in others.","Causal Strength Justification: Direct causal language 'with errors... affecting delegation decisions' explicitly links AI errors to human decision-making effects. | Relevance Justification: Directly addresses how AI characteristics (errors as a feature of non-deterministic systems) causally affect human performance (delegation decisions) across domains in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI making rare but large errors | Evidence Type: direct | Causal Strength: 8 | Performance Effect: humans less likely to delegate complex predictions to AI","Error types significantly influenced reliance patterns, with research showing that humans were less likely to delegate complex predictions to AI when it made rare but large errors, driven by higher self-confidence rather than lower confidence in the model [ 111].","Causal Strength Justification: Direct causal language 'significantly influenced' and explicit mechanism 'driven by' linking AI errors to human delegation behavior. | Relevance Justification: Directly addresses how AI characteristics (errors as a feature of non-deterministic systems) causally affect human performance (delegation decisions) in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI systems (generalized trust/distrust mechanism) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: uniform trust assumptions across contexts in high-risk environments","Research demonstrates that humans tend to generalize trust or distrust from one AI system to all AI agents, unlike with human teammates where trust is assessed individually [ 114]. This generalization becomes particularly problematic in high-risk environments where different AI systems may have vastly different reliability profiles, yet users apply uniform trust assumptions across contexts.","Causal Strength Justification: Direct causation is indicated by 'Research demonstrates that' leading to the effect, with explicit description of the problematic outcome in high-risk environments. | Relevance Justification: Directly addresses trust calibration challenges in human-AI systems, linking AI features (generalized trust/distrust) to human performance effects (uniform trust assumptions) in high-risk contexts, aligning with the research focus on novel AI characteristics affecting human performance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: generalization (implied from AI characteristics) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: inefficient monitoring behaviors and diversion of cognitive resources from primary decision-making tasks","The cognitive burden of this generalization manifests as inefficient monitoring behaviors following negative experiences, diverting cognitive resources from primary decision-making tasks—a critical concern in high-stakes environments where cognitive load management is essential for safety and effectiveness.","Causal Strength Justification: Direct causation is indicated by 'manifests as', explicitly linking the cognitive burden to inefficient monitoring behaviors and resource diversion. | Relevance Justification: Directly addresses how an AI-related cognitive burden affects human performance (monitoring and decision-making) in high-stakes contexts, aligning with the task focus on causal impacts of AI features on human performance.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI influence dynamics (temporal influence patterns) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: human performance improvement OR increased cognitive workload and inhibited skill development","AI teammates that decrease their influence over time enable humans to improve their performance, while highly influential AI teammates can increase perceived cognitive workload and potentially inhibit skill development [ 83].","Causal Strength Justification: Direct causal language using 'enable' and 'can increase...potentially inhibit' with specific outcomes | Relevance Justification: Directly addresses how AI characteristics (influence dynamics) affect human performance outcomes in high-risk contexts","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explainable AI / transparency mechanisms | Evidence Type: direct | Causal Strength: 8 | Performance Effect: dangerous over-reliance and undetected diagnostic errors","In healthcare settings, explainable AI increases clinicians’ trust compared to black-box systems, but can lead to dangerous over-reliance, resulting in undetected diagnostic errors [ 87]. This demonstrates that transparency mechanisms can create new vulnerabilities rather than simply enhancing decision quality, particularly in high-risk contexts where the stakes of misplaced trust are severe.","Causal Strength Justification: Direct causal language: 'can lead to' and 'resulting in' explicitly link explainable AI to over-reliance and diagnostic errors. | Relevance Justification: Directly addresses how an AI feature (explainability/transparency) causally affects human performance (over-reliance and diagnostic errors) in a high-risk domain (healthcare), matching the research focus on opacity and trust challenges.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI supporting individual goals | Evidence Type: direct | Causal Strength: 7 | Performance Effect: maintain human engagement and active participation in decision processes","Human acceptance of AI collaboration depends more on whether the AI supports individual goals rather than on the optimization of overall performance metrics [ 83]. This finding has important implications for skill preservation, as systems that align with human motivations may be more likely to maintain human engagement and active participation in decision processes.","Causal Strength Justification: Direct causal language 'depends more on' and 'may be more likely to' explicitly links AI feature to human effect, though 'may' indicates some uncertainty. | Relevance Justification: Directly addresses how AI characteristics (supporting goals) affect human performance (engagement and participation), relevant to skill preservation in human-AI interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI serving as a decision support tool | Evidence Type: mechanism | Causal Strength: 6 | Performance Effect: systematic degradation of critical diagnostic skills","In healthcare contexts, specific patterns of skill degradation emerge through error types that reflect diminished human judgment. “False Confirmation” errors occur when clinicians fail to identify AI mistakes due to over-reliance, while “False Conflict” errors arise from cognitive biases like commitment and confirmation bias [ 87]. These error patterns suggest the systematic degradation of critical diagnostic skills when AI serves as a decision support tool.","Causal Strength Justification: Causal mechanism 'through' and 'due to' explicitly links AI use to error patterns and skill degradation, though 'suggest' indicates correlation rather than direct causation. | Relevance Justification: Directly links AI features (decision support) to human performance effects (skill degradation via errors), highly relevant to the research focus on AI characteristics in high-risk industries like healthcare.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI recommendations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: False Confirmation errors (failure to identify AI mistakes) and False Conflict errors (inappropriate rejection of accurate AI recommendations)","“False Confirmation” errors occur when clinicians fail to identify AI mistakes due to over-reliance, while “False Conflict” errors arise from cognitive biases like commitment and confirmation bias when clinicians inappropriately reject accurate AI recommendations.","Causal Strength Justification: Direct causation is indicated by 'occur when' and 'arise from', linking over-reliance and cognitive biases to specific error types with clear mechanisms. | Relevance Justification: Directly addresses how human interaction with AI features (recommendations) leads to performance errors, aligning with the research focus on human-AI interaction effects.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: Explainable AI systems (transparency mechanisms) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Dangerous over-reliance and undetected diagnostic errors","Explainable AI systems increase clinicians’ trust compared to black-box alternatives but can paradoxically lead to dangerous over-reliance, resulting in undetected diagnostic errors [ 87]. This demonstrates that transparency mechanisms designed to enhance decision quality can create new vulnerabilities in healthcare settings.","Causal Strength Justification: Direct causal language is used: 'lead to' and 'resulting in' explicitly connect the AI feature to the human performance effect, with a clear mechanism described. | Relevance Justification: Directly addresses how opacity (or its reduction through explainability) affects human performance (over-reliance and errors) in a high-risk industry (healthcare), matching the research focus.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI presentation methods | Evidence Type: direct | Causal Strength: 8 | Performance Effect: user performance outcomes and trust outcomes","Table 2 examines the relationship between AI presentation methods and user performance outcomes, highlighting the critical role of presentation design in shaping collaboration effectiveness. The evidence shows that presentation methods significantly influence both performance and trust outcomes, with visual presentations and contextual feedback demonstrating particularly strong effects across different domains.","Causal Strength Justification: Direct causal language: 'examines the relationship', 'significantly influence', 'strong effects'. | Relevance Justification: Directly addresses causal relationships between AI features (presentation methods) and human performance/trust outcomes, aligning with the research focus on AI characteristics affecting human interaction.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: AI information first, human judgment second design | Evidence Type: direct | Causal Strength: 7 | Performance Effect: anchoring effects potentially influencing human researchers toward AI-suggested interpretations","Second, the “AI information first, human judgment second” design may create anchoring effects, potentially influencing human researchers toward AI-suggested interpretations.","Causal Strength Justification: The text uses 'may create' to indicate a direct causal possibility, supported by the mechanism of design influencing human judgment, though it is not a definitive statement. | Relevance Justification: This directly addresses how an AI feature (design order) causally affects human performance (cognitive bias in judgment), aligning with the research focus on human-AI interaction and performance effects.","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: explainable AI / transparency mechanisms | Evidence Type: direct | Causal Strength: 8 | Performance Effect: dangerous over-reliance","Some studies demonstrate that transparency mechanisms improve trust calibration, while other research found that explainable AI led to dangerous over-reliance in healthcare contexts [ 76,87,105].","Causal Strength Justification: Direct causal language 'led to' explicitly connects the AI feature to the performance effect. | Relevance Justification: Directly addresses how opacity/explainability (a novel AI characteristic) affects human performance (over-reliance) in a high-risk industry (healthcare).","y"
"Enhancing Intuitive Decision-Making and Reliance Through Human–AI Collaboration: A Review","","","AI Feature: interactive methods / user control | Evidence Type: mechanism | Causal Strength: 7 | Performance Effect: increase over-trust","Several studies demonstrated that user control improved outcomes, yet other research found that interactive methods may increase over-trust through co-creation effects [ 66,67,86].","Causal Strength Justification: Causal language 'may increase' with mechanism 'through co-creation effects' explicitly connects the AI feature to the performance effect. | Relevance Justification: Directly addresses how AI interaction features (related to control/adaptability) affect human performance (over-trust) via a specific mechanism, though not explicitly tied to non-deterministic, opacity, or context-adaptive characteristics.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","Category: opacity | Feature: System opaqueness from interconnected complexities","","","The peculiarity of non-routine situations in highly automated and high-risk industries is that they are infrequent but can develop dynamically and are characterized by complexities resulting from the interconnectedness of system elements, which can result in opaqueness for the operator [13,14].","This directly addresses opacity/explainability issues (black-box behavior) where interconnected system elements create complexities that result in opaqueness for operators, aligning with the research focus on AI/autonomous system characteristics in high-risk industries.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 8","","The infrequent use of skills relevant in non-routine situations in highly automated and high-risk industries is a major safety issue. The infrequent use of skills can lead to skill decay.","Severity Justification: The text identifies this as a 'major safety issue' in high-risk industries, indicating significant potential consequences. | Relevance Justification: Directly addresses skill degradation (skill decay) in highly automated environments, which is a core focus of the research query.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","skill acquisition, retention, and transfer into account. Keywords: automation; non-routine situation; complex cognitive skill; skill acquisition; skill deterioration; skill decay; skill loss; skill transfer; refresher","Severity Justification: The terms 'skill deterioration', 'skill decay', and 'skill loss' are explicitly mentioned, indicating a recognized and significant degradation in human performance capabilities, particularly in high-risk contexts where such decay can lead to errors. | Relevance Justification: The keywords directly reference skill degradation, which is a core aspect of human performance degradation, and are linked to automation and non-routine situations, aligning with the research focus on AI/autonomous systems in high-risk industries.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: situational_awareness | Severity: 7","","the serious danger constituted by the hurricane went unrecognized because none of the experienced operators could remember the severity of the past flooding [15].","Severity Justification: The degradation led to a major health and safety risk for people 1.5 miles around the facility, indicating high severity due to potential harm. | Relevance Justification: This directly relates to human performance degradation in high-risk industries, specifically involving reduced situational awareness or memory failure in experienced operators, which aligns with the research focus on human-AI interaction challenges.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Due to the infrequent occurrence of non-routine situations, the operator rarely applies all relevant skills and knowledge.","Severity Justification: The degradation is moderate because it directly links infrequent practice to reduced application of skills and knowledge, which is a known factor in skill atrophy, though it does not explicitly state severe performance failures. | Relevance Justification: High relevance as it explicitly mentions operators rarely applying skills and knowledge, which aligns with the skill/knowledge degradation category in the research focus on human performance issues in high-risk industries.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Nevertheless, in the infrequent occurrences of non-routine situations, operators are required to transfer their knowledge and skills under time pressure [ 12].","Severity Justification: The requirement to transfer knowledge and skills under time pressure in infrequent situations suggests performance degradation due to high cognitive load and lack of readiness. | Relevance Justification: Highlights a specific scenario where human performance is challenged, linking infrequent use of skills to potential degradation under pressure.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","that only limited amounts of learned knowledge and skills in the human–automation interaction are used [ 9]. However, knowledge and skills that have been once learned must only be retrieved in the case of system failure (i.e., years after the initial training).","Severity Justification: The text explicitly states that knowledge and skills are used only in limited amounts and retrieved years after training, indicating significant skill atrophy due to infrequent use. | Relevance Justification: Directly addresses skill degradation in human-automation interaction, which is a key performance issue in high-risk industries.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: behavioral_changes | Severity: 6","","operators experience difﬁculties in acting adequately when a non-routine situation arises. This is in line with the results from interviews with operators from a chemical plant [ 18].","Severity Justification: The excerpt indicates a clear performance issue where operators struggle to respond appropriately to non-routine situations, suggesting moderate to significant degradation in operational effectiveness. | Relevance Justification: This directly describes human performance degradation in a high-risk industrial context (chemical plant), aligning with the research focus on novel AI/autonomous system characteristics versus conventional automation, though it does not explicitly mention AI-related factors.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Knowledge and skills that have been acquired once but are infrequently applied over longer non-use periods may be prone to decay. Skill decay is deﬁned as the inability to retrieve formerly trained and acquired knowledge and skills after periods of non-use with a consequence of decreased performance [ 16,17].","Severity Justification: Skill decay directly impairs performance by reducing the ability to retrieve knowledge and skills, which can lead to errors or inefficiencies in high-risk tasks, though it is a gradual process. | Relevance Justification: The text explicitly mentions decreased performance as a consequence of skill decay, aligning with the research focus on human performance degradations, particularly in skill/knowledge aspects, though it does not specify AI-related contexts.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","In order to do so, operators need to be familiar with failures, identify subtle aspects of complex problems, and retrieve and integrate knowledge and skills that were once learned, but have been infrequently applied [12].","Severity Justification: It suggests performance challenges from infrequent application of skills, though less directly tied to safety than the first excerpt. | Relevance Justification: It relates to skill degradation from infrequent use, matching the skill/knowledge category, but does not mention AI specifically.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","Consequently, the combination of knowledge and skill decay due to an infrequent application and occurrence of non-routine situations can endanger people’s safety and reduce efﬁciency in high-risk organizations.","Severity Justification: The degradation directly links to safety risks and efficiency loss in high-risk contexts, indicating significant impact. | Relevance Justification: It explicitly mentions knowledge and skill decay, which aligns with the skill/knowledge category in the priority search, though it does not specify AI-related causes.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","Thus, to attenuate skill decay in non-routine situations and to promote a safe and efﬁcient operator performance within a high-risk and highly automated environment, the consideration of potential inﬂuencing factors related to the task, the method, and the person is crucial.","Severity Justification: Skill decay is directly identified as a problem requiring attenuation in high-risk, automated environments, implying it is a significant performance degradation with safety implications. | Relevance Justification: The excerpt directly addresses 'skill decay' (a form of performance degradation) in the context of a 'highly automated environment', which aligns with the research focus on human-AI interaction in high-risk industries, though it does not specify novel AI vs. traditional automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","the decay of skills is influenced by task-related, method-related, and person-related variables, which are already relevant in an early phase of skill acquisition but might also influence later phases of skill transfer or retention.","Severity Justification: It identifies variables that influence skill decay, suggesting a risk to performance, but it is more descriptive than indicative of high severity, so a moderate score is given. | Relevance Justification: This is relevant to skill degradation in human performance, aligning with the research focus, though it lacks direct mention of AI or novel system characteristics.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","skills for cognitively complex tasks, such as monitoring, observing changes, making predictions based on system elements, and adjusting deviant processes, take time to acquire and might decay faster, depending on their characteristics [25,26].","Severity Justification: The excerpt explicitly mentions skill decay for complex tasks, which can degrade human performance, but it does not specify severe or immediate impacts, so a moderate severity is assigned. | Relevance Justification: This directly addresses skill degradation, a key aspect of human performance issues, and is relevant to the research focus on human-AI interaction, though it does not specifically mention AI-related degradations.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","a re-examination of the skill decay literature with a focus on highly automated environments in high-risk industries seems warranted.","Severity Justification: The text directly addresses skill decay in high-risk, highly automated environments, indicating a recognized and significant concern that warrants updated research, suggesting moderate to high severity due to the critical nature of these industries. | Relevance Justification: This is a direct mention of skill decay (a form of human performance degradation) in the context of highly automated environments, which aligns perfectly with the research focus on novel AI/autonomous system characteristics vs. conventional automation in high-risk industries.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","One way to attenuate skill decay is through refresher interventions. The objective of refresher interventions is the re-establishment of a specific skill level that was acquired during the initial training but has not been practiced for a long period of time [29]. High-risk industries that have to deal with highly automated environments counter skill decay with periodic refresher interventions/recurrent trainings, which are declared mandatory in some industries (including aviation—see, e.g., [30]; nuclear power plants—see, e.g., [31]; maritime; see, e.g., [32]) but not in others.","Severity Justification: Skill decay is explicitly mentioned as a degradation requiring mandatory interventions in high-risk industries, indicating significant impact on performance and safety. | Relevance Justification: Directly addresses skill degradation in automated environments, aligning with the research focus on human performance issues in high-risk industries, though not specifically novel AI-related.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","we can extract valuable information from the existing literature about skill decay, but the transferability to complex cognitive skills in high-risk and highly automated environments is restricted.","Severity Justification: Skill decay is directly mentioned as a concern, but the severity is moderate as it focuses on transferability limitations rather than explicit degradation incidents. | Relevance Justification: Highly relevant to human performance degradation, specifically skill decay in high-risk automated settings, aligning with the research focus on novel AI-related issues.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","Thus, we extend the literature on skill decay by focusing on complex cognitive tasks, refresher interventions, and the retention of skills over a longer period of time (temporal transfer) and for novel situations (adaptive transfer), which to our knowledge have not been previously reviewed.","Severity Justification: Skill decay is a central theme, but severity is lower as it is framed as a research gap rather than an active degradation problem. | Relevance Justification: Directly relevant to human performance degradation through skill decay in complex cognitive tasks, closely tied to high-risk automated environments and AI-related challenges.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","Skill decay is deﬁned as the inability to retrieve formerly trained and acquired knowledge and skills after periods of non-use with a consequence of decreased performance.","Severity Justification: The text explicitly links skill decay to decreased performance, indicating a direct negative impact on human capability in high-risk domains. | Relevance Justification: Skill decay is a core human performance degradation issue relevant to high-risk industries where infrequent but critical tasks require retained skills.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","to identify the inﬂuencing factors that could be detrimental to complex cognitive skill retention and to identify the refresher interventions that could mitigate decay.","Severity Justification: The text explicitly identifies factors detrimental to skill retention, suggesting a moderate severity as it addresses skill decay in high-risk environments, but does not specify the extent or impact. | Relevance Justification: This directly relates to skill degradation, a key aspect of human performance degradation, and is mentioned in the context of high-risk, highly automated environments, aligning with the research focus on novel AI-related issues.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","Identify refresher interventions that are deemed to be successful for retaining complex cognitive skills for infrequent situations (temporal transfer) and discuss whether they can promote handling of novel situations (adaptive transfer) in highly automated environments (connection 2, Figure 1).","Severity Justification: The focus on retaining complex cognitive skills implies that degradation is a significant concern in automated settings, warranting interventions to mitigate performance decline. | Relevance Justification: This excerpt verbatim mentions skill retention in automated environments, directly relevant to skill degradation as a human performance issue in the context of novel AI systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 8","","The ﬁndings can be used by industry and training providers as resources to design training and refresher interventions based on a systematic approach that can help to mitigate complex cognitive skill decay in companies in high-risk industries.","Severity Justification: Mitigating complex cognitive skill decay is presented as a primary objective, highlighting its severe impact on human performance in high-risk automated contexts. | Relevance Justification: This verbatim excerpt directly addresses skill decay mitigation, making it highly relevant to the research focus on human performance degradations in automated systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 8","","Determine whether well-researched factors from the literature on skill decay (e.g., task, method, and person-related) are also relevant for attenuating complex cognitive skill decay for situations that are infrequent (temporal transfer) and/or novel (adaptive transfer) in companies in high-risk industries (connection 1, Figure 1);","Severity Justification: Complex cognitive skill decay is highlighted as a critical issue requiring attenuation in automated high-risk settings, suggesting high severity due to its impact on handling non-routine events. | Relevance Justification: This verbatim excerpt directly relates to skill degradation, a priority category, and addresses novel AI-related contexts through adaptive transfer in automated environments, aligning with the research focus.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","How can skill decay in non-routine situations be attenuated within a high-risk, highly automated environment?","Severity Justification: Skill decay is directly identified as a problem requiring attenuation in high-risk automated settings, indicating significant impact on human performance and safety. | Relevance Justification: This directly addresses skill degradation, a priority search category, and is verbatim from the chunk, making it highly relevant to the research focus on human performance in automated systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: performance_metrics | Severity: 7","","Tasks that require a considerable number of action steps that are performed in a specific required order and are accomplished in a timely manner, decay more rapidly [16,25,27,34,47,52,53,61–64].","Severity Justification: The degradation is explicitly stated as 'decay more rapidly' with multiple citations, suggesting a significant and documented effect on performance in complex tasks. | Relevance Justification: This directly addresses human performance degradation in task execution, relevant to the research focus on AI/autonomous systems in high-risk industries where complex tasks are common, though it does not specify AI-related aspects.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","The task-characteristics that proved to be more prone to decay were closed-loop, procedural, discrete, and controlled processing tasks [16,25,27,34,47,52,61–64]. Task characteristics that were less prone to skill decay included open-looped, continuous, and automatic processing tasks [ 16,25,27,34,47,52–54,61–65].","Severity Justification: The text explicitly discusses task characteristics being 'more prone to decay' and 'less prone to skill decay,' indicating a moderate severity as it directly links task types to skill retention issues without specifying extreme consequences. | Relevance Justification: This is highly relevant to human performance degradation as it directly addresses skill decay and retention in relation to task characteristics, aligning with the research focus on cognitive aspects in high-risk industries, though it does not specifically mention AI or automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Individuals with higher cognitive abilities acquired more knowledge during the acquisition phase, but the decay of knowledge and skills seemed to be equal for both high- and low-ability individuals [ 25,75]. This indicates that different cognitive strategies are necessary for recalling important knowledge and skills after periods of non-use, and for adapting these skills to novel situations.","Severity Justification: The excerpt explicitly mentions decay of knowledge and skills, which directly indicates performance degradation, but it is framed in a general cognitive context without specifying severe impacts or novel AI-related issues. | Relevance Justification: The text addresses skill degradation and knowledge decay, which aligns with the research focus on human performance degradations, but it does not explicitly link to novel AI characteristics or high-risk industries, reducing direct relevance.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","The demographic factor of age did not influence skill decay directly, but could potentially influence decay indirectly through stress and work experience [ 53,63]. It was stated that there are benefits of higher age in terms of the acquisition of experience through tenure, but the information processing ability under stress declined with age, which could hamper skill retention [63].","Severity Justification: The text explicitly mentions that age-related decline in information processing ability under stress could hamper skill retention, indicating a moderate impact on human performance. | Relevance Justification: This directly addresses skill degradation, a key focus area, by discussing factors (age, stress) that affect skill decay and retention, though it is not specifically tied to novel AI systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","Skill decay for a complex cognitive task also depends on person-related factors. A considerable number of studies have emerged in the last few decades that have concentrated on person-related factors regarding complex cognitive skill decay, focusing on cognitive factors (cognitive ability, general mental ability (GMA), retentivity, memory), experience, age, personality factors, and motivation.","Severity Justification: The text directly mentions 'skill decay' as dependent on person-related factors, indicating a degradation in human performance, though it is presented as a factor rather than an absolute outcome. | Relevance Justification: The text directly addresses 'skill decay' as a form of human performance degradation, which is a priority search category (skill/knowledge: skill degradation, knowledge loss, skill atrophy, reduced competence).","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Tasks that are not simple to proceduralize, that require considerable numbers of discrete cognitive processes, and that are not well organized are prone to skill decay, whereas tasks that are more meaningful, highly organized, and consistent across situations might reduce the mental effort [70], and thus might decrease the amount of skill decay (Table 4).","Severity Justification: The text explicitly states that tasks with certain characteristics are 'prone to skill decay,' indicating a direct and significant degradation in human performance (skill loss). | Relevance Justification: The text directly addresses 'skill decay' as a form of human performance degradation, which is a priority search category (skill/knowledge: skill degradation, knowledge loss, skill atrophy, reduced competence).","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 4","","It was argued that when individuals who were open to experience were confronted with a novel situation, they were likely to try out different approaches that are not necessarily promising.","Severity Justification: The excerpt suggests a potential negative influence on skill transfer or adaptation due to trying unpromising approaches, but it is not explicitly stated as a severe degradation. | Relevance Justification: This relates to skill/knowledge issues (skill transfer) in human performance when dealing with novel situations, which could be relevant to AI/autonomous system interactions, though not directly mentioned.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 4","","Further, individuals who were highly deliberate, orderly, and dutiful, which are characteristics of the personality trait of conscientiousness, were less able to adapt their skills to different situations [ 75].","Severity Justification: The degradation is moderate as it describes a specific limitation in skill adaptation for a personality trait, but it does not imply complete loss or severe atrophy. | Relevance Justification: Moderate relevance to skill degradation as it directly mentions reduced ability to adapt skills, which aligns with skill/knowledge issues, though it is not explicitly linked to AI or automation contexts.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","skill deterioration could not be mitigated [ 88].","Severity Justification: The degradation is directly stated as a failure to mitigate skill deterioration, implying a moderate impact on performance. | Relevance Justification: This directly addresses skill degradation, which is a key focus in human performance issues, though it is not explicitly linked to novel AI characteristics in this chunk.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: cognitive_overload | Severity: 6","","When individuals were inexperienced in handling complex tasks, it was suggested that a behavior-based approach (e.g., overlearning) first be provided and then a cognitive-based approach to minimize cognitive overload [57].","Severity Justification: Cognitive overload is explicitly mentioned as a risk for inexperienced individuals handling complex tasks, requiring specific training approaches to mitigate it, indicating a moderate severity as it is a recognized issue with a proposed solution. | Relevance Justification: This directly addresses cognitive overload, which is a human performance degradation related to cognitive load, as specified in the priority search. It is relevant to the research focus on novel AI/autonomous systems in high-risk industries, where complex tasks and training methods are critical.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Whereas knowledge was retained, there was an overall moderate skill decay when cognitive-based refresher interventions were conducted [ 66]. This suggests that some parts of skill decay can be retarded by cognitive-based refresher interventions, but that skills still decline signiﬁcantly [48,53].","Severity Justification: The text explicitly mentions 'moderate skill decay' and that 'skills still decline signiﬁcantly', indicating a notable but not extreme level of degradation. | Relevance Justification: This directly addresses skill degradation, which is a key aspect of human performance degradation in the context of training and refresher interventions, aligning with the research focus on human performance issues.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","both cognitive-based and behavioral-based approaches have positive effects on parts of skill retention, although skill decay cannot be attenuated completely.","Severity Justification: Skill decay is directly stated as a persistent issue that cannot be fully mitigated, suggesting a moderate but ongoing degradation. | Relevance Justification: Skill decay is a key human performance degradation related to skill/knowledge loss, which is a priority search category, and it is explicitly mentioned in the chunk.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","decayed the most, despite the practice refresher intervention. It remained unclear why this skill area decayed more than other more complex skills in spite of the practice refresher intervention. Similar results have been found in other studies that showed that some skill areas remain almost intact (e.g., declarative knowledge) and others decline rapidly [ 67,97]. Thus, practice has many positive effects on the retention of some skill areas by increasing the retrieval strength, but other skill areas might still decay [62].","Severity Justification: The text explicitly mentions skill decay occurring despite practice refresher interventions, indicating a persistent degradation issue, though it is context-specific to certain skills. | Relevance Justification: This directly addresses skill degradation, which is a key aspect of human performance degradations in human-AI interaction, as per the priority search list.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","the concept of team skill decay emerged as a key concept when considering non-routine situations in high-risk and highly automated environments.","Severity Justification: The text explicitly identifies 'team skill decay' as a 'key concept' in high-risk, automated contexts, indicating a recognized and significant issue, though the severity is not quantified. | Relevance Justification: Directly mentions 'skill decay' in the context of 'highly automated environments', which aligns with the research focus on human performance degradations related to automation/AI systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","team skill decay cannot be solely based on individual skill decay and that the execution of coordinative and communicative processes, and interdependence, play additional roles.","Severity Justification: The excerpt directly mentions 'team skill decay,' indicating a performance degradation related to skill loss, but it does not specify severity levels or impacts, so a moderate score is assigned. | Relevance Justification: The excerpt is relevant to skill degradation as it discusses team skill decay, which aligns with the research focus on human performance degradations, though it is not explicitly linked to novel AI-related degradations vs traditional automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Consequently, when team members were less able to coordinate and exchange information, despite the task being highly interdependent, team skill decay was more likely to occur [ 98].","Severity Justification: The excerpt directly mentions 'team skill decay' as a consequence of reduced coordination and information exchange, indicating a moderate impact on performance. | Relevance Justification: This excerpt explicitly discusses skill decay in a team context, which aligns with the research focus on human performance degradations, though it is not specifically tied to novel AI-related degradations vs. traditional automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","This means that team skill decay was not only caused by a loss of individual skills, but also by a loss of team interaction skills (team coordination and team situational awareness) [ 99].","Severity Justification: The excerpt identifies multiple causes of team skill decay, including loss of team coordination and situational awareness, suggesting a significant degradation in performance. | Relevance Justification: This excerpt directly addresses skill decay and its components, such as situational awareness, which is relevant to human performance degradations, though it lacks explicit mention of AI or automation contexts.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Conversely, operators who lack the essential knowledge elements to understand system dynamics can be overwhelmed when training cognitive-based methods.","Severity Justification: Being overwhelmed during training directly indicates a performance issue, but the context is training rather than operational performance, so severity is moderate. | Relevance Justification: This relates to skill/knowledge issues as operators lack essential knowledge, which is a form of skill degradation or competence gap relevant to human performance with systems.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","It was demonstrated that the level of skill acquisition has a significant influence on the retention of complex cognitive skills that are not applied for long periods (temporal transfer) and on effectiveness in novel situations (adaptive transfer).","Severity Justification: The text identifies a significant influence on skill retention when skills are not applied for long periods, which implies degradation, but it does not specify extreme or catastrophic outcomes, so a moderate-high severity is appropriate. | Relevance Justification: This directly addresses skill degradation in complex cognitive tasks relevant to high-risk and highly automated environments, matching the research focus on human performance issues, though it does not explicitly mention AI or automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","The level of skill acquisition and thus influence on skill retention are affected by person-related factors, method-related factors, and task characteristics.","Severity Justification: The text implies that skill retention can be negatively affected by various factors, suggesting degradation, but it does not specify the extent or impact, so a moderate severity is assigned. | Relevance Justification: This is relevant as it relates to skill retention in complex cognitive tasks, which ties to human performance in automated environments, though it lacks explicit mention of degradation or automation.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 6","","Additionally, the research revealed that any refresher intervention is better than no refresher intervention to attenuate skill decay.","Severity Justification: The text confirms the existence of skill decay, which is a form of performance degradation, but it suggests it can be attenuated with interventions, indicating it is manageable rather than severe. | Relevance Justification: This is highly relevant as it directly states 'skill decay' in the context of complex cognitive skills in high-risk and highly automated environments, aligning closely with the research focus on human performance degradations.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","Thus, for measuring skill decay, it remains unclear when skill acquisition should end and the non-use period should begin. Additionally, several studies have investigated the influences of individual differences on skill decay, and valuable insights could be gained with respect to cognitive ability. However, it was still not possible to draw conclusions about the processes of cognitive structures or learning preferences leading to a greater or lesser decay after periods of non-use.","Severity Justification: The text explicitly mentions 'skill decay' and its measurement challenges, indicating a moderate severity as it highlights a recognized issue but does not specify severe consequences or direct links to novel AI-related degradations. | Relevance Justification: The text is relevant to skill degradation, a priority search category, but it does not explicitly connect to novel AI-related degradations vs. traditional automation, focusing more on general skill decay and cognitive factors.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 5","","Further, attitudes and team skill decay were not often considered in relation to complex cognitive skill acquisition, transfer, and retention in high-risk industries, which indicates a promising research area.","Severity Justification: The text explicitly mentions 'skill decay' as a factor not often considered, implying it is a recognized issue but not extensively studied, suggesting moderate severity due to its potential impact on performance in high-risk contexts. | Relevance Justification: This directly relates to human performance degradation through skill decay in high-risk industries, aligning with the research focus on novel AI/autonomous system characteristics, though it does not specify AI-related aspects; it is highly relevant as it addresses skill degradation in the context of complex cognitive tasks.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","Category: skill_degradation | Severity: 7","","Likewise, individuals may not need to apply formerly acquired skill components for months or years and can become unable to act when confronted with technological failures, which can consequently lead to the escalation of a situation.","Severity Justification: The excerpt directly links skill non-use over time to an inability to act during failures, which can escalate situations in high-risk contexts, indicating a moderate to high severity impact on performance. | Relevance Justification: This excerpt explicitly mentions skill components not being applied for long periods and the resulting inability to act during technological failures, which aligns with the research focus on human performance degradations in high-risk industries, though it does not specifically mention AI-related aspects.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: interconnectedness of system elements | Evidence Type: direct | Causal Strength: 7 | Performance Effect: opaqueness for the operator","The peculiarity of non-routine situations in highly automated and high-risk industries is that they are infrequent but can develop dynamically and are characterized by complexities resulting from the interconnectedness of system elements, which can result in opaqueness for the operator [ 13,14].","Causal Strength Justification: Direct causal language is used: 'complexities resulting from the interconnectedness... which can result in opaqueness'. The relationship is explicitly stated, not implied. | Relevance Justification: Directly addresses how a feature of automated/high-risk systems (interconnectedness leading to complexity) causally affects human operator performance by creating opaqueness, which aligns with the research focus on opacity/lack of explainability.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: automation technology | Evidence Type: direct | Causal Strength: 8 | Performance Effect: change in the nature of work","The implementation of automation technology in many organizational processes in high-risk industries has led to a change in the nature of work [ 5,6].","Causal Strength Justification: Direct causation is explicitly stated with 'led to', indicating a strong causal link. | Relevance Justification: The excerpt directly links automation technology to a change in work nature, which is relevant to human performance, but it does not specify novel AI characteristics like non-deterministic, opacity, or adaptive features as requested in the task.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: highly automated environments | Evidence Type: direct | Causal Strength: 8 | Performance Effect: operators are required to handle tasks that are increasingly complex and demand predominately cognitive skills","Due to highly automated environments, operators in high-risk industries are required to handle tasks that are increasingly complex and demand predominately cognitive skills [ 6].","Causal Strength Justification: Direct causation is explicitly stated with 'Due to', indicating a strong causal link. | Relevance Justification: The excerpt directly links highly automated environments to increased task complexity and cognitive demands on operators, which is relevant to human performance, but it does not specify novel AI characteristics like non-deterministic, opacity, or adaptive features as requested in the task.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: behavior-based methods (drills, practice, part-whole training) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: proceduralized skills and increased long-term retention of task components, but no skill transfer to novel situations","While well-acquired task components resulted in proceduralized skills, which increased the long-term retention of task components [23,61,70], these proceduralized skills did not necessarily result in skill transfer to novel situations [60].","Causal Strength Justification: Direct causal language: 'resulted in' and 'increased' explicitly indicate causation. The text directly states the cause (well-acquired task components) and effect (proceduralized skills and increased retention). | Relevance Justification: The text discusses causal relationships between training methods (behavior-based methods like drills and practice) and human performance outcomes (proceduralized skills, retention, skill transfer). However, it does not explicitly mention AI features (non-deterministic, opacity, adaptive) or directly link them to human performance. The causal relationships are about training methods and skill acquisition, not AI characteristics.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: provision of feedback | Evidence Type: direct | Causal Strength: 7 | Performance Effect: increased motivation and improved accuracy of performance","The provision of feedback seemed to have a positive influence by increasing the individual’s motivation and improving the accuracy of performance.","Causal Strength Justification: Direct causal language ('by increasing', 'improving') explicitly connects the cause (feedback provision) to the effects (increased motivation, improved performance accuracy). | Relevance Justification: The text discusses feedback as a facilitating factor for skill acquisition and retention, which relates to human performance, but does not explicitly mention AI features like non-deterministic, opacity, or adaptive behavior.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: cognitive-based training approaches | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increased mental model forming, higher knowledge levels, and increased skill retention","These cognitive-based approaches led to an increase in mental model forming and higher levels of knowledge, which in turn increased skill retention.","Causal Strength Justification: Direct causal language ('led to', 'which in turn increased') explicitly connects the cause (cognitive-based approaches) to the effects (mental model forming, knowledge levels, skill retention). | Relevance Justification: The text discusses training methods for handling complex tasks, which relates to human performance with systems, but does not explicitly mention AI features like non-deterministic, opacity, or adaptive behavior.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: Not explicitly mentioned in the excerpt. The text discusses causes of team skill decay, not specific AI features. | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Team skill decay.","This means that team skill decay was not only caused by a loss of individual skills, but also by a loss of team interaction skills (team coordination and team situational awareness) [ 99].","Causal Strength Justification: Direct causal language ('caused by') explicitly links the causes (loss of individual skills, loss of team interaction skills) to the effect (team skill decay). | Relevance Justification: The excerpt explicitly identifies causes of team skill decay, a human performance effect. However, it does not connect these causes to any specific AI feature (non-deterministic, opacity, adaptive) as requested in the task. The causes are related to individual and team skills, not AI characteristics.","y"
"Factors Influencing Attenuating Skill Decay in High-Risk Industries: A Scoping Review","","","AI Feature: Not explicitly mentioned in the excerpt. The text discusses team dynamics, not specific AI features. | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Team skill decay was more likely to occur.","Consequently, when team members were less able to coordinate and exchange information, despite the task being highly interdependent, team skill decay was more likely to occur [ 98].","Causal Strength Justification: Direct causal language ('Consequently... was more likely to occur') links the cause (less able to coordinate/exchange information) to the effect (team skill decay more likely). | Relevance Justification: The excerpt discusses causal factors for team skill decay, which is a human performance effect. However, it does not connect this to any specific AI feature (non-deterministic, opacity, adaptive) as requested in the task. The cause is about team coordination and information exchange, not AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Machine Learning and Deep Learning Applications","","","Artificial Intelligence is beginning to appear in civil aviation, principally via Machine Learning and Deep Learning approaches [ 1] in a wide variety of applications, including, for example, flight operations and unmanned aerial vehicles [ 2], weather prediction [ 3], and numerous improvements in air traffic management [ 4].","This excerpt explicitly mentions AI features (Machine Learning and Deep Learning) and their applications in aviation, which are data-driven and non-deterministic in nature, aligning with the research focus on novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI autonomy levels in aviation","","","This paper briefly reviews the types of AI and ‘Intelligent Agents’ along with their associated levels of AI autonomy being considered for future aviation applications.","This mentions AI autonomy as a feature being considered for future applications, highlighting a key characteristic of AI systems that differs from conventional automation by involving varying degrees of independence.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI pushing boundaries of human-automation interaction","","","Since AI will inevitably push the boundaries of traditional human-automation interaction, there is a need to revisit Human Factors to meet the challenges of future human-AI interaction design.","This directly mentions AI's impact on traditional human-automation interaction, indicating novel characteristics that differ from conventional automation, specifically the need to address new challenges in interaction design.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Machine Learning vs Generative AI in safety-critical systems","","","This would rely on Machine Learning approaches rather than Generative AI models such as ChatGPT [5] and other Large Language Models, as the latter are currently ruled out by the EU Act on AI [6] for safety-critical systems due to the problem of hallucinations [7,8].","This excerpt discusses AI features by contrasting Machine Learning approaches with Generative AI models, highlighting regulatory restrictions due to hallucinations, which relates to AI capabilities and safety considerations in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Human AI Teaming with autonomy and advice-giving","","","would afford more human-machine interaction, lending more autonomy to the IA to take on tasks, as well as giving advice during challenging and time-critical flight upsets. Assistance by AI at such a level is known as Human AI Teaming (HAT: also Human Autonomy Teaming and Human Machine Teaming) [ 9].","This excerpt directly mentions AI (IA) features: lending more autonomy to take on tasks and giving advice during challenging situations, which are key AI capabilities in human-machine interaction, as specified in the research focus on novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Varying Levels of AI Autonomy","","","which is exploring six futuristic HAT use cases—two cockpit, two air traffic, and two airport—with varying levels of AI autonomy.","This directly mentions 'varying levels of AI autonomy,' which is a core AI capability distinguishing it from conventional automation, as it implies different degrees of independent decision-making and action by the AI system.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Rapid development pace and profound implications","","","However, AI is developing at a rapid pace and could have profound implications for aviation systems and human-machine interaction. It will be hard for a traditional approach of gaining experience to keep up with AI developments.","This excerpt directly mentions AI's rapid development pace and its potential profound implications for systems and interaction, which represents a key AI capability characteristic compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-assisted automation with human oversight","","","UC4—a digital assistant for remote tower operations, to alleviate the tower controller’s workload by carrying out repetitive tasks. The tower controller monitors the situation and intervenes if there is a deviation from normal (e.g., a go-around situation, or an aircraft that fails to vacate the runway). The controller is in charge, but the AI can take certain actions unless the controller vetoes them.","This represents an AI feature as it highlights the AI's capability to autonomously perform tasks (carrying out repetitive actions) within a human-in-the-loop control system, where the AI operates under human supervision and can be overridden, indicating a level of automation and interaction typical in advanced AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI direction with human oversight","","","The AI directs the pilot concerning instruments to focus on in order to resolve the emergency situation. Although the AI supports and directs the pilot, the pilot remains in charge throughout.","This excerpt describes an AI system that provides specific guidance to a human operator (pilot) in a high-risk scenario, highlighting its capability to support decision-making while maintaining human control, which is a key feature in human-AI interaction for safety-critical applications.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: AI-assisted rerouting with multi-factor consideration","","","UC2—a cockpit AI to help flight crew re-route an aircraft to a new airport destination due to deteriorating weather or airport closure, for example, taking into account a large number of factors (e.g., category of aircraft and runway length; fuel available and distance to airport; connections for passengers, etc.). The flight crew remain in charge, but communicate/negotiate with the AI to derive the optimal solution.","This excerpt illustrates an AI system that adapts to dynamic contexts (e.g., weather changes) by processing a large number of factors to assist in decision-making, demonstrating context-aware and adaptive behavior in a high-risk industry like transportation, with human-AI collaboration through communication and negotiation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Level of autonomy","","","depending on the level of autonomy of the AI under consideration for implementation into flight operations.","This directly references an AI system characteristic (level of autonomy) that influences operational guidance, distinguishing it from conventional automation by implying variable autonomy levels.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI and HAT system safety and operational effectiveness","","","The fundamental question arising for such future HAT concepts, and the principal focus of this paper, is how to ensure that such HAT systems will be safe and operationally effective if implemented, given that most existing Human Factors assurance approaches have not been designed or developed to deal with AI and HAT.","This excerpt directly addresses the challenge of ensuring safety and effectiveness for AI and HAT systems, which is a core concern for novel AI characteristics in high-risk industries, as it implies these systems have capabilities or behaviors that existing assurance approaches cannot handle.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Rapid AI development pace","","","However, AI is developing at a rapid pace and could have profound implications for aviation systems and human-machine interaction. It will be hard for a traditional approach of gaining experience to keep up with AI developments.","This excerpt directly mentions AI's rapid development pace and its profound implications for systems and interaction, which is a key AI capability distinguishing it from conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI error detection capability","","","the ability to detect AI errors [19]. This broader scope leads to a wider range of Human Factors requirements than a regulatory set.","This excerpt directly references 'AI errors', indicating a feature or characteristic of AI systems where errors can occur and need to be detected, which is relevant to AI capabilities in high-risk contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Level of autonomy in AI systems","","","Human-AI Teaming concepts of operation are both effective and safe. Aviation regulators such as the European Union Aviation Safety Agency (EASA), as well as regulators in other safety-critical domains such as Oil and Gas [ 16], are well aware of this predicament, and EASA in particular have issued advance guidance for aviation ‘Human-AI Teaming’ scenarios [ 17,18], depending on the level of autonomy of the AI under consideration for implementation into flight operations.","This excerpt directly mentions 'the level of autonomy of the AI,' which is a key AI feature distinguishing it from conventional automation, as it relates to the AI's decision-making independence and operational scope in safety-critical domains like aviation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: novel autonomy level","","","AI is likely to change this picture, however, as it can have a level of autonomy not seen in current aviation systems.","This directly describes a characteristic of AI systems (high autonomy) that distinguishes them from conventional automation in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: Level of autonomy and automation framework","","","the level of autonomy of the AI (the relative degree of control by human and AI elements). Given that the focus of this paper is aviation, the EASA guidance on levels of automation is used as the principal framework to consider levels of AI assistance and style of interaction and work-sharing between AI and human elements.","This directly addresses AI features related to autonomy levels and control mechanisms, which are key characteristics distinguishing AI from conventional automation, as it involves dynamic interaction and work-sharing between human and AI elements.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI surpassing human cognitive capabilities","","","Hence, the fundamental notion was one of computation, of ‘crunching the numbers’, which humans could do given enough time, but rarely error-free. As computing power grew, it soon surpassed what humans could do even given infinite time, and with the advent of deep learning [1], it could sometimes derive novel solutions to problems that we would never have thought of [21]. Thus, the goal of AI shifted from replicating human cognitive capabilities to surpassing them. Figure 2.","This represents an AI feature as it highlights AI's ability to exceed human limitations in computation and problem-solving, specifically through deep learning, which aligns with novel AI characteristics like non-deterministic decision-making and adaptability in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Novel solution generation and surpassing human cognition","","","with the advent of deep learning [ 1], it could sometimes derive novel solutions to problems that we would never have thought of [ 21]. Thus, the goal of AI shifted from replicating human cognitive capabilities to surpassing them.","It highlights a key AI feature where data-driven models (deep learning) produce unpredictable, innovative outcomes that exceed human capabilities, aligning with non-deterministic and advanced AI characteristics versus conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: deterministic and explainable automation","","","In ‘normal’ programming, e.g., for automation, a machine is programmed exactly what to do and, given stable inputs, that is exactly what it will do. The code may be complex, but is completely explainable, at least to a software analyst or data scientist.","This excerpt highlights key features of conventional automation (deterministic behavior and explainability) that contrast with novel AI characteristics such as non-deterministic decision-making and opacity, as specified in the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Opacity and non-deterministic errors in AI systems","","","Even if most people have little idea of how AI works, they know that it can do things for them, whether helping their research, finishing or finessing a report, or providing a diagnosis of their health symptoms. Though many have experienced firsthand the errors and biases of GenAI systems, they accept the trade-off between its accessibility and instant power to answer questions and requests, against the occasional inaccuracies or plausible fabrications called hallucinations [8].","This excerpt directly mentions opacity ('little idea of how AI works') and non-deterministic features ('errors and biases', 'inaccuracies or plausible fabrications called hallucinations'), which are key novel AI characteristics in high-risk industries, contrasting with more predictable conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: deterministic and explainable automation","","","In ‘normal’ programming, e.g., for automation, a machine is programmed exactly what to do and, given stable inputs, that is exactly what it will do. The code may be complex, but is completely explainable, at least to a software analyst or data scientist.","This excerpt highlights key features of conventional automation (deterministic behavior and explainability) that contrast with novel AI characteristics such as non-deterministic decision-making and opacity, as specified in the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: unpredictable failures and inaccuracies","","","Though many have experienced firsthand the errors and biases of GenAI systems, they accept the trade-off between its accessibility and instant power to answer questions and requests, against the occasional inaccuracies or plausible fabrications called hallucinations [8].","This excerpt discusses errors, biases, and hallucinations in GenAI systems, which relate to non-deterministic/data-driven decision-making by showing variability, uncertainty, and unpredictable failures in AI outputs.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: lack of explainability","","","Even if most people have little idea of how AI works, they know that it can do things for them, whether helping their research, finishing or finessing a report, or providing a diagnosis of their health symptoms.","This excerpt directly mentions that people have little idea of how AI works, which aligns with the opacity/explainability characteristic of AI systems, indicating a black-box nature that challenges transparency and trust.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI Capabilities and Applications","","","Narrow AI supports humans in their analysis, decision-making, and other tasks. In cases where tasks are well-specified and predictable, it can execute its functions without human intervention and with minimal supervision. Machine Learning can develop models and predictions that perhaps, given enough time, humans could do. Deep Learning is different and can come up with solutions humans likely would never think of. That said, Deep Learning typically uses artificial neural networks, themselves inspired by the way human brains work. Deep Learning is used for some of the more complex human cognitive processes we take for granted, such as natural language processing [ 30] and image recognition [ 31], and also for tackling complex problems such as finding cures for intractable diseases [ 32] (see [ 9,21,25,33–35] for a general summary of contemporary AI and HAT application areas).","This excerpt directly describes AI system features such as Narrow AI's support for human tasks, Machine Learning's model development, and Deep Learning's ability to generate novel solutions for complex problems, which are core AI capabilities relevant to the research focus on novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Anthropomorphization of AI","","","The problem is that particularly with advanced LLMs, it can feel to the user as if they are interacting with a person (known as anthropomorphising or personifying AI) [ 37,38]. This can matter in a safety-critical environment and is returned to at the end of Step 2.","This excerpt highlights a novel AI characteristic where advanced language models create a perception of human-like interaction, which differs from conventional automation by introducing psychological and trust factors that impact safety-critical applications.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: Variable output validity","","","Large Language Models (LLMs) like ChatGPT can respond to a human user to any query. Whether the response is a valid or correct one is another matter [36].","This directly relates to non-deterministic/data-driven decision-making, as it indicates that AI outputs can vary and may be incorrect or uncertain, reflecting unpredictability compared to deterministic automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Deep learning and NLP integration","","","They utilise deep learning neural networks trained on vast data sets and natural language processing to render interaction with human users smoother.","This highlights data-driven decision-making through neural networks and NLP, which are core to AI systems, enabling adaptive and context-aware responses unlike static automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Content generation capability","","","The hallmark of Generative AI (GenAI) tools, such as ChatGPT, Google Gemini, DALL-E, and DeepSeek-R1 are that they can create new content that is often indistinguishable from human-generated content.","This represents a novel AI characteristic where the system produces original outputs based on data-driven models, contrasting with conventional automation that typically follows fixed rules without creative generation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Independent reasoning and goal-setting capability","","","Artificial General Intelligence (AGI) does not yet exist but is predicted to emerge in the coming decades, e.g., by 2041 [ 20]. It would effectively comprise a mind capable of independent reasoning and could therefore in theory attain sentience. AGI would be able to set its own objective functions (goals), and its intelligence could grow very rapidly to","This excerpt directly describes novel AI characteristics (AGI) that differ from conventional automation, including independent reasoning, potential sentience, and the ability to set its own objective functions, which aligns with the research focus on AI system features in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Autonomous agency with intent and decision-making","","","The essential nature of HAT and in particular Intelligent Assistants (IAs), which differentiates it from conventional and contemporary automation, is the idea of an IA having a degree of autonomy/agency such that it can have intent, form goals and decisions, and execute such decisions, in collaboration with one or more human agents.","This directly addresses novel AI characteristics vs. conventional automation by highlighting autonomous agency, intent formation, goal setting, and decision execution—key features of advanced AI systems that go beyond traditional deterministic automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Increased autonomy in Intelligent Assistants","","","Given that Intelligent Assistants (IAs) are intended to interact and collaborate with humans in aviation contexts, there is clearly an increase in their autonomy compared to automation simply presenting information and warnings.","This directly contrasts novel AI characteristics (increased autonomy in IAs) with conventional automation (presenting information/warnings), highlighting a key feature of AI systems in human-AI interaction contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Opacity and lack of explainability in AI decision-making","","","First, the information or advice, or even executive action, can be based on calculations that are opaque to the end users (e.g., pilots), because the level of complexity and transparency of how AIs derive their answers means that no amount of theoretical training for pilots will enable them to follow the IA’s ‘reasoning’, unless an additional layer of ‘explainability’ is afforded to the pilot by the AI-based automation.","This directly describes the opacity/explainability characteristic from the research focus, highlighting black-box behavior where AI reasoning is not transparent to users, creating trust challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: shared control and autonomy mapping","","","control becomes to a greater or lesser degree shared between human and AI. It is therefore useful to map the degree of IA autonomy, and autonomy sharing between human and IA. Increasing levels of autonomy can be represented on a scale, and the most influential scale in aviation currently is that provided by the European Union Aviation Safety Agency (EASA).","This represents an AI feature as it describes the interdependence and shared control between humans and AI, which is a key aspect of human-AI interaction, particularly in high-risk industries like aviation, aligning with the research focus on novel AI characteristics such as adaptive behavior and control mechanisms.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Cooperative AI system with predefined task allocation and feedback","","","Cooperation Level 2A: cooperation is a process in which the AI-based system works to help the end user accomplish his or her own goal. The AI-based system works according to a predefined task-allocation pattern with informative feedback to the end user on the decisions and/or actions implementation.","This excerpt highlights an AI feature involving cooperation with users, predefined task patterns, and feedback mechanisms, which are key aspects of AI capabilities in human-AI interaction, as opposed to conventional automation that may lack such adaptive or interactive elements.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Collaborative AI System","","","collaboration is a process in which the end user and the AI-based system work together and jointly to achieve a predefined shared goal and solve a problem through a co-constructive approach. Collaboration implies the capability to share situation awareness and to readjust strategies and task allocation in real time. Communication is paramount to share valuable information needed to achieve the goal.","This excerpt highlights AI features such as joint problem-solving, shared situation awareness, and real-time adaptation, which are novel characteristics compared to conventional automation, emphasizing dynamic interaction and communication in human-AI collaboration.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: AI executive agent with human oversight and intervention","","","3A—AI executive agent–the AI is basically running the show, but there is human oversight, and the human can intervene (sometimes called management by exception)","This excerpt illustrates an AI feature where the AI takes a leading role in control, but with human oversight and the ability for human intervention, representing a nuanced level of automation in high-risk settings.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: Full AI control without human intervention","","","3B—the AI is running everything, and the human cannot intervene. It has been argued that AI innovation, for all its benefits, is essentially ‘just more automation’ supporting the human operator [ 37].","This excerpt presents an AI feature of complete autonomous control without human intervention, and it references the debate on whether AI innovation is merely an extension of automation, relevant to distinguishing AI from conventional systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Collaborative autonomous agent with initiative and negotiation","","","2B—collaborative agent–an autonomous agent that works with human colleagues, but which can take initiative and execute tasks, as well as being capable of negotiating with its human counterparts","This excerpt highlights advanced AI capabilities such as autonomy, initiative-taking, task execution, and negotiation, which are novel features distinguishing AI from conventional automation in human-AI interaction contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: AI opacity/black-box behavior","","","there remains the opacity issue, wherein the AI is often akin to a ‘black box’, and as such may surprise, confound, or confuse the end users, because most end users will not be able to follow the computations underpinning AI advice.","This directly addresses the research focus on opacity/lack of explainability, describing AI as a 'black box' that confuses users who cannot follow its computations.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Operational explainability challenges","","","Furthermore, new issues such as operational explainability of AI systems may require completely new approaches or design requirements [34,35].","This relates to the opacity/explainability research focus, specifically highlighting 'operational explainability' as a new issue requiring novel approaches.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Real-time context-aware adaptation","","","It is based on Standard Operating Procedures (SOPs) for emergency events (e.g., lightning strike), coupled with sensors related to dynamic flight parameters (which together define for the IA what is happening and what needs to be done), compared to where the pilot is looking (via eye tracking), and what the pilot is doing (all in real time).","This represents a context-aware/adaptive AI feature as it involves dynamic response to emergency events and real-time environment monitoring (via sensors and eye tracking), enabling the IA to adapt its understanding and actions based on changing conditions and pilot behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-based sensor data analysis for startle detection","","","The FOCUS (Flight Operational Companion for Unexpected Situations) IA supports the pilot firstly by detecting startle via various psycho-physiological sensors (breathing, heart rate, skin conductance etc.) analysed by a trained AI, using an AI technique known as Extreme Gradient Boosting (see [ 1]).","This directly describes an AI capability (trained AI using Extreme Gradient Boosting) performing a specific function (analyzing psycho-physiological sensor data for startle detection), which is a novel AI characteristic compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: feedback | Feature: Context-aware gaze analysis with visual feedback","","","Second, it analyses where the pilot is looking compared to where the pilot should be looking given the situation, and if different, FOCUS highlights the relevant parameter on the cockpit displays (see Figure 4).","This describes an AI capability to analyze context (where pilot is looking vs. should be looking) and provide adaptive feedback (highlighting parameters), representing context-aware behavior and feedback mechanisms characteristic of novel AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-based assistant with supervised learning","","","An AI-based assistant has been developed called COMBI (using a data-based supervised learning AI ML approach), which can identify up to three airports within reach.","This represents an AI feature as it explicitly mentions an AI-based assistant using a data-based supervised learning approach, which is a key characteristic of AI systems involving machine learning for decision-making tasks.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: operational explainability","","","can also query COMBI’s selection according to a number of parameters; the IA therefore has a degree of operational explainability.","This directly addresses the research focus on opacity/explainability by explicitly stating the IA system has 'a degree of operational explainability,' which contrasts with black-box behavior and helps address trust/regulation challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-assisted situation awareness and stabilization","","","FOCUS is there to help the pilot regain situation awareness and stabilise the aircraft. Once the pilot feels back in control, they can cancel the automation, and carry on flying without the IA.","This represents an AI capability as it describes an AI system (FOCUS) providing support to the pilot in a high-risk context (aviation), enabling recovery from a startle event and stabilization, with the pilot retaining ultimate control to cancel automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI assistant with supervised learning","","","An AI-based assistant has been developed called COMBI (using a data-based supervised learning AI ML approach), which can identify up to three airports within reach.","This excerpt explicitly mentions an AI-based assistant and its data-driven approach, highlighting a novel AI capability for decision-making in aviation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: operational explainability","","","the IA therefore has a degree of operational explainability.","This directly addresses the explainability characteristic of AI systems, indicating that the IA's decision-making process is not entirely opaque and can be queried or understood to some extent, which is a key feature distinguishing novel AI from conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: operational explainability","","","can also query COMBI’s selection according to a number of parameters; the IA therefore has a degree of operational explainability.","This directly addresses the opacity/explainability characteristic by explicitly stating the IA has 'a degree of operational explainability,' which contrasts with black-box behavior and supports transparency in decision-making.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-based assistant with supervised learning","","","An AI-based assistant has been developed called COMBI (using a data-based supervised learning AI ML approach), which can identify up to three airports within reach.","This represents an AI feature as it explicitly mentions an AI-based assistant using a data-based supervised learning approach, indicating machine learning capabilities for decision-making in a specific context.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: on-demand explainability","","","ISA also provides explainability on-demand. For example, in Figure 6, ISA signals that the take-off ‘window’ for the KLM is now too small due to the BAW’s increased speed, and so the BAW will land prior to the KLM take-off and takes position ‘2’ in the strip (as indicated by the upward arrow in the bottom left hand corner of Figure 6).","This directly addresses the opacity/explainability characteristic by showing the AI system provides transparency into its reasoning, a key feature distinguishing novel AI from conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: on-demand explainability","","","ISA also provides explainability on-demand.","This directly addresses the opacity/explainability characteristic by indicating the AI system can provide explanations for its decisions, which is a key feature for trust and regulation in high-risk applications like air traffic control.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI-associated human-machine interaction","","","Arguably, these developments over time have prepared Human Factors for the next major step change in human-machine interaction associated with AI, HAT, and IAs.","This directly references AI as part of a significant advancement in human-machine interaction, indicating its role in evolving systems beyond conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Real-time assistance with forecast updates","","","The real-time assistance provided by ISA ensures timely and accurate forecast updates, allowing Tower Air Traffic Controllers (ATCOs) to manage the traffic flow more efficiently, with more ‘look-ahead time’ than currently, as ISA can see further ‘upstream’.","This describes an AI system (ISA) providing real-time assistance with accurate forecasting, enabling more efficient traffic management through enhanced look-ahead time and upstream visibility, which aligns with AI capabilities in dynamic decision support.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Improved decision-making and operational efficiency","","","The benefits are improved decision-making, enhanced runway utilisation, increased operational efficiency, and a safer and more streamlined air traffic flow that reduces the need for ‘go-arounds’.","This excerpt highlights benefits such as improved decision-making, enhanced runway utilisation, and increased operational efficiency resulting from AI assistance, demonstrating AI's role in optimizing complex systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Real-time assistance with forecast updates","","","The real-time assistance provided by ISA ensures timely and accurate forecast updates, allowing Tower Air Traffic Controllers (ATCOs) to manage the traffic flow more efficiently,","This describes an AI system (ISA) providing real-time assistance and accurate forecast updates, which are AI capabilities that enhance decision-making and operational efficiency in air traffic control.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Increased look-ahead time and upstream visibility","","","with more ‘look-ahead time’ than currently, as ISA can see further ‘upstream’. The benefits are improved decision-making, enhanced runway utilisation, increased operational efficiency, and a safer and more streamlined air traffic flow that reduces the need for ‘go-arounds’.","This highlights AI capabilities of extended perception (seeing further upstream) and predictive assistance (more look-ahead time), which result in tangible benefits such as improved decision-making, efficiency, and safety in air traffic management.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI pattern recognition and data interpretation capabilities","","","Notably at the time, most of the cognitive ‘heavy lifting’, including pattern recognition and interpretation of ‘noisy’ data, was left to the human. A more recent analysis [43] showed that the distinction between people and machine’s relative capabilities has shifted, or at least blurred, and is likely to blur further given ongoing advances in AI (both ML and GenAI).","This excerpt directly discusses AI capabilities (pattern recognition, interpretation of noisy data) and their advancement relative to humans, which is a core aspect of novel AI characteristics in human-AI interaction, as it highlights shifting roles and capabilities due to AI progress.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Intelligent Assistants in Human-AI Teaming","","","If AI-based systems begin to play a role in either cockpit or ATC workplaces, or significantly support flight crew/ATCOs, potential impacts on CRM/TRM need to be understood and safeguards put in place. This will be particularly the case with Intelligent Assistants at EASA category 2A or 2B, where the flight crew and the IA are sharing tasks and even negotiating over how best to achieve goals.","This excerpt mentions AI-based systems and Intelligent Assistants that share tasks and negotiate with flight crews, indicating adaptive and context-aware behavior in human-AI teaming, which aligns with novel AI characteristics like context-aware/adaptive behavior and interactive capabilities in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Operational Explainability (OpXAI)","","","operational explainability (OpXAI) to help the end user understand the AI’s decision-making processes [ 52]. Operational explainability is poised to become a major new area for Human Factors research.","This directly addresses the opacity/explainability characteristic by mentioning a tool (OpXAI) to make AI's decision-making processes understandable to end users, which is a key feature for building trust and meeting regulatory challenges in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: adaptive system with knowledge-based planning","","","Joint Cognitive Systems (JCS) [53] and Cognitive Systems Engineering (CSE) [54] introduced the concept of a cognitive system as an adaptive system that functions using knowledge about itself and the environment in the planning and modification of actions.","This represents an AI feature as it highlights context-aware/adaptive behavior, where the system dynamically responds to its environment and uses knowledge to plan and modify actions, aligning with novel AI characteristics like real-time learning and variability handling.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: Automation precedence and human intervention role","","","automation is often given precedence, while humans are left to do the things that automation cannot do, including stepping in when the automation can no longer deal with the current conditions, or simply fails.","This represents a fundamental characteristic of automation systems, highlighting their limitations and the resulting human role in managing failures or unhandled conditions, which is a core aspect of human-automation interaction in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: automation | Feature: Levels of Automation and Adaptive Automation","","","4.1.7. Levels of Automation and Adaptive Automation Sheridan was one of the key proponents of ‘levels of automation’, which can be seen as an alternative or complement to Fitts’ List. His 10 levels of automation [ 58,59] run from fully manual to fully automated:","This represents an AI feature as it describes automation levels and adaptive automation, which are key characteristics in human-AI interaction, particularly in high-risk industries where dynamic control mechanisms are crucial.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: adaptive automation based on human state","","","Sheridan’s work also contributed to the notion of adaptive automation [60], wherein, for example, the automation could step in when (or ideally, before) the human became overloaded in a work situation.","This excerpt directly mentions 'adaptive automation' and describes its behavior of stepping in when the human is overloaded, which exemplifies context-aware/adaptive AI characteristics involving dynamic response to environmental or human states, a key novel feature compared to conventional static automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Real-time data-driven adaptive behavior","","","Adaptive automation also raises ethical issues in terms of data protection, e.g., where AI components such as neural networks use real-time human performance data (EEG, heart rate, galvanic skin response, etc.) as inputs to determine when to take over.","This excerpt describes AI components (neural networks) that adaptively respond to dynamic real-time human performance data inputs to determine when to take over, which aligns with the context-aware/adaptive behavior characteristic of novel AI systems versus conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Black-box nature of AI systems","","","There is a very real danger that AI systems, which tend to be ‘black boxes’, can undermine the human crew’s situation awareness, both in terms of what is going on, and of what the AI is doing or attempting to do.","This directly mentions the 'black box' characteristic of AI systems, which aligns with the opacity/explainability category in the research focus, highlighting lack of transparency and potential trust/regulation challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Operational Explainability (OpXAI)","","","Additionally, there will need to be operational explainability (OpXAI), so that the human crews can determine (i.e., make sense of) why a course of action has been recommended (or taken) by the AI.","This directly addresses the opacity/explainability characteristic from the research focus, specifically the need for explainability to overcome black-box behavior and trust/regulation challenges, as it enables humans to make sense of AI decisions.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: testing AI explanations and anomalies","","","In relation to AI, sense-making helps to test the plausibility of an AI’s explanations as well as anomalous outputs or event characteristics.","This represents an AI feature related to opacity/explainability because it focuses on testing the plausibility of AI explanations and handling anomalous outputs, which are key aspects of addressing black-box behavior and trust challenges in AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: lack of explainability in AI outputs","","","However, the perception of what is happening (and going to happen) to an object may be insufficient when the AI is processing its inputs; the human may need to understand what is behind the AI output.","This represents an AI feature related to opacity/explainability because it explicitly mentions the human need to understand what is behind the AI output, indicating challenges in transparency and interpretability of AI decision-making.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: lack of explainability","","","an inability of the AI to explain its recommendations, could well be a recipe for disaster.","This directly mentions the AI's inability to explain its recommendations, which aligns with the opacity/explainability characteristic of novel AI systems, indicating a black-box behavior that poses trust and safety challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Real-time optimization and fluid rule adaptation","","","rules (e.g., Standard Operating Procedures [SOPs] in cockpits) may become more fluid as AI-based support systems find ever-new ways of optimising operations in real time.","This describes AI systems that adapt and optimize operations dynamically in real time, a key characteristic of context-aware/adaptive behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI impact on system barriers","","","What is interesting is that AI could in theory affect all these layers, either increasing or decreasing the size and quantity of the holes. It could also reduce the independence between each barrier, so that in reality, a system has fewer barriers before an accident occurs.","This excerpt describes AI's potential to dynamically alter system characteristics (barrier layers and independence), which relates to AI capabilities in high-risk industries, though it does not explicitly mention non-deterministic, opacity, or context-adaptive features.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: AI complexity and explainability limitations","","","However, the problem with AI is that its complexity may be unfathomable for humans, at least in reasonable and normal operational timescales. This suggests a need for display approaches that make the complex system’s workings and output relatable, backed up by explainability function that can at least approximate what the AI has done and why. Since AI operational explainability can probably never be completely trustworthy (it is an approximation, and few will be capable of understanding what goes on ‘under the hood’ of an AI),","This directly addresses the opacity/explainability characteristic of AI systems, describing AI's 'unfathomable' complexity for humans, the need for explainability functions to 'approximate' AI behavior, and the inherent untrustworthiness of such explanations due to their approximate nature and human inability to understand what happens 'under the hood' of AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: unpredictability and intent opacity","","","e.g., some AI-based systems may not be predictable; reciprocal knowledge of the others’ intent may prove difficult to achieve in practice.","This directly mentions AI-based systems being unpredictable and having difficulty with reciprocal knowledge of intent, aligning with non-deterministic and opacity characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: AI-AI and Human-AI-AI-Human interfaces","","","Such a layered model also leads to the question of how different AIs will interface with one another. We already talk of Human-AI Teaming, but there will also be Human-AI-AI-Human and Human-AI-Human-AI variants before long,","This excerpt directly discusses AI features related to interfaces: how different AIs will interface with one another, and mentions Human-AI Teaming and more complex variants (Human-AI-AI-Human, Human-AI-Human-AI), which are novel AI characteristics in interaction and teaming contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI integration into safety layers","","","It would be useful to consider how AI could affect the Swiss Cheese layers differentially, e.g., use of LLMs at the organisational and preconditions layers, and AI-based tools at the unsafe acts and defences layers.","This excerpt explicitly mentions AI (LLMs and AI-based tools) and their potential application at specific layers (organisational, preconditions, unsafe acts, defences), representing an AI capability feature related to system integration and safety modeling.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Executive AI without human oversight","","","especially as those systems become more advanced and even executive (i.e., not requiring human oversight). Additionally, as the AI is seen more as an ‘agent’, consideration must be given to the ways in which it, too, can fail.","This excerpt describes AI systems that become 'executive' and do not require human oversight, representing a novel AI capability compared to conventional automation, which typically involves human supervision.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Opacity and explainability requiring data forensics","","","The scenario in which ‘data forensics’ is required to understand why an AI suggested something that contributed to an accident, is probably not far in the future.","It directly addresses the opacity/explainability of AI systems by mentioning the need for 'data forensics' to understand AI decisions, which relates to black-box behavior and trust/regulation challenges in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI delegation and executive role in safety","","","the introduction of AI into operational aviation systems could aid or degrade safety culture in aviation. In particular, the concern is that human personnel may delegate some of their safety responsibility to the AI, especially if the AI is taking more of an executive role.","This excerpt discusses AI's role in operational systems, highlighting its capability to take on executive responsibilities and affect human safety delegation, which relates to AI's autonomous decision-making and integration features.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: adaptation | Feature: instant availability and adaptability","","","However, AI could also act as a diverse and potentially more comprehensive ‘mental model’ backup system for the pilots, one that is instantly available and adaptable when a sudden unexpected situation occurs.","This excerpt highlights AI's ability to adapt dynamically to unexpected situations, which is a key characteristic of context-aware/adaptive AI systems, contrasting with slower human re-adaptation times.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: contextual explainability","","","Probably key for Human-AI Teamwork will be trust and closed-loop communication. The latter will likely entail short, succinct, and contextual explainability provided by the AI, whether in procedural or natural language, and/or visually via displays.","This directly addresses the opacity/explainability characteristic by describing how AI provides 'short, succinct, and contextual explainability' to overcome black-box limitations and build trust in human-AI interaction.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Opacity/lack of explainability","","","cross-checking AI is likely to be more complex, as the way the AI works will itself be more complex and sometimes not open to scrutiny (either non-explainable or unfathomable for humans).","This directly describes AI's black-box nature and lack of transparency, which are key novel characteristics compared to conventional automation, aligning with the research focus on opacity/explainability.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Explainability and State Awareness","","","Ways to show such ‘alternates’ therefore need to be considered, possibly including the trade-offs the AI has made, or data it has ignored as outliers or irrelevant. Similarly, knowing the state of the AI will also be important; not simply whether it is ‘on’ or ‘off’, but its confidence level given the situation at hand compared to the data it was trained on.","This excerpt highlights opacity/explainability by mentioning the need to show AI trade-offs and ignored data, and AI capabilities by discussing the AI's state and confidence level, which are novel features compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: AI failure modes and aberrant behavior","","","The problem is that we are missing two inputs. The first is a model of how the AI can fail, or exhibit aberrant behaviour, or suggest inaccurate/biased resolutions or advice. We already know some of the answers, in terms of hallucinations, edge cases, corner cases,","This excerpt directly discusses AI-specific failure characteristics such as hallucinations, edge cases, and corner cases, which are novel compared to conventional automation and relate to non-deterministic behavior and unpredictable outputs.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Need to revisit automation bias","","","As with several other major study areas of Human Factors, the area of automation bias (especially complacency) needs to be revisited, for several reasons.","This represents an AI feature as it emphasizes the ongoing need to study and address automation bias and complacency in AI systems, which is crucial for improving safety and effectiveness in high-risk applications, aligning with general AI capabilities.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Automation bias and complacency","","","One of the more worrying biases was the withdrawal of attention from cross-checking the automation and considering contradictory evidence, summarised as ‘looking but not seeing’. The effects such as complacency (not checking) and automation bias (over-trust) are not easy to fix, e.g., via training, and are apparently prevalent in both experienced or naïve (i.e., new) users [76].","This represents an AI feature as it addresses biases such as automation bias and complacency, which are key challenges in AI systems for high-risk industries, impacting trust and reliability in human-AI interaction, relevant to general AI capabilities and feedback mechanisms.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Automation reliability and state awareness","","","He defined four (non-exclusive) categories: use, misuse (over-reliance), disuse (disengagement), and abuse (poor allocation decisions between human and machine). He found the primary factors influencing these end states to be trust, mental workload, risk, automation reliability and consistency, and knowing the state of the automation.","This represents an AI feature as it highlights factors such as automation reliability, consistency, and state knowledge, which are critical for AI systems in high-risk industries to ensure safe and effective human-AI interaction, aligning with general AI capabilities and feedback mechanisms.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Context awareness and goal awareness in HACO tenets","","","context awareness, goal awareness, effective communication, pro-activeness, predictability, and observability.","This excerpt directly lists 'context awareness' and 'goal awareness' as core tenets of the HACO concept for Human-AI Teaming, which are specific AI features related to adaptive and context-aware behavior in human-AI interaction systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: Data-driven and context-dependent failure modes","","","What we need is a way to determine when these and other AI ‘failure modes’ are likely, given the type of ML/LLM being used, its data, and the operational context it is being applied to.","This excerpt directly mentions AI features: 'type of ML/LLM being used' (indicating model-dependent decisions), 'its data' (data-driven aspect), and 'operational context' (context-aware behavior). It shows AI failure modes vary with these factors, contrasting with more predictable conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: Unpredictable complacency in human-AI design","","","We already have ‘complacency’, but this is a catch-all term, and knowing when it is likely or not, as a function of the human-AI system design, is unpredictable.","This excerpt mentions AI features through 'human-AI system design' and 'unpredictable,' indicating that AI systems can lead to variable and uncertain human behaviors (like complacency) depending on their design, which contrasts with more controlled conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: Human-AI interaction failure modes","","","The second unknown, or ‘barely known’, relates to ‘human+AI’ failure modes, i.e., the likely failure types when people are using and interacting with various types of AI tools.","This excerpt describes AI features related to human interaction: 'people are using and interacting with various types of AI tools.' It highlights how AI systems involve dynamic interfaces and feedback mechanisms between humans and AI, which is a novel characteristic compared to static conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Context-aware interpretation of data","","","Context awareness is possibly a more useful (and less anthropomorphic) label than situation awareness when applied to intelligent agents. It seeks to establish what the AI was responding to within its inputs and data sets. However, this is not always simply the superficial data in the environment (e.g., weather patterns that might affect flight route), but the way the AI will use statistical and other algorithms to interpret such data according to the task and goals at hand.","This excerpt directly addresses context-aware/adaptive behavior by describing how AI systems respond to inputs and data sets, using algorithms to interpret environmental data dynamically, which aligns with the research focus on dynamic environment response and variability.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Contextual awareness with evolving goals","","","Similarly, as for contextual awareness, a design challenge is how to ensure the human is aware of the goals the AI is working towards as they change and evolve.","This directly addresses context-aware/adaptive behavior, as it involves AI systems dynamically responding to changes (evolving goals) in the environment, which is a novel characteristic compared to static conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Dynamic goal shifting and safety conflicts","","","and where goals may dynamically shift during a scenario (e.g., for an emergency, as conditions worsen or become more stable). It can also be important where there are a mixture of safety and other goals, some of which may conflict with safety.","This excerpt describes an AI feature involving adaptive behavior, as it discusses dynamic goal shifts in response to changing conditions (e.g., emergencies) and potential conflicts between safety and other goals, which relates to context-aware/adaptive characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Goal awareness and context awareness","","","Goal awareness (a precursor to goal alignment) is a higher-level attribute, related to context awareness, and ensures that human and AI goals are aligned. This becomes important in work arrangement scenarios where the AI is able to modify the goal hierarchy,","This excerpt describes an AI feature involving context awareness and adaptive behavior, as it highlights the AI's ability to align goals and modify goal hierarchies dynamically in response to work scenarios, which relates to context-aware/adaptive characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Explainability requirement for AI tasks","","","Communication for AI tools or systems below EASA’s category 2B may not need to be that advanced, though for 1B and 2A there may require explainability if the AI’s task or output is sufficiently complex.","This represents an AI feature related to opacity/explainability, as it explicitly mentions the need for explainability in AI systems when tasks or outputs are complex, which is a key characteristic distinguishing AI from conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Context-aware communication for human-AI collaboration","","","For human AI collaboration at the 2B level, there will likely not only need to be communication, but a degree of rapport, so that the AI is communicating in terms and contexts familiar to the human.","This represents an AI feature related to context-aware/adaptive behavior, as it highlights the AI's ability to communicate in terms and contexts familiar to humans, indicating adaptation to human interaction contexts, which is a novel characteristic compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: adaptation | Feature: Adaptive Automation Update","","","This is effectively an update of the Adaptive Automation concept. The three HAIKU use cases are close to this notion of sliding or rather ‘flexible’ autonomy.","This represents an AI feature as it discusses the evolution of automation towards adaptive and flexible autonomy, which aligns with context-aware/adaptive behavior in AI systems, enabling dynamic response and environment adaptation in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: Sliding autonomy","","","However, in contrast to the distinctive categorisations of EASA the HACO authors [ 89] suggest the concept of ‘sliding autonomy’, wherein the human (or the system) determines the level of autonomy.","This represents a control mechanism where autonomy levels can be adjusted dynamically, which is a feature in advanced AI systems for managing human-AI interaction, differing from fixed automation levels.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Proactive autonomy and agency","","","Proactiveness links again to EASA’s 2B and above categorisations, whereby the AI can initiate its own tasks and shift or re-prioritise goals, giving the AI a degree of autonomy and agency (since it can act under its own initiative).","This describes an AI capability to act on its own initiative, which is a novel characteristic compared to conventional automation that typically follows predefined instructions without such autonomy.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Observability and Explainability","","","Observability refers to the transparency of the progress of the AI agent when resolving a problem, and can be linked to explainability, though in practice the AI’s workings might be routinely monitored via a dashboard or other visualisation rather than a stream of textual explanations, with the user able to pose questions as needed.","This excerpt directly discusses the transparency and explainability of AI systems, which are key features in human-AI interaction, particularly in high-risk industries where understanding AI behavior is critical for trust and regulation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Types and levels of AI autonomy","","","Having reviewed types of AI, levels of AI autonomy, and examples of prototype Intelligent Agents, as well as Human Factors themes relevant to AI-based systems in aviation, this section (Step 3) outlines the development of a preliminary set of Human Factors requirements for future aviation HAT systems.","This represents an AI feature as it directly discusses types of AI, levels of AI autonomy, and prototype Intelligent Agents, which are key characteristics of AI systems in human-AI interaction and high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI effectiveness in task execution","","","Overall, therefore, preliminary indications are that aviation needs neither anthropomorphic (personified) nor emotional AI. What matters is the effectiveness of the AI in the execution of its tasks.","This represents an AI feature as it explicitly mentions AI characteristics (anthropomorphic and emotional AI) and focuses on AI effectiveness, which relates to AI capabilities and performance in high-risk industries like aviation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Adaptability to team tempo and state","","","Adaptability to the tempo and state of the team functioning, as in the controllers’ approach to using ISA in UC4. With respect to Adaptability , one of the hallmarks of an effective team, it may be useful to resurrect High Reliability Organisation (HRO) theory [ 90]. All five of the pillars of HROs—preoccupation with failure, commitment to resilience, reluctance to simplify explanations, deference to expertise, and sensitivity to operations—could well apply to Human-AI Teams.","This directly describes an AI system's ability to adapt to dynamic team conditions and operational tempo, which aligns with context-aware/adaptive behavior characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI agency and decision authority","","","Anthropomorphism relates to the identity we assign an AI system, and hence the degree of agency we accord it. The more we ‘personify’ an IA, the more the danger of delegating responsibility to it, or of surrendering authority to its logic and databases, or of basically second-guessing ourselves rather than the AI.","This addresses AI's capability to be assigned agency and authority, which relates to its decision-making role in human-AI interaction.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: AI situation representation and context assessment","","","HRO theory leans towards collective mindfulness of the team, and here is where it is necessary to consider and contrast how humans think ‘in the moment’, compared to how an AI might build up its own situation representation or context assessment.","This describes AI's capability to dynamically assess context and build situation representations, which is a core aspect of context-aware/adaptive behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Human Factors integration in HAT system life cycle","","","Accordingly, in this section, the issue of how Human Factors can inform the different stages in the design, development, and deployment life cycle of new HAT systems is considered first.","This highlights the integration of Human Factors into AI system development, which is a key feature of novel AI systems that require human-AI interaction, unlike conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI forms for human interaction in safety-critical systems","","","It must accommodate the various forms of AI that humans may need to interact with in safety-critical systems (note—this currently excludes LLMs) both now and in the medium future, including ML and Intelligent Agents or Assistants (EASA’s Categories 1A through to 3A).","This directly mentions AI capabilities (ML and Intelligent Agents/Assistants) that humans interact with in safety-critical contexts, which is a core aspect of novel AI characteristics compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: New requirements for HAT and autonomy-sharing","","","Third, a sample of the new requirements themselves, contextualised in HAT and autonomy-sharing terms in such a way that they can be applied meaningfully by HAT system developers, is presented.","This mentions new requirements for HAT and autonomy-sharing, which are specific to AI systems involving human-AI teaming, distinguishing them from traditional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI forms and categories for safety-critical interaction","","","It must accommodate the various forms of AI that humans may need to interact with in safety-critical systems (note—this currently excludes LLMs) both now and in the medium future, including ML and Intelligent Agents or Assistants (EASA’s Categories 1A through to 3A).","This excerpt directly mentions specific AI capabilities (ML and Intelligent Agents/Assistants) and their categorization (EASA's Categories 1A through 3A) relevant to human-AI interaction in safety-critical contexts, addressing the research focus on novel AI characteristics versus conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Human-AI system deployment considerations","","","This is the time to consider in earnest the ramifications of entering a human-AI system into an operational organisation, with requirements for competencies and training of end users, as well as socio-technical considerations such as staffing, user acceptance, ethics, and well-being.","This excerpt explicitly mentions 'human-AI system' and discusses deployment considerations specific to AI systems in operational settings, including training requirements and socio-technical factors like ethics and user acceptance.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI Architecture and Interaction Prototyping","","","TRLs 3-4 add a lot more detail, fleshing out the concept’s architecture, and gaining a picture of what the AI will look and feel like to interact with, via early prototypes and walk-throughs of human-AI interaction scenarios.","This excerpt represents an AI feature by describing the process of fleshing out AI architecture and interaction experiences, which are fundamental to AI system development and capability enhancement.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: Human-AI Interaction Modes and Usage Context","","","primary human-AI interaction modes, e.g., ‘conventional’ (keyboard, mouse, touchscreen, etc.) or more advanced (speech, gesture recognition) can be made, as well as whether the AI will be used by a single person or a team.","This excerpt represents an AI feature by detailing the interface types (conventional and advanced) and the context of AI usage (single person or team), which are key aspects of AI system design and interaction capabilities.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Operational explainability and AI assessment transparency","","","Sense-Making—this is where shared situation awareness, operational explainability, and human-AI interaction sit, and as such has the largest number of requirements. Arguably, this area could be entitled (shared) situation awareness, but sense-making includes not only what is happening and going to happen, but why it is happening, and why the AI makes certain assessments and predictions.","This directly addresses the opacity/explainability characteristic by mentioning 'operational explainability' and the need to understand 'why the AI makes certain assessments and predictions,' which relates to making AI's internal reasoning more visible and interpretable.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI Aberrant Behavior and Human Recovery","","","Errors and Failure Management—the requirements here focus on identification of AI ‘aberrant behaviour’ and the subsequent ability of human end users to detect, correct, or ‘step in’ to recover the system safely.","It highlights AI's potential for unpredictable failures (aberrant behavior) and the need for human oversight, aligning with novel AI characteristics like non-deterministic outputs and adaptive response challenges in high-risk contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI autonomy and explainability","","","trustworthiness, AI autonomy, operational explainability, speech recognition and human-AI dialogue, Just Culture, etc.","This excerpt directly mentions AI autonomy and operational explainability, which are novel AI characteristics relevant to opacity/explainability and adaptive behavior in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: non_deterministic | Feature: AI aberrant behaviour and human recovery","","","Errors and Failure Management—the requirements here focus on identification of AI ‘aberrant behaviour’ and the subsequent ability of human end users to detect, correct, or ‘step in’ to recover the system safely.","This represents an AI feature because it explicitly mentions AI 'aberrant behaviour', which implies non-deterministic or unpredictable failures characteristic of AI systems, and the requirement for human end users to manage such failures, distinguishing it from conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: New training requirements for AI systems","","","The requirements for preparing end users to work with and manage AI-based systems will not be ‘business as usual’; new training approaches and practices will almost certainly be required (e.g., pilots and controllers who participated in UC1, 2 and 4 simulations stated they would want specialised training).","This represents an AI feature because it specifies that AI-based systems require new training approaches, implying unique AI characteristics like non-deterministic decision-making or lack of explainability that differ from conventional automation, as evidenced by the need for specialized training for pilots and controllers.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Human mental model and oversight of AI tasks and goals","","","8. Does the human have a mental model of how the AI performs each task? Joint Cognitive Systems/ Cognitive Systems Engineering/ Situation Awareness HACO–goal awareness and predictability 9. Can the human monitor and adjust the AI’s goal formulation/prioritisation?","This text directly addresses key AI system features: the need for humans to understand how AI performs tasks (relates to opacity/explainability) and the ability to monitor and adjust AI goal formulation (relates to control mechanisms and human-AI interaction).","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: human override capability","","","6. In the case of AI control, can the human reject a task?","This relates to AI control mechanisms and human oversight, addressing trust and safety in AI-driven systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: task-switching control mechanisms","","","5. If there is task-switching, is it controlled by the human, by the AI, or a mixture of both?","This highlights AI capabilities in task management and control sharing, relevant to adaptive behavior and human-AI interaction in dynamic systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: unexpected control transfer information","","","7. If the AI cedes control to the human unexpectedly, is there enough information for the human to safely take control?","This addresses opacity/explainability and interface design, as it concerns whether AI provides sufficient information for safe human intervention, a key challenge in AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: adaptation | Feature: dynamic autonomy level changes","","","4. If the level of autonomy changes dynamically, has it been determined when/why it changes?","This directly addresses context-aware/adaptive behavior where autonomy levels change dynamically in response to conditions, a novel AI characteristic compared to static conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Human-AI system usability and safety","","","The aim of the HAT HF requirements questions is to help the designers, developers and Human Factors specialists realise and deliver a highly usable and safe Human-AI system, one that end users can use effectively and will want to use (avoiding misuse, disuse, and abuse, as mentioned earlier).","This excerpt directly mentions a 'Human-AI system' and discusses its design for usability and safety, which relates to AI capabilities in human interaction contexts, as specified in the priority search for general AI capabilities.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Explainability of AI goals and strategy","","","Can the AI explain both its current goal and longer-term strategy if it has one? Sense-Making Big 5 shared mental model","This relates to opacity/explainability by questioning whether AI systems can articulate their goals and strategies, which helps make their internal reasoning more transparent and understandable to users.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Explainability in operational terms","","","Does explainability detail how the advice was derived, in end user (operational) terms? HRO theory: sensitivity to operations, JCS, Sense-Making, SA","This directly addresses the opacity/explainability characteristic by questioning whether AI systems can detail how advice is derived in user-understandable terms, which is a key aspect of making black-box behavior transparent.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Transparency in data usage and omission","","","Can the human view both data that were used and data that were ignored by the AI, e.g., anomalies or outliers? HCD/HCA Ironies of Automation/AI","This addresses opacity/explainability by exploring whether AI systems allow humans to see both used and ignored data (e.g., anomalies), which is key to understanding AI decision-making and mitigating black-box effects.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Multi-level explainability for trust judgment","","","Is explainability multi-levelled, based on different levels of abstraction, including context of the AI’s goals and ‘reasoning’ available to the end user, any historical perspective underlying the AI’s reasoning and key data sources it has accessed, so the user can fully judge its appropriateness to the situation, and progressively determine how far (or when) to trust the AI? Complex Systems/KBB","This addresses opacity/explainability by exploring whether AI systems provide multi-level explanations (goals, reasoning, history, data sources) to enable users to judge appropriateness and build trust, which is crucial for overcoming black-box limitations.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: Non-AI displays for verification","","","42. Are there sufficient ‘unfiltered’ (non-AI) displays of critical functions to allow the human to verify independently true system status?","Refers to 'unfiltered (non-AI) displays' as a means for humans to verify system status independently, implying AI systems may filter or process information in ways that require separate verification mechanisms.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI failure or erroneous behavior","","","38. Are there sufficient skilled human crew to operate or recover or stabilise the system (i.e., the aircraft, air traffic situation, etc.) in case of AI failure or erroneous behaviour?","Directly mentions AI failure or erroneous behavior as a scenario requiring human intervention, indicating AI systems can exhibit problematic behaviors that necessitate human oversight.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI error modes and incorrect information","","","40. Is the human trained on AI error modes and how to verify AI results? 41. Has the human seen examples of AI incorrect information/advice in simulation training?","Mentions AI error modes and AI incorrect information/advice, indicating AI systems can produce errors or faulty outputs that require human verification and training.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI robustness against vulnerabilities","","","39. Is the AI robust against edge and corner cases, data bias, and data poisoning?ERROR and FAILURE MANAGEMENT HAZOP/STPA/FRAM","Explicitly queries AI robustness against edge/corner cases, data bias, and data poisoning, highlighting AI-specific vulnerabilities that can lead to failures.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: AI support features and human factors considerations","","","AI display aspects related to the ‘on/off’ status of the AI support; use of aural SA directional guidance; application of workload measures; consideration of how to better maintain a strategic overview during the emergency; pilot trust issues with the AI; consideration of the utility of personalisation of the AI to individual pilots; consideration of potential interference of the AI support with other alerts during an emergency; and use of HAZOP for identification of potential failure modes and recovery/mitigation measures.","This represents AI features as it explicitly mentions AI support, AI display, pilot trust with AI, personalisation of AI, and AI support interference, all of which are characteristics of AI systems in high-risk industries like aviation, aligning with the research focus on human-AI interaction.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Explainability in Sense-Making","","","e.g., in UC2 on Roles and Responsibilities and Sense-Making; in UC4 on the detail of Human-AI Teaming interaction as well as Error and Failure Management, and in a further airport use case (UC5 in HAIKU, EASA category 1A/1B, at TRL 7) on Sense-Making (including the usability and explainability of the system) and Organisational Readiness","This directly references 'explainability of the system' as part of Sense-Making, which relates to the opacity/explainability characteristic of AI systems where internal reasoning may not be transparent.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: Human-AI interaction and control mechanisms","","","In UC1 the IA is triggered by detection of pilot startle; the pilot can activate/deactivate the IA at any time. In UC2 the IA is triggered by certain circumstances, and the pilots can ignore it if they choose to do so. For UC4 the IA suggests changes when required, and if the ATCO does nothing the change of landing/take-off sequence will be automatically implemented. The ATCO can also switch the IA on and off.","This excerpt represents an AI feature as it details the IA's context-aware triggering, adaptive behavior in responding to specific events, and the control mechanisms where humans can activate, deactivate, ignore, or override the IA, highlighting novel AI characteristics like dynamic interaction and human-in-the-loop control compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Advisory AI with supervised training adaptation","","","YYes, for pilots in UC1 the AI is like a clever flight director or attention director, but the pilot remains in control. For UC2 the IA’s advice on three airports is very quick, with the supervised training still being fine-tuned to ensure the recommendations fit with pilots’ expertise and preferences. For UC4 the advice is seen as useful, giving them forewarning of arrivals/departures pressures.","This excerpt describes AI capabilities in providing advisory functions (flight/attention direction, quick advice, forewarning) and mentions supervised training being fine-tuned to fit user expertise and preferences, which relates to AI's adaptive and context-aware characteristics in human-AI interaction.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: context_adaptive | Feature: Situation representation building","","","Does the AI build its own situation representation? YYes, for UC1 coming from the aircraft data-bus, and from the pilot’s attentional behaviour (eye-tracking). Context is also from the SOPs (Standard Operating Procedures) for the events.","This describes the AI's ability to construct its own understanding of the situation by integrating multiple data sources (aircraft data-bus, pilot behavior, SOPs), which is a key characteristic of context-aware/adaptive AI systems that dynamically respond to their environment.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: explainability layer","","","In UC2 the icon display and explainability layer unpack the AI’s computation, showing which factors were prioritized.","This directly relates to opacity/explainability by showing how the AI's internal computations are made transparent to users, helping mitigate black-box behavior and trust challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: interface | Feature: AI explainability and visualization interface","","","For UC1, the EFB (electronic flight bag) to the captain’s left summarises the AI’s situation assessment. For UC2 the results of the three airports selected are shown on the moving map display and in an icon-based display, with a further explainability layer accessible to the flight crew.","This represents an AI feature related to opacity/explainability, as it discusses how AI's internal reasoning (situation assessment and results) is made transparent and accessible to end-users via visualizations and explainability layers, addressing trust and regulation challenges in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: control | Feature: Human modification of AI parameters/goals","","","In UC2 the flight crew can modify the goal priorities of the IA. In UC4 the ATCOs cannot modify the parameters.","This directly addresses a control mechanism in human-AI interaction, specifically whether humans can modify AI parameters or goal priorities, which is a key feature in high-risk industries for managing AI behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Error detection and guidance with dynamic interface feedback","","","In UC1, the AI is intended to detect temporary performance decrement due to startle, and to guide the pilot, but does not go as far as correcting his/her action. However, the dynamic highlighting of key instruments, along with callouts (e.g., “vertical speed!”) could be considered a form of error correction.","This excerpt describes AI capabilities for detecting human performance decrements and providing guidance through dynamic interface elements, which are novel features compared to conventional automation, focusing on human-AI interaction in high-risk contexts.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: AI explainability vs black-box nature","","","Is at least some operational explainability possible, rather than the AI being a ‘black box’? Y In UC1, explainability is via the EFB. However, due to the very short response times in a loss of control in flight scenario, pilots had little time for explainability in the two simulations. This could differ in a scenario where the event was less clear cut, e.g., electronics failures, bus-bar failures, automation malfunction, etc. For UC2, there is a high degree of explainability. In UC4, the explainability needs are basic (aircraft on approach coming in too fast/too slow, etc.) and are deemed sufficient.","Directly addresses the opacity/explainability characteristic by questioning whether AI can be explainable rather than a 'black box', and provides specific examples of explainability levels in different operational contexts, which is a key novel AI feature versus conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: AI_capability | Feature: Error detection and absence of specific AI failure modes","","","In all three simulations, pilots noticed if something was incorrect, usually due to an error in the IA’s database. Such errors should largely be eradicated by TRL 9. No edge cases or hallucinations nor alignment errors have arisen so far.","This excerpt directly mentions AI system characteristics: errors in the AI's database (a data-driven component), and specific AI failure modes like hallucinations and alignment errors, which are novel AI features compared to conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","Category: opacity | Feature: Operational Explainability and AI Error Management","","","Human-IA Teamworking arrangements, sensemaking, shared situation awareness and operational explainability, AI error and failure management, and how to train end users to work effectively with IAs.","This represents an AI feature because it directly mentions 'operational explainability' and 'AI error and failure management', which relate to the opacity/explainability characteristic of novel AI systems, addressing challenges in understanding and managing AI behavior in safety-critical settings.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_overload | Severity: 7","","Startle response is when a pilot in the cockpit is startled by a sudden, unexpected event in or outside the cockpit, leading to a temporary disruption of cognitive functioning, usually lasting approximately 20 s.","Severity Justification: The disruption is temporary (20 seconds), indicating moderate severity as it impairs cognitive functioning but is not permanent. | Relevance Justification: Directly mentions cognitive functioning disruption, aligning with cognitive overload category in the research focus on human performance degradations in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 7","","The last time there was a radical change in human-machine arrangements—namely the introduction of glass cockpits into the industry—it initially led to a spate of ‘automation-assisted accidents’, Appendix 1 in [ 15].","Severity Justification: The excerpt explicitly mentions 'a spate of ‘automation-assisted accidents’', which implies significant negative outcomes and performance issues, warranting a high severity score due to the direct reference to accidents. | Relevance Justification: This is highly relevant as it directly discusses performance degradation (accidents) resulting from changes in human-machine interaction, specifically automation, aligning with the research focus on novel AI/autonomous system characteristics vs. conventional automation in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 7","","The pilot must therefore come to trust the IA, or its advice will be rejected.","Severity Justification: Trust issues can lead to rejection of AI advice, potentially causing errors or inefficiencies in high-risk operations, but the text does not specify severe outcomes. | Relevance Justification: Directly mentions trust as a factor in human-AI interaction, aligning with the research focus on opacity and explainability challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 6","","a sudden unexpected serious event (e.g., a lightning strike) can cause ‘startle’, leading to diminished cognitive performance for a short period of time (e.g., 20 s ) [10].","Severity Justification: The degradation is explicitly stated as 'diminished cognitive performance' and is linked to a serious event, but it is described as temporary ('for a short period of time'), limiting its severity. | Relevance Justification: This directly mentions human performance degradation ('diminished cognitive performance') in the context of a novel AI system (FOCUS IA) designed to address it, aligning with the research focus on human-AI interaction in high-risk industries like transportation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: misinterpretation | Severity: 7","","there remains the opacity issue, wherein the AI is often akin to a ‘black box’, and as such may surprise, confound, or confuse the end users, because most end users will not be able to follow the computations underpinning AI advice.","Severity Justification: The excerpt directly states that AI opacity can cause users to be surprised, confounded, or confused, which are clear indicators of performance degradation due to misunderstanding AI outputs, though it does not specify severe outcomes like errors or accidents. | Relevance Justification: This excerpt is highly relevant as it explicitly mentions novel AI-related degradations (opacity, black-box behavior) that can negatively impact human performance (surprise, confound, confuse) compared to traditional automation, aligning with the research focus on opacity/lack of explainability.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 6","","As work complexity grew, constructs such as situation awareness and mental workload came to the fore, and as automation became the norm, there was a corresponding need to consider complacency and bias, as well as a move from a focus on human error to system resilience.","Severity Justification: The excerpt mentions complacency and bias as significant issues arising from automation, which can degrade human performance by reducing vigilance and increasing errors, but it does not specify novel AI-related degradations. | Relevance Justification: This excerpt is relevant as it addresses human performance degradation linked to automation, including complacency and bias, which are key factors in the research focus on AI vs. traditional automation, though it does not explicitly mention novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 6","","humans are allocated tasks and functions the AI can’t easily do, whether or not the human can do them or take over from the AI when it fails.","Severity Justification: The text implies a systemic issue where humans may be assigned tasks they cannot perform or may fail to take over effectively from AI, suggesting moderate severity in performance degradation. | Relevance Justification: This directly relates to human performance degradation in the context of novel AI systems, as it highlights how task allocation based on AI limitations can lead to human inability to perform tasks or handle failures, which is a key aspect of human-AI interaction in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_overload | Severity: 7","","As an example, as automation increases, human work can require exhausting monitoring tasks, so that rather than needing less training, operators need to be trained more to be ready for the rare but crucial interventions.","Severity Justification: Exhausting monitoring tasks indicate high cognitive load and potential fatigue, which can degrade performance, especially during rare but crucial interventions. | Relevance Justification: Directly addresses human performance degradation due to automation, specifically cognitive overload from monitoring tasks and the need for increased training, which is relevant to the research focus on AI/autonomous system characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_overload | Severity: 5","","This led to the study of mental workload (MWL), considering the task demands vs. human capabilities and capacities, including problems of overload and underload [61,63].","Severity Justification: The text explicitly mentions 'problems of overload and underload' as part of mental workload study, indicating performance degradation related to cognitive load, but does not specify severity or context. | Relevance Justification: The text directly addresses cognitive load issues (overload and underload) as part of human performance in relation to task demands, which aligns with the research focus on human-AI interaction degradations, though it is not explicitly linked to novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: situational_awareness | Severity: 6","","As complexity rose in human-plus-automation environments such as cockpits and air traffic control centres, the question became how well the human understood what was going on, what was going to happen in the near future, and how to (re)act.","Severity Justification: The text explicitly questions human understanding of what's happening and what will happen, indicating potential degradation in situational awareness, though it doesn't describe actual degradation. | Relevance Justification: Directly addresses human understanding in automation environments, which relates to situational awareness degradation, though it frames it as a question rather than stating degradation occurred.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_overload | Severity: 5","","The next question became how much the pilot or air traffic controller could assimilate from their controls and displays, in both short time frames and longer durations.","Severity Justification: Questions human capacity to assimilate information from controls and displays, suggesting potential cognitive overload issues, but doesn't confirm actual degradation. | Relevance Justification: Directly addresses human information processing capabilities in automation contexts, relevant to cognitive overload concerns, though framed as a question.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: situational_awareness | Severity: 8","","There is a very real danger that AI systems, which tend to be ‘black boxes’, can undermine the human crew’s situation awareness, both in terms of what is going on, and of what the AI is doing or attempting to do.","Severity Justification: Directly states a 'very real danger' that AI systems can 'undermine' situation awareness, which is critical for safety in high-risk industries like aviation. | Relevance Justification: Directly addresses a novel AI-related degradation (opacity/lack of explainability leading to reduced situational awareness) as specified in the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: situational_awareness | Severity: 8","","Low SA, plus a sudden spike in mental workload due to loss of the AI support, and an inability of the AI to explain its recommendations, could well be a recipe for disaster.","Severity Justification: The excerpt explicitly links low situational awareness, a sudden spike in mental workload, and lack of AI explainability to a potential disaster, indicating a high risk of severe performance failure. | Relevance Justification: This directly addresses human performance degradation in the context of novel AI characteristics (non-deterministic decision-making and opacity/lack of explainability), as it highlights how AI failure and unpredictability can degrade human situational awareness and cognitive load.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_bias | Severity: 8","","the question is one of how to ensure that no human ‘tunnel vision’ or misdiagnosis by an AI leads to catastrophe.","Severity Justification: The degradation is directly linked to catastrophic outcomes, indicating high severity. | Relevance Justification: This is a direct mention of a human cognitive bias (tunnel vision) in the context of AI interaction, which aligns with the research focus on novel AI-related degradations.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: skill_degradation | Severity: 6","","It may be worth revisiting this model as there are undoubtedly skill sets that may be lost (this already happens with automation), and rules (e.g., Standard Operating Procedures [SOPs] in cockpits) may become more fluid as AI-based support systems find ever-new ways of optimising operations in real time.","Severity Justification: The excerpt explicitly states skill loss as an existing issue with automation, suggesting a moderate severity due to its direct mention and relevance to high-risk industries. | Relevance Justification: This directly addresses skill degradation and behavioral changes related to AI and automation, aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: situational_awareness | Severity: 6","","Such principles (i.e., all of them, not only the top nine) could be revisited for AI, as already several of them are in danger of not being upheld, e.g., some AI-based systems may not be predictable; reciprocal knowledge of the others’ intent may prove difficult to achieve in practice.","Severity Justification: The excerpt highlights potential failures in AI predictability and reciprocal knowledge, which could moderately degrade human performance by causing confusion or mistrust, but it does not specify severe outcomes. | Relevance Justification: This directly relates to human performance degradation in AI interaction by addressing unpredictability and knowledge gaps, aligning with the research focus on non-deterministic decision-making and opacity, though it does not explicitly mention specific degradations like trust issues or cognitive biases.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 6","","Although some blanket terms already exist, such as complacency and over-trust with respect to automation, these are probably not nuanced enough to capture the full extent of the transactional relationships that will exist between human crews and AI support systems, especially as those systems become more advanced and even executive (i.e., not requiring human oversight).","Severity Justification: The text explicitly mentions complacency and over-trust as existing issues with automation, which are known human performance degradations, but notes they may not fully capture future complexities, indicating moderate severity. | Relevance Justification: This directly addresses trust issues (over-trust) and complacency, which are key human performance degradations in human-AI interaction, aligning with the research focus on novel AI characteristics vs. conventional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 7","","the introduction of AI into operational aviation systems could aid or degrade safety culture in aviation. In particular, the concern is that human personnel may delegate some of their safety responsibility to the AI, especially if the AI is taking more of an executive role.","Severity Justification: The degradation is directly linked to safety culture and responsibility delegation in high-risk aviation, which could lead to significant safety issues, but the text presents it as a concern rather than a confirmed outcome. | Relevance Justification: This directly addresses human performance degradation in the context of novel AI systems (executive role AI) versus traditional automation, focusing on behavioral changes (delegation of safety responsibility) that could degrade performance, aligning with the research focus on AI characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 7","","This contrasts with today, where it can take humans a short time to re-adapt, time that can be crucial in an emergency scenario in-flight (one of the principal reasons there are always two pilots in the cockpits of commercial airliners).","Severity Justification: The degradation is described as a time-critical limitation in emergency scenarios, directly linked to a fundamental safety practice (two-pilot rule), indicating significant operational impact. | Relevance Justification: Directly addresses a human performance limitation (re-adaptation time) in the context of novel AI/autonomous systems (IAs) being introduced, contrasting human adaptability with potential AI capabilities.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: automation_bias | Severity: 6","","The availability and salience of contradictory evidence is highly pertinent to the human capacity of acting as a back-up to the AI, given the well-known biases of humans, including representativeness, availability, anchoring and confirmation biases [ 77,78] (AIs, especially LLMs, are also not immune to biases).","Severity Justification: The excerpt explicitly mentions multiple cognitive biases (representativeness, availability, anchoring, confirmation) that are known to degrade human decision-making when interacting with AI systems, particularly in backup roles. This directly addresses bias in decision-making. | Relevance Justification: This directly addresses cognitive biases in human-AI interaction, specifically mentioning confirmation bias and other biases that can degrade human performance when acting as backup to AI systems.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: automation_bias | Severity: 7","","One of the more worrying biases was the withdrawal of attention from cross-checking the automation and considering contradictory evidence, summarised as ‘looking but not seeing’. The effects such as complacency (not checking) and automation bias (over-trust) are not easy to fix, e.g., via training, and are apparently prevalent in both experienced or naïve (i.e., new) users [76].","Severity Justification: The excerpt highlights significant biases like withdrawal of attention and complacency that impair human performance in monitoring automation, with noted difficulty in remediation and widespread prevalence, indicating moderate to high severity in high-risk contexts. | Relevance Justification: This directly addresses human performance degradations such as automation bias and complacency, which are key issues in human-AI interaction, aligning with the research focus on novel AI characteristics and trust challenges in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: skill_degradation | Severity: 6","","existing pilots can compare their experience to what the AI is suggesting, whereas new pilots (in the future) who have never known a system without AI support, may not have such ‘unfiltered’ prior experience.","Severity Justification: The degradation is moderate as it suggests a future scenario where pilots' foundational skills could atrophy due to dependency on AI, but it is not explicitly linked to immediate safety risks or severe errors. | Relevance Justification: Highly relevant as it directly addresses skill degradation in human performance due to novel AI characteristics, specifically the lack of prior experience without AI support, which aligns with the research focus on human-AI interaction in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: automation_bias | Severity: 7","","We already have ‘complacency’, but this is a catch-all term, and knowing when it is likely or not, as a function of the human-AI system design, is unpredictable.","Severity Justification: Complacency is directly mentioned as a known issue, and its unpredictability in safety-critical contexts makes it a significant concern, though it's described as a 'catch-all term' which may moderate severity slightly. | Relevance Justification: This directly addresses human performance degradation (complacency) in the context of human-AI interaction, matching the research focus on novel AI characteristics vs. traditional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: automation_bias | Severity: 7","","Conversely, if they choose not to follow AI advice and there is an accident, they will be asked why they did not follow the AI’s advice.","Severity Justification: Involves legal questioning and potential accidents due to decision-making regarding AI advice. | Relevance Justification: Directly relates to human decision-making in response to AI recommendations and potential performance consequences.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 7","","If they are advised by an AI to do something and it results in an accident, they may be asked by a court of law to justify why they did not recognise the advice was faulty.","Severity Justification: The scenario involves legal consequences and potential accidents, indicating significant performance impact. | Relevance Justification: Directly addresses human interaction with AI advice and potential failure to evaluate it correctly.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 6","","Deception about AI teammate’s identity (pretending it is a human) did not improve overall performance and led to less acceptance of AI solutions.","Severity Justification: The degradation is directly stated as a negative outcome (did not improve performance, led to less acceptance), but it is specific to a deceptive scenario rather than a general performance issue. | Relevance Justification: This directly relates to human performance degradation in the context of AI interaction, specifically addressing how deception affects performance and acceptance, which aligns with the research focus on novel AI characteristics and human-AI interaction.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: automation_bias | Severity: 6","","19. Is the information/decision offered accompanied by uncertainty estimates upon request?Sense-Making, HCA Bias and Complacency","Severity Justification: The mention of 'Bias and Complacency' directly relates to cognitive biases like automation bias and complacency, which can degrade human performance in high-risk industries, but the severity is moderate as it's listed without detailed impact description. | Relevance Justification: This excerpt is highly relevant as it explicitly includes 'Bias and Complacency', which aligns with the research focus on novel AI characteristics like opacity and non-deterministic decision-making, and matches the priority search for cognitive biases.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: cognitive_overload | Severity: 7","","29. Does the level of human workload enable the human to remain proactive rather than reactive, except for short periods? Abnormal Events and Emergencies Mental Workload Ironies of Automation/AI","Severity Justification: High workload preventing proactive behavior can lead to reactive responses, increasing error risk in emergencies, but it's framed as a question rather than a confirmed issue. | Relevance Justification: Directly addresses human workload and its impact on proactive vs. reactive behavior in the context of automation/AI ironies, relevant to cognitive load and performance degradation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: situational_awareness | Severity: 8","","30. Can the human retain situation awareness in emergencies/abnormal events, including what the AI is doing (and why)? Complex Systems/KBB Ironies of Automation/AI","Severity Justification: Loss of situation awareness in emergencies, especially with AI involvement, can severely compromise safety and decision-making, but it's posed as a question. | Relevance Justification: Explicitly mentions situation awareness in emergencies with AI, directly relevant to awareness/task issues and novel AI-related degradations vs. traditional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 6","","pilot trust issues with the AI; consideration of the utility of personalisation of the AI to individual pilots; consideration of potential interference of the AI support with other alerts during an emergency; and use of HAZOP for identification of potential failure modes and recovery/mitigation measures.","Severity Justification: Trust issues are directly stated as a concern, but the severity is moderate as it is identified during design evaluation rather than an observed failure. | Relevance Justification: Directly matches the priority search for trust issues (over-trust, under-trust, trust calibration, trust mismatch) and is explicitly about novel AI-related degradations vs traditional automation.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: performance_metrics | Severity: 5","","the AI is intended to detect temporary performance decrement due to startle, and to guide the pilot, but does not go as far as correcting his/her action.","Severity Justification: The text explicitly mentions a 'temporary performance decrement', which is a direct human performance degradation. It is linked to a specific cause (startle) and is being detected by the AI, indicating it is a recognized issue within the system's scope. The severity is moderate as it is described as temporary and the system has a response (guidance). | Relevance Justification: This is a direct, verbatim mention of a human performance degradation ('temporary performance decrement') within the context of AI interaction (the AI detects it). It perfectly matches the research focus on novel AI characteristics (context-aware/adaptive behavior) addressing human performance issues in high-risk industries (transportation/aviation).","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 5","","UC4 has the potential to correct errors of judgment and memory failures/omissions or vigilance failures, or at the least alert controllers to something they have overlooked or misjudged.","Severity Justification: The excerpt explicitly mentions 'errors of judgment and memory failures/omissions or vigilance failures,' which are direct human performance degradations, but it frames them as correctable by UC4, indicating moderate severity. | Relevance Justification: The excerpt is highly relevant as it verbatim describes human performance degradations (errors of judgment, memory failures/omissions, vigilance failures) in the context of AI/autonomous systems (UC4), aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 7","","Pilots and controllers did not over-trust the IAs, knowing they were prototypes. They were not taught how to recognize aberrant IA behaviour.","Severity Justification: The lack of training on recognizing aberrant AI behavior directly impacts human performance by leaving operators unprepared to identify and respond to AI malfunctions, which is a significant degradation in high-risk contexts. | Relevance Justification: This directly addresses trust calibration (not over-trusting) and a knowledge/skill gap (not being taught to recognize AI malfunction), both of which are key human performance degradations related to novel AI systems as specified in the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: skill_degradation | Severity: 6","","In UC4, the controllers stated that they would likely use ISA in high workload situations, depending on their workload capacity and the complexity of the traffic situation. This partial use was also to avoid ‘skill-fade’.","Severity Justification: Skill-fade is explicitly mentioned as a concern, suggesting moderate severity as it involves proactive avoidance rather than observed degradation. | Relevance Justification: Directly addresses skill degradation in the context of AI/automation use, aligning with the research focus on novel AI-related degradations.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: behavioral_changes | Severity: 8","","The more we ‘personify’ an IA, the more the danger of delegating responsibility to it, or of surrendering authority to its logic and databases, or of basically second-guessing ourselves rather than the AI.","Severity Justification: The text uses strong language like 'danger' and lists multiple negative consequences (delegating responsibility, surrendering authority, second-guessing), indicating high severity. | Relevance Justification: Directly addresses human performance issues related to AI anthropomorphism, which is a novel AI characteristic compared to traditional automation, perfectly matching the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","Category: trust_issues | Severity: 7","","Calibrated Trust wherein the humans learn when to trust and when to ignore the suggestions or override the decisions of the AI, an over-riding concern in UC2.","Severity Justification: The text explicitly mentions 'an over-riding concern', suggesting significant importance and potential impact on performance if trust is not properly calibrated. | Relevance Justification: Directly addresses trust calibration, which is a key aspect of human performance degradation in AI interactions, matching the research focus on novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: radical change in human-machine arrangements (glass cockpits) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: spate of automation-assisted accidents","The last time there was a radical change in human-machine arrangements—namely the introduction of glass cockpits into the industry—it initially led to a spate of ‘automation-assisted accidents’, Appendix 1 in [15].","Causal Strength Justification: Direct causation with 'led to' linking cause and effect, supported by reference to specific accidents. | Relevance Justification: Directly addresses how technological changes (analogous to AI features) affect human performance in safety-critical contexts, though not explicitly about AI features like non-deterministic, opacity, or adaptive behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: radical change in human-machine arrangements (glass cockpits) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: spate of automation-assisted accidents","The last time there was a radical change in human-machine arrangements—namely the introduction of glass cockpits into the industry—it initially led to a spate of ‘automation-assisted accidents’, Appendix 1 in [ 15].","Causal Strength Justification: Direct causal language 'led to' explicitly connects the cause (radical change) to the effect (accidents). | Relevance Justification: The text discusses a historical example of technological change (glass cockpits) causing accidents, which is relevant to the research focus on AI/autonomous systems in high-risk industries, though it does not explicitly mention AI features like non-deterministic, opacity, or adaptive behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: opaque calculations, lack of explainability | Evidence Type: direct | Causal Strength: 8 | Performance Effect: pilots unable to follow AI reasoning, must trust AI or reject advice","First, the information or advice, or even executive action, can be based on calculations that are opaque to the end users (e.g., pilots), because the level of complexity and transparency of how AIs derive their answers means that no amount of theoretical training for pilots will enable them to follow the IA’s ‘reasoning’, unless an additional layer of ‘explainability’ is afforded to the pilot by the AI-based automation. The pilot must therefore come to trust the IA, or its advice will be rejected.","Causal Strength Justification: Direct causal link using 'because' and 'means that' to connect AI opacity to pilot inability and trust requirement. | Relevance Justification: Directly addresses opacity/lack of explainability from research focus and its causal effect on human performance (trust and advice rejection).","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: Not applicable (the cause is the event, not an AI feature) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: diminished cognitive performance for a short period of time","a sudden unexpected serious event (e.g., a lightning strike) can cause ‘startle’, leading to diminished cognitive performance for a short period of time (e.g., 20 s ) [10].","Causal Strength Justification: Direct causal language: 'can cause' and 'leading to' explicitly link the event to startle and startle to diminished cognitive performance. | Relevance Justification: The causal link is explicit and involves human performance (cognitive performance), but it does not directly connect to the specified AI features (non-deterministic, opacity, adaptive) or AI elements; the cause is an external event, not an AI feature.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: opacity/black-box behavior of AI | Evidence Type: direct | Causal Strength: 8 | Performance Effect: surprise, confound, or confuse end users; potential insufficiency of conventional design approaches for safe and acceptable use","Even for ‘lesser’ autonomy levels such as 1B to 2A, and also for 3A, there remains the opacity issue, wherein the AI is often akin to a ‘black box’, and as such may surprise, confound, or confuse the end users, because most end users will not be able to follow the computations underpinning AI advice. This leads to the raison d’etre of this paper, namely that AI and IAs represent something novel, and as such, conventional automation interface design and Human Factors approaches may not be sufficient to assure the safe use and acceptability of such systems.","Causal Strength Justification: Direct causal language: 'as such may surprise, confound, or confuse the end users, because...' and 'This leads to...' explicitly link AI opacity to user confusion and the need for novel design approaches. | Relevance Justification: Directly addresses how opacity (a novel AI characteristic) causally affects human performance (confusion) and system safety/acceptability, matching the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: ISA (Intelligent Support Assistant) real-time assistance | Evidence Type: direct | Causal Strength: 8 | Performance Effect: ATCOs manage traffic flow more efficiently with more look-ahead time, improved decision-making, enhanced runway utilisation, increased operational efficiency, safer and more streamlined air traffic flow, reduced need for go-arounds","The real-time assistance provided by ISA ensures timely and accurate forecast updates, allowing Tower Air Traffic Controllers (ATCOs) to manage the traffic flow more efficiently, with more ‘look-ahead time’ than currently, as ISA can see further ‘upstream’. The benefits are improved decision-making, enhanced runway utilisation, increased operational efficiency, and a safer and more streamlined air traffic flow that reduces the need for ‘go-arounds’.","Causal Strength Justification: Direct causal language: 'allowing' and 'The benefits are' explicitly link ISA's assistance to the listed performance effects. | Relevance Justification: Directly addresses how an AI feature (ISA's real-time assistance) causally affects human performance (ATCO efficiency and decision-making), though it does not explicitly mention non-deterministic, opacity, or adaptive characteristics from the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: automation (as a precursor to AI systems) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: emergence of situation awareness and mental workload as key constructs; need to consider complacency and bias; shift from human error focus to system resilience","As work complexity grew, constructs such as situation awareness and mental workload came to the fore, and as automation became the norm, there was a corresponding need to consider complacency and bias, as well as a move from a focus on human error to system resilience.","Causal Strength Justification: Direct causal language is used ('As work complexity grew, constructs... came to the fore', 'as automation became the norm, there was a corresponding need...'), explicitly linking the causes (work complexity growth, automation becoming norm) to the effects (emergence of constructs, need to consider issues, shift in focus). | Relevance Justification: The excerpt directly links automation (a key aspect of AI/autonomous systems) to human performance effects like complacency and bias, which are relevant to the research focus on AI characteristics in high-risk industries. However, it does not explicitly mention the novel AI features (non-deterministic, opacity, adaptive) from the prompt.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: Not applicable (no AI features mentioned in this excerpt) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Team and communication errors, leading to the need for specific training on leadership, decision-making, and communication","The need for CRM grew from the world’s worst civil aviation air disaster at Tenerife airport in 1977 but was also linked to the United Airlines Flight 173 air crash in 1978 [ 45]. Both accidents highlighted team and communication errors, and the need for specific training on leadership, decision-making, and communication on the flight deck, and with air traffic control (which in Europe later developed its own version of CRM called Team Resource Management or TRM [ 46]).","Causal Strength Justification: Direct causal language ('grew from', 'linked to') explicitly connects accidents to the need for CRM, with strong historical evidence from specific events. | Relevance Justification: This excerpt discusses causal relationships in human performance (team and communication errors leading to training needs), but it does not involve AI features as specified in the task; it is about historical aviation accidents and CRM development.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI systems which tend to be 'black boxes' | Evidence Type: direct | Causal Strength: 8 | Performance Effect: undermine the human crew's situation awareness","There is a very real danger that AI systems, which tend to be ‘black boxes’, can undermine the human crew’s situation awareness, both in terms of what is going on, and of what the AI is doing or attempting to do.","Causal Strength Justification: Direct causal language ('can undermine') explicitly links the AI feature (black box nature) to the negative effect on human performance (situation awareness). | Relevance Justification: Directly addresses how an AI characteristic (opacity/black box) causally affects human performance (situation awareness), matching the research focus on opacity/lack of explainability.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: loss of AI support and inability to explain recommendations | Evidence Type: direct | Causal Strength: 8 | Performance Effect: low SA, sudden spike in mental workload, and potential disaster","Low SA, plus a sudden spike in mental workload due to loss of the AI support, and an inability of the AI to explain its recommendations, could well be a recipe for disaster.","Causal Strength Justification: The text uses direct causal language ('due to' and 'could well be a recipe for disaster') to link AI features to human effects, though it is conditional ('could') rather than definitive. | Relevance Justification: This directly addresses how AI opacity (inability to explain) and adaptive behavior (loss of support) causally affect human performance in high-risk contexts, matching the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: misdiagnosis by an AI | Evidence Type: direct | Causal Strength: 8 | Performance Effect: catastrophe","the question is one of how to ensure that no human ‘tunnel vision’ or misdiagnosis by an AI leads to catastrophe.","Causal Strength Justification: Direct causal language ('leads to') explicitly links AI misdiagnosis to catastrophe. | Relevance Justification: Directly addresses how an AI feature (misdiagnosis) causally affects a severe performance outcome (catastrophe), though it does not specify a novel AI characteristic like non-deterministic, opacity, or adaptive behavior.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI taking an executive role | Evidence Type: direct | Causal Strength: 7 | Performance Effect: human personnel delegating safety responsibility","the introduction of AI into operational aviation systems could aid or degrade safety culture in aviation. In particular, the concern is that human personnel may delegate some of their safety responsibility to the AI, especially if the AI is taking more of an executive role.","Causal Strength Justification: Direct causal language ('could aid or degrade', 'the concern is that') explicitly links AI introduction to potential effects on safety culture and human behavior. | Relevance Justification: Directly addresses how AI features (executive role) could causally affect human performance (delegation of safety responsibility) in aviation, aligning with the research focus on human-AI interaction in high-risk industries.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI anthropomorphism (personifying AI) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: danger of delegating responsibility to AI, surrendering authority to AI logic, second-guessing human judgment","The more we ‘personify’ an IA, the more the danger of delegating responsibility to it, or of surrendering authority to its logic and databases, or of basically second-guessing ourselves rather than the AI.","Causal Strength Justification: Direct causal language ('the more...the more the danger') explicitly links increased anthropomorphism to specific negative human performance outcomes. | Relevance Justification: Directly addresses how an AI characteristic (anthropomorphism) causally affects human performance (decision-making authority and responsibility delegation), which aligns with the research focus on human-AI interaction effects.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI teammate identity deception (pretending it is human) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: less acceptance of AI solutions","Deception about AI teammate’s identity (pretending it is a human) did not improve overall performance and led to less acceptance of AI solutions.","Causal Strength Justification: Direct causal language 'led to' explicitly links the cause (deception) to the effect (less acceptance), with no ambiguity. | Relevance Justification: Directly addresses how an AI characteristic (identity deception) affects human performance (acceptance), though not specifically tied to non-deterministic, opacity, or adaptive features from the research focus.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: Observability of AI progress | Evidence Type: correlation | Causal Strength: 5 | Performance Effect: Human understanding and trust through transparency and explainability","Observability refers to the transparency of the progress of the AI agent when resolving a problem, and can be linked to explainability, though in practice the AI’s workings might be routinely monitored via a dashboard or other visualisation rather than a stream of textual explanations, with the user able to pose questions as needed.","Causal Strength Justification: Uses causal language ('can be linked to') to indicate a relationship between observability and explainability, suggesting a mechanism for affecting human performance, but it is indirect and correlational. | Relevance Justification: Addresses opacity and explainability in AI, linking it to human performance through observability, which is relevant to the research focus on trust and regulation challenges.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI outputs that might surprise humans | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Predictability of AI pace and outputs, affecting human understanding and management","The latter will depend on the amount of human-AI training afforded prior to teamworking in real operational settings.","Causal Strength Justification: Direct causal language ('will depend on') explicitly links the cause (human-AI training) to the effect (predictability of AI surprises), though it is conditional rather than absolute. | Relevance Justification: Directly addresses how an AI feature (non-deterministic outputs) affects human performance (predictability and trust) through a causal mechanism (training), aligning with the research focus on novel AI characteristics.","y"
"Human Factors Requirements for Human-AI Teaming in Aviation","","","AI Feature: AI tools (implied as non-deterministic or unreliable) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: break trust and lead to non-use of the IA","A general comment, though, from many end users, is that if such tools are to be implemented in the cockpit or ATC tower, they must be trustworthy and highly reliable; one serious mistake would irrevocably break trust and lead to non-use of the IA.","Causal Strength Justification: Direct causation with 'lead to' linking AI mistake to human trust and usage outcomes. | Relevance Justification: Directly addresses how AI characteristics (unreliability) affect human performance (trust and usage) in high-risk industries like transportation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Need for insight into internal reasoning and actions","","","providing insight into the agents’ internal reasoning and actions becomes a key consideration in supporting future supervisors.","This directly addresses the opacity/explainability characteristic by emphasizing the need to provide insight into agents' internal reasoning and actions, which is a key aspect of making AI systems transparent and interpretable.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous behavior of AI agents","","","Given the trends towards the application of artificially intelligent agents capable of autonomous behaviour, this study anticipates that transparency becomes an essential prerequisite for safe and effective human-autonomy system oversight.","This directly mentions 'artificially intelligent agents capable of autonomous behaviour', which is a core AI feature distinguishing it from conventional automation, as it implies self-directed action rather than pre-programmed responses.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: autonomous conflict resolution and vessel operation","","","conflicts and berthing automatically (Rolls Royce, 2018). The AUTOSHIP research project aimed to build, test, and operate two autonomous vessels with capabilities for short sea shipping and inland waterway scenarios (AUTOSHIP, 2019).","This excerpt describes autonomous features like automatic berthing and conflict resolution, along with project-based development of autonomous vessels, showcasing AI-driven automation and adaptive capabilities in maritime contexts.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: autonomous obstacle detection and avoidance","","","Furthermore, Rolls Royce proposed an autonomous ferry in Finland showing its capabilities for fusing sensor information, detecting obstacles, avoiding","This excerpt highlights AI capabilities such as sensor fusion and obstacle avoidance, which are key features of autonomous systems in high-risk industries like maritime, demonstrating context-aware and adaptive behavior.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: Human supervision challenges with autonomous systems","","","humans supervising autonomous systems may, among others, be prone to information overload, reduced vigilance, automation bias, data misinterpretation, inappropriate SA, lack of knowledge or skills, and lack of mechanisms to intervene.","This highlights interface and interaction issues specific to AI/autonomous systems, such as automation bias and lack of intervention mechanisms, which are relevant to human-AI interaction in high-risk contexts.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous goal-directed behavior and independent task execution","","","autonomous ships are envisioned to deploy artificially intelligent agents capable of sensing its environment and executing goal-directed behaviour using actuators (Russell and Norvig, 2022). Such agents are anticipated to be able to perform the navigational tasks, independent of human input, by having an ability to detect targets, determine collision risks, decide how to avoid collisions, and execute avoidance manoeuvres.","This directly describes AI agents with capabilities for autonomous operation, including sensing, decision-making, and action execution without human input, which are key novel AI characteristics compared to conventional automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Human-supervised autonomous systems","","","However, as there are challenges related to developing highly reliable systems capable of performing collision avoidance in all relevant situations, most of the maritime autonomy concepts currently under development employ a human supervisor that monitors the ship’s operation and can intervene when the system’s performance is insufficient (Rødseth et al., 2021).","This excerpt describes a key feature of current maritime autonomy concepts: the integration of human supervisors to monitor and intervene when autonomous system performance is inadequate, highlighting the limitations of fully autonomous systems in handling all relevant situations.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Risk of human supervision in autonomous systems","","","Although the idea of having a human as a backup to compensate for system limitations is attractive, the introduction of humans in a supervisory position to monitor advanced autonomous systems can introduce new and unknown risks (Ramos et al., 2019, Ramos et al., 2018, Veitch and Alsos, 2022).","This excerpt identifies a feature of advanced autonomous systems: the potential for human supervision to introduce novel risks, which is a critical consideration in the design and deployment of AI-driven automation in high-risk environments like maritime operations.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Difficulty with qualitative and interpretative rules","","","This rule is a good example of the use of terminology that may make sense to a human, but that an autonomous system may find difficult to conform with.","This excerpt highlights an AI feature related to opacity/explainability, as autonomous systems find it hard to conform with human-interpretable terminology, indicating challenges in understanding and applying qualitative rules, which aligns with the research focus on opacity/lack of explainability in AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: control | Feature: Boundary-defined autonomous operation with responsibility delegation","","","One could consider setting boundaries for the autonomous system by defining when collision avoidance responsibility is performed by the system and when it is performed by the navigator. This would allow the system to operate autonomously within clearly defined limits and delegate responsibility when it cannot perform its function to a sufficient degree.","This excerpt highlights a control mechanism for autonomous systems, where the system's operational boundaries are explicitly defined, and responsibility is delegated to a human operator when the system cannot perform adequately, which is a key feature in human-AI interaction for managing system limitations and ensuring safety.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous system self-awareness and human intervention request","","","When a n autonomous CAGA system is unable to resolve a collision situation, it can prompt the supervisor by providing a take-over request allowing the supervisor to assume the collision avoidance task or otherwise take control of the ship.","This represents an AI feature because it shows the autonomous system has situational awareness of its own failure modes and can initiate a structured handover protocol to human supervision, which is characteristic of advanced autonomous systems versus simple automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: control | Feature: Human supervisor override capability","","","Alternatively, a supervisor may be unsatisfied with the performance of the system and initiate an intervention at the supervisor’s own discretion.","This represents an AI feature because it describes a control mechanism where human judgment can override autonomous system operation, which is a key aspect of human-AI interaction in safety-critical systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Human understanding of system decisions","","","a case where this task is performed by a human-supervised system. In particular, this study aims to systematically explore how human supervisors will be able to understand the system’s collision and grounding avoidance decisions and actions through mapping goals, decision, and information needs.","This directly addresses the opacity/explainability characteristic by examining how humans can comprehend the system's internal decision-making processes, which is a key challenge for AI systems versus conventional automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: System understandability and predictability","","","CAGA systems and their interaction with human users. That is, the study concentrates on identifying the information required to make CAGA systems understandable and predictable to the human user irrespective of how autonomy is solved.","This directly addresses the opacity/explainability characteristic by focusing on making AI systems 'understandable and predictable' to humans, which is a core challenge with black-box AI behavior.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Supervisory role shift in autonomous systems","","","Regardless of the type of intervention, the introduction of a CAGA system changes the role of navigators from the ones performing collision avoidance to ones supervising an agent performing collision avoidance.","This excerpt highlights a key AI feature: the transition from conventional automation (direct human control) to novel AI systems where humans supervise autonomous agents, reflecting adaptive and context-aware behavior in high-risk industries like maritime.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: platforms/frameworks | Feature: Experimental testbeds for human-AI interaction","","","In addition, some have built and evaluated control centre research facilities to study human factors in potential remote supervision scenarios (Alsos et al., 2022, Hoem et al., 2022).","This excerpt represents an AI feature by highlighting platforms/frameworks (control centre research facilities) used to study human-AI interaction, which is essential for addressing opacity, adaptability, and non-deterministic aspects of autonomous systems in high-risk settings.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: Remote supervision interface design","","","To better understand this change, some studies have focused on how to design remote control centres to support humans in performing remote supervision of autonomous vessels (Man et al., 2018, Man et al., 2016, Porathe, 2021, Porathe, 2014).","This excerpt relates to AI features by addressing the need for specialized interfaces (remote control centers) to manage autonomous systems, which is crucial for handling non-deterministic and context-aware behaviors in dynamic environments like maritime operations.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Risk modeling for autonomous systems","","","Others have focused on developing risk models and methods to accommodate human reliability in maritime autonomous collision avoidance with remote supervision (Ramos et al., 2020, Ramos et al., 2019, Thieme and Utne, 2017, Ventikos et al., 2020).","This excerpt illustrates an AI feature by emphasizing the development of risk models to handle human reliability in autonomous systems, which relates to non-deterministic decision-making and adaptive behavior in unpredictable maritime environments.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: autonomous system with human-machine teaming","","","interaction with an autonomous CAGA system (i.e., the supervision case). For this part, a series of questions based on a modified MITRE Human-Machine Teaming Systems Engineering Guide (MITRE, 2018) was used to identify SA requirements when humans team with advanced automation to perform a task. Table 2.","This represents an AI feature as it involves an autonomous system (CAGA) and advanced automation, which are key characteristics of AI in high-risk industries, focusing on human interaction and teaming requirements.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: System perception and environment appraisal insight","","","By providing insight into what the system perceives and how it appraises its environment, the user should have an adequate information basis to understand the system’s interpretation of its surroundings.","This represents an AI system feature related to human-AI interaction, specifically the interface mechanism that conveys the system's internal processing (perception and appraisal) to help the user understand its interpretation, which addresses aspects of explainability and transparency in AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: System disclosure for understandability and predictability","","","The location of the user was not defined as this case aimed to answer the question as to what information the system should disclose about its itself to provide understandability and predictability to its user.","This addresses opacity/explainability by emphasizing the requirement for the AI system to provide information to make its behavior understandable and predictable, a key challenge in AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous decision-making and execution","","","This system was assumed to be able to sense and keep track of its environment through a suite of sensors, estimate a safe sailing speed, determine collision risk, and calculate, plan, and execute avoidance manoeuvres, in accordance with COLREGs.","This excerpt highlights AI features such as context-aware behavior through environmental sensing and adaptive decision-making for collision avoidance, which are novel compared to conventional automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: System perception display for user insight","","","In order for the user to obtain insight into the system’s perception of its environment, the CAGA system should, at minimum, show which elements in its environment it has identified (e.g., vessels, objects, terrain, and other navigational constraints) and which of these pose a collision risk (see Fig. 5).","This addresses the need for the AI system to make its internal perception and risk assessment transparent to the user, which relates to explainability and trust in AI decision-making.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Automated look-out function with user verification","","","In the supervision case, the look-out function is now performed by the CAGA system, and the aim of the user of CAGA is to verify that the system has performed an adequate appraisal of the situation and determined collision risk.","This describes an AI system (CAGA) taking over a critical safety function (look-out) from humans, with the user supervising its performance, which is a key AI capability in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: non_deterministic | Feature: Sensor uncertainty and interpretation errors","","","uncertainties in its sensor systems (e.g., cameras) shall be considered when estimating speed. In cases of uncertainties in the sensor information, or errors in the CAGA system's interpretation of sensor uncertainty, user intervention can be considered.","This excerpt highlights uncertainties in sensor data and potential errors in the AI system's interpretation, which are characteristics of non-deterministic, data-driven decision-making where outputs can vary with data states, leading to unpredictable failures or the need for user intervention.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Human-supervised autonomous collision avoidance","","","Transitioning from conventional collision avoidance to human-supervised autonomous collision avoidance has consequences for the locus of the decisions and actions associated with this task.","This excerpt explicitly mentions 'human-supervised autonomous collision avoidance,' which represents an AI system feature involving autonomous decision-making with human oversight, contrasting with conventional automation where decisions are solely human-made.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: AI agent performing safety-critical tasks","","","In the supervision case, these have now been outsourced to an artificially intelligent agent capable of collision and grounding avoidance. In turn, the supervisor is left with the task of overseeing the agent’s decision-making and actions.","This represents an AI feature as it explicitly mentions an 'artificially intelligent agent' performing specific safety-critical functions (collision and grounding avoidance), which is a capability beyond conventional automation, requiring human supervision of its decision-making and actions.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous decision-making with cost-based optimization","","","In this type of supervisory control, the system informs the supervisor of a collision avoidance solution and allows for a restricted time to veto before the solution is executed. In our supervision case, the system may detect a ship on collision course, determines that own-ship is the give-way vessel, calculate evasion alternatives and select the optimal option based on a cost function.","This demonstrates AI capabilities in autonomous decision-making, where the system performs complex tasks (detection, determination, calculation, selection) based on a cost function, which is characteristic of data-driven or optimization-based AI systems rather than simple rule-based automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Autonomous decision-making and execution","","","The CAGA system continuously detects and analyses its environment in search for traffic conflicts, and subsequently makes decisions and executes manoeuvres to avoid collisions whilst informing the human supervisor about its actions.","This describes an AI system's ability to independently perform complex tasks (detection, analysis, decision-making, execution) in a dynamic environment, which is a key characteristic of advanced AI versus conventional automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: High-level autonomous operation","","","As the system is not dependent on input from the supervisor to perform its actions unless deemed necessary, this type of automation implementation requires a high degree of sophistication.","This highlights the system's autonomy and sophistication in not requiring constant human input, which is a feature of advanced AI systems compared to simpler, deterministic automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: Human supervisory control","","","The role of the human has therefore changed from an active one to supervisory one that manages a system by means of exception (Cummings et al., 2007, Sheridan and Verplank, 1978).","This reflects an AI feature where human interaction is reduced to oversight and exception handling, indicating a shift in human-AI interaction dynamics typical of advanced systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Risk estimation errors","","","rule 7a states that a supervisor may intervene if there is an error in the risk estimation capabilities of the system.","This represents an AI feature as it involves the system's ability to estimate risk, a key function in AI decision-making, and errors in this capability indicate potential unreliability or unpredictability, aligning with AI characteristics in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Error-prone AI actions","","","a supervisor may intervene in the system in cases of errors in the CAGA system's current actions, planned actions, or timeliness of its actions.","This represents an AI feature as it highlights the system's capability to have errors in its decision-making and actions, which is a characteristic of AI systems, especially in dynamic contexts, and relates to unpredictability and potential failures.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: control | Feature: supervisor assessment capability for system behavior near operational limits","","","Self-paced take-overs, however, imply that the supervisor can assess system performance, i.e., able to assess the system’s and/or ship’s behaviour close to the operational envelope but which has not (yet) led to a take-over request.","This represents an AI/autonomous system feature related to control mechanisms and human-AI interaction, specifically the supervisor's ability to monitor and assess system performance in dynamic situations, which is relevant to context-aware/adaptive behavior and control in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Automation bias in human-agent interaction","","","Automation bias may manifest itself when humans interact with agents designed to aid decision making in complex environments. The tendency to rely on the agent’s decisions makes humans less critical to scrutinise the background information for the proposed solution. Therefore, placing too high trust in the agent’s proposals becomes a problem when the agent is not fully reliable (Bowden e t al., 2021, Hutchinson et al., 2022, Lee and See, 2004).","This excerpt directly discusses a key feature of AI/autonomous systems in high-risk contexts: the interaction between humans and decision-aiding agents, highlighting the risk of automation bias where humans overly trust agent proposals without proper scrutiny, which is particularly relevant to supervisory control of autonomous systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Automation conundrum in human-supervised systems","","","An intricate challenge exists in cases where humans interact with systems to which more automation is added. The addition of automation adds to the overall reliability and robustness of the system, whilst the human’s ability to take over manually decreases due to reduced SA. This “automation conundrum” and its effects need to be well understood when developing human-supervised CAGA systems in which humans take a management-by-exception role (Endsley, 2017).","This excerpt describes a key feature of advanced automation systems where increased automation leads to reduced human manual takeover capability, highlighting a novel characteristic compared to conventional automation that requires careful understanding in human-supervised contexts like CAGA systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Agent reasoning insight","","","However, some have suggested that insight into the agent’s reasoning, allowing a supervisor to understand its internal activities, may alleviate some of the effects of automation bias (Gegoff et al., 2023, Wright et al., 2016).","Directly addresses the opacity/explainability characteristic by discussing the need for insight into internal reasoning to reduce automation bias, which relates to trust and understanding of AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Transparency benefits on human factors","","","Several recent reviews have investigated the relation between automation transparency and typical human factors variables, demonstrating a promising effect in terms of improved operator performance variables (e.g., decision making) and SA without the added cost of mental workload (Bhaskara et al., 2020, van de Mer we et al., 2022).","Discusses the impact of transparency (related to opacity/explainability) on human factors like decision-making and situational awareness, highlighting its role in AI system interaction.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Agent transparency for understanding","","","Making an agent’s internal reasoning available to its user, i.e., making it transparent, should provide the ability for a user to understand what the agent is doing, why it is doing it, and what it will do next (Endsley, 2017).","Explicitly defines transparency as making internal reasoning available, directly addressing the opacity/explainability characteristic by enabling users to understand AI behavior.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: feedback | Feature: Visual feedback for system function understanding","","","For example, to understand that the CAGA system is performing the lookout function (rule 5), it should show which vessels, terrain, and other objects and constraints it has detected.","This illustrates a feedback mechanism where the system shows what it has detected, which supports the research focus on novel AI characteristics by enhancing explainability (making internal reasoning visible) and context-aware behavior (responding to dynamic environments like maritime settings).","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Situation awareness for understanding and predicting autonomous system actions","","","For supervising autonomous CAGA systems, the results of this study depict the SA requirements to allow the user to understand and predict the systems’ actions.","This excerpt highlights the need for users to understand and predict autonomous system actions, which connects to the research focus on novel AI characteristics such as opacity/explainability (addressing black-box behavior) and context-aware/adaptive behavior (managing dynamic responses) in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: interface | Feature: Interface design for understandability and predictability","","","understandability and predictability of the agent’s actions is something that can, theoretically, be designed in its human machine interface.","This directly addresses the design of AI system interfaces to enhance understandability and predictability, which relates to the research focus on novel AI characteristics like opacity/explainability and context-aware/adaptive behavior by making system actions more transparent and comprehensible to users.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Explainable collision avoidance planning","","","Finally, in determining the collision avoidance manoeuvre (rule 8), the system should show, amongst others, which actions it intends to perform, when it intends to perform these actions, including its understanding of vessel priorities, i.e., which ship will stand-on, and which ship will give-way.","This represents an AI feature by requiring the system to transparently communicate its planned actions and reasoning, such as vessel priorities, which addresses opacity and supports human oversight in autonomous decision-making.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Context-aware collision risk estimation","","","In terms of estimating collision risk (rule 7), the system should show which vessels, in the short to long range, form a collision risk, including its understanding of vessel type, size, distance to target and the presence of a tow.","This represents an AI feature by demonstrating context-aware behavior, where the system dynamically assesses and explains collision risks based on multiple environmental factors, enhancing adaptability and transparency.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Transparency in safe speed determination","","","In determining which speed is safe given the circumstances (rule 6), the system should provide the chosen safe speed, the parameters this is based on, and any uncertainties in the sensor data affecting the chosen safe speed.","This represents an AI feature by requiring the system to explain its decision-making process, including parameters and uncertainties, which addresses explainability and trust in autonomous systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: internal reasoning depiction","","","the information depicting the system’s internal reasoning regarding a collision situation.","This directly references the system's internal reasoning, which is a key aspect of opacity/explainability in AI systems, as it implies the need to understand or depict how the system makes decisions, aligning with the research focus on black-box behavior and trust/regulation challenges.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: automation | Feature: Function allocation between human and AI system","","","That is, in the baseline case, the collision avoidance function was allocated to the human navigator, and in the supervision case, this function was performed to the CAGA system.","This demonstrates a key AI/automation characteristic - the allocation of critical functions (collision avoidance) to autonomous systems versus human operators, which is fundamental to human-AI interaction in high-risk domains.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: Transparency requirements for autonomous systems","","","As such, by using a cognitive task analysis approach, this study derived at SA requirements for making autonomous CAGA systems transparent to its user for this type of supervisory control.","This directly addresses the opacity/explainability characteristic by focusing on making autonomous systems transparent to users, which is a key challenge in human-AI interaction for supervisory control.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: control | Feature: Supervisor intervention and manual control in autonomous systems","","","In addition, future studies should address means for how supervisors can effectively intervene in the system. For example, on a bridge, a navigator may look out the window to cross-check the information from the collision avoidance system and may intervene using the ship’s existing control options.","This excerpt describes features related to human-AI interaction in autonomous systems, specifically focusing on how supervisors can intervene and use manual control options, which is a key aspect of AI system capabilities in high-risk industries like maritime.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: lack of transparency in autonomous systems","","","This study has elucidated the relevance of affording human supervisors with insight into an autonomous system’s reasoning to support human-autonomy system oversight and discussed transparency as an important prerequisite on the path towards safe and effective human-supervisory control.","This excerpt directly addresses the opacity/explainability characteristic by emphasizing the need for insight into the system's reasoning and transparency, which are central to mitigating black-box behavior and trust challenges in AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: Supervisory verification and intervention decision-making","","","The study also depicted changes to the navigators’ role from those performing collision avoidance to those verifying a system performing collision avoidance, including deciding on whether to intervene. Decisions of this kind require the supervisor to perceive the current and anticipated actions of the CAGA system, create a mental model of its behaviour, and evaluate its adequacy in its context.","This excerpt highlights a key AI feature where humans transition from performing tasks to supervising autonomous systems, involving perception of system actions, mental modeling of behavior, and contextual evaluation—characteristic of advanced AI/autonomous systems in high-risk domains like maritime, emphasizing human-AI interaction and control mechanisms.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: intelligent agents in safety-critical industries","","","Current societal trends point towards the application of intelligent agents across safety critical industries, e.g., automobile (Society of Automotive Engineers, 2021), healthcare (Coronato et al., 2020, Loftus et al., 2020), and manufacturing (Elghoneimy and Gruver, 2012).","This highlights the deployment of AI systems (intelligent agents) in high-risk domains, which is a key aspect of novel AI characteristics compared to conventional automation, as it implies advanced capabilities beyond simple automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: opacity | Feature: agent reasoning disclosure requirement","","","Therefore, it is essential that future supervisors of autonomous systems are supported in this role by ensuring agent reasoning is disclosed such that they remain on the information loop.","This directly addresses the opacity/explainability characteristic by emphasizing the need to disclose agent reasoning, which relates to making internal decision-making processes visible to human supervisors.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","Category: AI_capability | Feature: advanced autonomous capabilities in maritime","","","Also in the maritime industry, ships with advanced autonomous capabilities are impending (IMO, 2021).","This describes the emergence of AI-driven autonomous systems in the maritime sector, indicating sophisticated, self-governing functionalities that go beyond traditional automation, aligning with novel AI features like adaptive or context-aware behavior.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: cognitive_overload | Severity: 4","","The results further indicate a change towards increased cognitive activities required to verify agent performance.","Severity Justification: The excerpt explicitly mentions increased cognitive activities, which implies higher cognitive load, but it does not specify severe degradation or negative outcomes. | Relevance Justification: This directly relates to cognitive load issues in human-AI interaction, as it describes a change in cognitive demands due to AI agent performance verification, aligning with the research focus on novel AI characteristics.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: behavioral_changes | Severity: 7","","the introduction of humans in a supervisory position to monitor advanced autonomous systems can introduce new and unknown risks (Ramos et al., 2019, Ramos et al., 2018, Veitch and Alsos, 2022).","Severity Justification: The excerpt directly links human supervision of advanced autonomous systems to the introduction of new and unknown risks, which suggests significant potential for performance issues, though it does not specify exact degradations. | Relevance Justification: This is highly relevant as it addresses novel AI-related degradations (monitoring advanced autonomous systems) versus traditional automation, focusing on risks from human-AI interaction in a supervisory context.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: cognitive_overload | Severity: 7","","They found that humans supervising autonomous systems may, among others, be prone to information overload, reduced vigilance, automation bias, data misinterpretation, inappropriate SA, lack of knowledge or skills, and lack of mechanisms to intervene.","Severity Justification: The degradations listed (information overload, reduced vigilance, automation bias, data misinterpretation, inappropriate SA, lack of knowledge/skills) are significant and directly impact safety-critical tasks in high-risk industries, though the text uses 'may be prone to' indicating potential rather than certainty. | Relevance Justification: The excerpt directly addresses human performance degradations in the context of supervising autonomous/AI systems, matching the research focus on novel AI characteristics vs. conventional automation, and includes multiple specific degradations from the priority search list (information overload, automation bias, reduced situational awareness, misinterpretation, skill/knowledge issues).","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: behavioral_changes | Severity: 7","","human factors related challenges associated with monitoring complex systems, such as complacency (Parasuraman and Manzey, 2010), automation bias (Wickens et al., 2015), reduced vigilance (Wohleber et al., 2019), increased workload during manual take-overs (Endsley, 2017), and SA related issues (Endsley and Kiris, 1995).","Severity Justification: The degradations listed (complacency, automation bias, reduced vigilance, increased workload, SA issues) are well-documented in human factors literature as significant performance problems in complex system monitoring, particularly in high-risk industries. | Relevance Justification: The text directly mentions multiple human performance degradations (complacency, automation bias, reduced vigilance, increased workload, SA issues) that align with the research focus on human-AI interaction challenges, though it doesn't explicitly distinguish between novel AI-related and traditional automation degradations.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: performance_metrics | Severity: 5","","uncertainty in the information available to the navigator can hamper accurate risk estimation.","Severity Justification: The degradation is moderate as it directly affects risk estimation accuracy, a critical task in high-risk industries, but is not explicitly linked to severe outcomes like accidents. | Relevance Justification: Highly relevant as it addresses uncertainty in information, which aligns with the research focus on non-deterministic decision-making and unpredictability in AI/autonomous systems, impacting human performance in risk assessment.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: skill_degradation | Severity: 6","","The role of the human has therefore changed from an active one to supervisory one that manages a system by means of exception (Cummings et al., 2007, Sheridan and Verplank, 1978).","Severity Justification: The change from active to supervisory role suggests a reduction in hands-on skills and decision-making, which could lead to skill atrophy over time, but it is not explicitly stated as a severe degradation. | Relevance Justification: This directly relates to human performance degradation in the context of novel AI systems, as it highlights a shift in human involvement that may impact skill retention and competence, aligning with the research focus on adaptive behavior and human-AI interaction.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: trust_issues | Severity: 6","","Alternatively, the system may perform an avoidance manoeuvre that, according to the supervisor, is not deem ed sufficiently safe in terms of distance (Fig. 8, rule 8d), leading the supervisor to decide to perform an avoidance manoeuvre manually.","Severity Justification: This indicates under-trust or trust mismatch, as the supervisor overrides the system due to safety concerns, potentially degrading performance by increasing workload and decision latency. | Relevance Justification: Directly relates to trust issues in human-AI interaction, where the supervisor's judgment of system safety leads to manual action, highlighting performance degradation in decision-making and reliance.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: situational_awareness | Severity: 6","","Such deviations between system behaviour and the supervisor’s expectations of the system’s behaviour may prompt a decision to intervene at own discretion. To make such decisions in an ever-changing context, and to intervene at any moment and at one’s own initiative, the supervisor needs to have appropriate SA based on access to continuous, sufficient, and relevant information (Endsley et al., 2003, Sheridan, 2002).","Severity Justification: The excerpt implies a potential degradation in situational awareness if supervisors lack appropriate SA due to insufficient information, which could lead to poor intervention decisions in dynamic contexts, but it does not explicitly state severe performance failures. | Relevance Justification: This directly relates to reduced situational awareness (a priority search item) in the context of novel AI systems where deviations from expected behavior (non-deterministic/adaptive features) challenge supervisor expectations and decision-making, aligning with the research focus on human-AI interaction in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: behavioral_changes | Severity: 5","","Evaluating agent behaviour in a dynamic high-risk context puts responsibility on the human to adequately perform the supervisory task.","Severity Justification: The text implies a potential for performance issues if humans do not adequately perform supervisory tasks, but it does not explicitly describe severe degradation or specific negative outcomes. | Relevance Justification: This relates to human performance in supervisory roles with AI systems, aligning with the research focus on human-AI interaction and behavioral changes, though it is not explicitly about novel AI-related degradations vs. traditional automation.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: situational_awareness | Severity: 7","","whilst the human’s ability to take over manually decreases due to reduced SA.","Severity Justification: Decreased ability to take over manually in high-risk systems is a significant performance issue with potential safety consequences. | Relevance Justification: Directly mentions human performance degradation (decreased ability) and links it to reduced situational awareness, which is a key focus area.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: automation_bias | Severity: 6","","However, some have suggested that insight into the agent’s reasoning, allowing a supervisor to understand its internal activities, may alleviate some of the effects of automation bias (Gegoff et al., 2023, Wright et al., 2016).","Severity Justification: Automation bias can lead to significant errors in decision-making, but the text suggests it is mitigatable, indicating moderate severity. | Relevance Justification: Directly addresses automation bias, a cognitive bias related to human-AI interaction, aligning with the research focus on novel AI characteristics.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: automation_bias | Severity: 8","","Automation bias occurs when operators do not search for information disconfirming the proposed solution by the system (Parasuraman and Manzey, 2010). The consequences of automation bias in supervisory control of autonomous CAGA systems are a concern because of the risks associated with erroneously executed avoidance manoeuvres. Automation bias may manifest itself when humans interact with agents designed to aid decision making in complex environments. The tendency to rely on the agent’s decisions makes humans less critical to scrutinise the background information for the proposed solution. Therefore, placing too high trust in the agent’s proposals becomes a problem when the agent is not fully reliable (Bowden et al., 2021, Hutchinson et al., 2022, Lee and See, 2004).","Severity Justification: Automation bias is explicitly linked to risks such as erroneously executed avoidance maneuvers in high-risk supervisory control contexts, indicating significant potential for safety-critical errors and performance degradation. | Relevance Justification: The text directly addresses automation bias as a human performance degradation in the context of autonomous systems, matching the research focus on novel AI characteristics like non-deterministic decision-making and opacity, and it is explicitly mentioned in the priority search for cognitive biases.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: cognitive_overload | Severity: 6","","care should be taken not to overload the user with information. Implementing transparency is “as much an art as it is a science”, and the potential for visual clutter and distraction should be considered as this may potentially offset transparency’s benefits (Wickens, 2018, p. 39). Presenting all information identified in this study, as depicted in Fig. 5, Fig. 6, Fig. 7, Fig. 8, continuously is likely not prudent, and further work should focus on teasing out which information has precedent over other information and in which circumstances (see van de Mer we et al., 2023 for preliminary results).","Severity Justification: The text explicitly warns against overloading users with information and identifies visual clutter and distraction as potential problems that could offset benefits, indicating a moderate risk of cognitive overload. | Relevance Justification: This directly relates to cognitive load issues in human-AI interaction, specifically information overload and interface complexity, which are priority search items for performance degradation in novel AI systems.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: performance_metrics | Severity: 5","","Given the well-known human performance challenges with supervisory control of highly automated systems, finding ways to assist supervisors in performing this task therefore becomes a key focus area going forward.","Severity Justification: The text acknowledges challenges but does not describe their severity or impact, suggesting a moderate issue. | Relevance Justification: Directly mentions human performance challenges related to supervisory control, aligning with the research focus on novel AI characteristics in high-risk industries.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","Category: behavioral_changes | Severity: 5","","changes to the navigators’ role from those performing collision avoidance to those verifying a system performing collision avoidance, including deciding on whether to intervene. Decisions of this kind require the supervisor to perceive the current and anticipated actions of the CAGA system, create a mental model of its behaviour, and evaluate its adequacy in its context. Given the foreseen supervisory role of the hu man in autonomous shipping, these types of decisions are therefore likely to become an increasingly important part of the human’s task repertoire (Banks et al., 2014).","Severity Justification: The shift implies potential skill degradation and increased cognitive load due to new supervisory tasks, but it is presented as a foreseeable change rather than an explicit degradation, with moderate impact. | Relevance Justification: Directly addresses human performance changes related to novel AI/autonomous systems in high-risk industries (maritime), focusing on role changes and decision-making tasks, aligning with the research focus on context-aware/adaptive behavior and human-AI interaction.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: advanced autonomous systems | Evidence Type: direct | Causal Strength: 8 | Performance Effect: introduce new and unknown risks","Although the idea of having a human as a backup to compensate for system limitations is attractive, the introduction of humans in a supervisory position to monitor advanced autonomous systems can introduce new and unknown risks (Ramos et al., 2019, Ramos et al., 2018, Veitch and Alsos, 2022).","Causal Strength Justification: Direct causation is indicated by 'can introduce', which explicitly states that the action (introduction of humans as supervisors) leads to the effect (new and unknown risks). | Relevance Justification: This directly addresses how AI features (autonomous systems) affect human performance by causing risks, though it does not specify non-deterministic, opacity, or adaptive characteristics from the research focus.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: CAGA system's interpretation of parameters | Evidence Type: direct | Causal Strength: 7 | Performance Effect: incorrect speed estimations","An error in CAGA’s interpretation of one of these parameters may result in incorrect speed estimations.","Causal Strength Justification: The text uses 'may result in' to explicitly indicate a potential causal link between the error and the outcome, though it is probabilistic rather than deterministic. | Relevance Justification: The excerpt directly links an AI feature (interpretation error) to a performance effect (incorrect estimations), which is relevant to human-AI interaction in supervision tasks, though it does not explicitly mention human performance beyond the context of system outputs.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: system's avoidance manoeuvre | Evidence Type: direct | Causal Strength: 8 | Performance Effect: supervisor decides to perform an avoidance manoeuvre manually","Alternatively, the system may perform an avoidance manoeuvre that, according to the supervisor, is not deem ed sufficiently safe in terms of distance (Fig. 8, rule 8d), leading the supervisor to decide to perform an avoidance manoeuvre manually.","Causal Strength Justification: Direct causal language 'leading the supervisor to decide to' explicitly connects the system's action (cause) to the human decision (effect). | Relevance Justification: The excerpt directly links an AI system action (an avoidance manoeuvre) to a human performance outcome (a decision to intervene manually), which is relevant to human-AI interaction in high-risk contexts like collision avoidance. However, it does not explicitly tie to the specific novel AI characteristics listed (non-deterministic, opacity, context-adaptive).","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: addition of automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: human's ability to take over manually decreases","The addition of automation adds to the overall reliability and robustness of the system, whilst the human’s ability to take over manually decreases due to reduced SA.","Causal Strength Justification: Direct causal language is used: 'decreases due to reduced SA.' This explicitly links the cause (reduced SA) to the effect (decreased manual takeover ability). The preceding clause also establishes automation addition as the context for this effect. | Relevance Justification: The excerpt directly addresses a causal relationship between an automation/AI system feature (its addition) and a human performance effect (decreased manual takeover ability linked to reduced SA). It aligns with the research focus on human-AI interaction in high-risk contexts, though it does not specify the novel AI characteristics (non-deterministic, opacity, adaptive) mentioned in the prompt.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: agents designed to aid decision making in complex environments | Evidence Type: direct | Causal Strength: 8 | Performance Effect: operators do not search for disconfirming information, become less critical to scrutinise background information, place too high trust in proposals","Automation bias occurs when operators do not search for information disconfirming the proposed solution by the system (Parasuraman and Manzey, 2010). The consequences of automation bias in supervisory control of autonomous CAGA systems are a concern because of the risks associated with erroneously executed avoidance manoeuvres. Automation bias may manifest itself when humans interact with agents designed to aid decision making in complex environments. The tendency to rely on the agent’s decisions makes humans less critical to scrutinise the background information for the proposed solution. Therefore, placing too high trust in the agent’s proposals becomes a problem when the agent is not fully reliable (Bowden et al., 2021, Hutchinson et al., 2022, Lee and See, 2004).","Causal Strength Justification: Direct causal language is used: 'occurs when', 'makes humans less critical', 'becomes a problem when'. The relationship is explicitly stated, not implied. | Relevance Justification: Directly addresses how AI/autonomous systems (agents) affect human performance through automation bias, trust issues, and reduced critical scrutiny, which aligns with the research focus on human-AI interaction in high-risk contexts.","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: agent transparency (insight into reasoning) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: alleviation of automation bias effects","However, some have suggested that insight into the agent’s reasoning, allowing a supervisor to understand its internal activities, may alleviate some of the effects of automation bias (Gegoff et al., 2023, Wright et al., 2016).","Causal Strength Justification: Direct causal language ('may alleviate') connects transparency to reduced automation bias, though qualified as 'suggested'. | Relevance Justification: Directly addresses how transparency (countering opacity) affects human performance (automation bias).","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: automation transparency | Evidence Type: direct | Causal Strength: 8 | Performance Effect: improved operator performance (e.g., decision making) and situational awareness","Several recent reviews have investigated the relation between automation transparency and typical human factors variables, demonstrating a promising effect in terms of improved operator performance variables (e.g., decision making) and SA without the added cost of mental workload (Bhaskara et al., 2020, van de Mer we et al., 2022).","Causal Strength Justification: Direct causal language ('demonstrating a promising effect in terms of improved') links transparency to performance improvements, supported by reviews. | Relevance Justification: Explicitly connects transparency (countering opacity) to multiple human performance variables (decision making, SA, workload).","y"
"Supporting human supervision in autonomous collision avoidance through agent transparency - ScienceDirect","","","AI Feature: agent transparency | Evidence Type: direct | Causal Strength: 7 | Performance Effect: increased trust and reliance","Also, in a recent study with navigators using a tool to perform collision avoidance manoeuvring, agent transparency was suggested as a means to increase trust and reliance in the technology (Aylward et al., 2022).","Causal Strength Justification: Direct causal language ('as a means to increase') links transparency to improved trust/reliance, though based on a study suggestion. | Relevance Justification: Directly connects transparency (addressing opacity) to human performance outcomes (trust and reliance).","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Semi-autonomous and autonomous operation","","","These systems may include intelligent assistants that have the potential to function semi-autonomously, or even autonomously, in collaboration with human crews and teams.","This directly describes a key AI feature - the ability to function with varying levels of autonomy (semi-autonomous or fully autonomous), which distinguishes advanced AI systems from conventional automation that typically operates with fixed, predetermined rules.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI surpassing human capabilities","","","An example of early AI is the ‘Bombe’ machine [ 22], used to break German ‘enigma’ codes used in the second world war. Such codes were unbreakable by humans, and so the ‘Bombe’ machine did indeed surpass our capabilities.","This excerpt directly discusses an AI system's capability to perform tasks that humans cannot, which is a core characteristic of AI systems versus conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Data-driven problem-solving beyond human intellect","","","The general aim of artificial intelligence (AI), therefore, is currently seen as supporting human intelligence, and society, by using data science techniques to analyse complex datasets to find new patterns or solutions to problems that are beyond our own intellectual capabilities.","This directly describes AI's capability to use data science techniques on complex datasets to find new patterns or solutions that exceed human intellectual capabilities, which aligns with the research focus on novel AI characteristics in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Rule-based expert systems","","","In the 1980s, there was another surge in AI interest via (rule-based) expert systems, which ultimately failed to deliver operationally useful tools. This was in part due to their inability to account for the experience-based and highly contextual ‘tacit knowledge’ that human operators amass, which often far exceeds what is written in procedures.","This represents an AI feature as it explicitly mentions 'AI interest via (rule-based) expert systems' and discusses their operational failure due to limitations in handling contextual human knowledge, which relates to AI capabilities versus conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: non_deterministic | Feature: Unpredictable and variable outputs","","","Whether the answer makes sense is up to the user, and sometimes it produces answers that are inaccurate, or so bizarre they are referred to as hallucinations.","This represents non-deterministic/data-driven decision-making as it describes variable outputs (inaccurate or bizarre answers) that depend on the AI's model states, leading to uncertainty and unpredictable failures (hallucinations).","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI Cognitive Categories and Applications","","","Cognition categories found in AI are typically learning, perception, reasoning, communication, and knowledge representation. Common AI applications include expert systems, machine learning, robotics, natural language processing, machine vision, and speech recognition [29].","This represents an AI feature by detailing specific cognitive functions (e.g., learning, perception) and applications (e.g., machine learning, robotics) that are characteristic of AI systems, as opposed to traditional automation, which may lack such adaptive and data-driven capabilities.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: narrow AI capabilities and limitations","","","Rather than generative AI, what aviation at least initially requires is narrow AI [ 30]. Narrow AI can solve speciﬁc problems in a domain but cannot generalize as broadly as humans can. Such systems (sometimes called idiot savants ) can be superhuman at some tasks, and subhuman at others.","This excerpt directly discusses AI system features, specifically the capabilities and limitations of narrow AI, including its problem-solving scope, lack of generalization, and performance variability, which relates to novel AI characteristics like non-deterministic behavior and context-specific adaptation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Data-driven problem-solving and optimization","","","An AI tool can then answer speciﬁc questions or ﬁnd solutions to problems, or simply show how to optimize system performance based on a limited set of parameters for which there are plentiful data.","This excerpt directly describes AI system features: it can answer questions, find solutions, and optimize performance, which are core AI capabilities as opposed to conventional automation that typically follows fixed rules without such adaptive problem-solving.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: automation | Feature: Shift from automation to generative AI","","","Such AI tools can be seen as ‘just more automation’ [ 27], and their impact on safety culture might therefore be expected to be minimal. However, this understanding of AI, as effectively a more powerful automation tool support, shifted dramatically with the release of ChatGPT in 2022 [28], heralding generative AI.","This excerpt addresses the evolution of AI from conventional automation to novel generative capabilities, relevant to comparing AI features vs. automation in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Generative AI and conversational interface","","","ChatGPT is effectively a large language model (LLM), using the entire internet as its database, which sits behind a ‘chatbot’. This chatbot enables a human user to have a ‘conversation’ (via the keyboard) on a vast range of issues. It is reminiscent of Turing’s ‘imitation game’ challenge to develop thinking machines.","This excerpt details the capabilities of generative AI, including data-driven decision-making and human-AI interaction through conversation, which are novel characteristics compared to traditional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Unfathomable complexity and lack of thinking","","","Machine learning can analyse very large, complex and heterogeneous datasets in ways the human mind ﬁnds difﬁcult or impossible (e.g., via n-dimensional analysis). So far, such AI tools, though yielding impressive results, do not constitute thinking; they are still computing machines that are ‘running the numbers’, albeit in very complex and often unfathomable ways.","This excerpt highlights the opacity of AI tools, as they operate in 'unfathomable ways' and lack explainability, aligning with the research focus on black-box behavior and trust challenges.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Advanced AI collaboration and negotiation","","","Next, it is useful to consider contemporary visions of future AI concepts, some of which go beyond today’s machine learning tools, leading to humans collaborating and negotiating with advanced AI systems.","This excerpt highlights advanced AI capabilities that go beyond conventional automation, specifically mentioning concepts that exceed current machine learning tools and involve human-AI collaboration and negotiation, which aligns with novel AI characteristics in human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: interface | Feature: Human-AI teaming with dialogue","","","This level of interaction and collaboration between humans and AI, often supposed to involve some kind of dialogue, has led to the term human-AI teaming.","This excerpt describes a feature of AI systems involving interaction and collaboration with humans through dialogue, which represents a novel interface characteristic compared to traditional automation, as it emphasizes teaming and communication.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Dynamic environment response","","","UC2—a cockpit IA used to help ﬂight crew re-route an aircraft to a new airport destination due to deteriorating weather or airport closure, for example. The IA must consider a large number of factors including category of aircraft, runway length,","This excerpt demonstrates context-aware/adaptive behavior as the IA responds to real-time changes (deteriorating weather or airport closure) and considers various contextual factors (aircraft category, runway length) to adapt its decision-making, aligning with the novel AI characteristic of dynamic environment response.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: executive_agent_with_human_oversight","","","The AI is an executive agent with a human overseer and is handling most of the trafﬁc,","This describes the AI's role as an executive agent handling traffic, indicating advanced AI capabilities in autonomous decision-making and coordination, with human oversight for safety and intervention.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Collaborative autonomous agent with initiative and negotiation","","","2B— Collaborative agent—an autonomous agent that works with human colleagues, but which can take initiative and execute tasks, as well as being capable of negotiating with its human counterparts;","This excerpt directly describes an advanced AI capability where the agent operates autonomously, takes initiative, and engages in negotiation with human counterparts, representing a novel AI characteristic beyond conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: control | Feature: AI executive agent with human oversight","","","3A— AI executive agent—the AI is basically running the show, but there is human oversight, and the human can intervene (sometimes called management by exception);","This excerpt describes an AI system feature where the AI has primary control ('running the show') but includes human oversight and intervention mechanisms, representing a specific control structure in human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: control | Feature: Fully autonomous AI without human intervention","","","3B—the AI is running everything, and the human cannot intervene.","This excerpt describes the highest level of AI autonomy where the system controls everything without human intervention capability, representing a key AI feature in automation levels.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: automation | Feature: task automation with human oversight","","","UC4—a digital assistant for remote tower operations, to ease the tower controller’s workload by carrying out repetitive tasks. The human monitors the situation and will intervene if there is a deviation from normal (e.g., a go-around situation, or an aircraft that fails to vacate the runway).","This excerpt highlights an AI feature involving automation of repetitive tasks, which is a characteristic of AI systems in high-risk industries, though it does not explicitly mention novel aspects like non-deterministic behavior or opacity.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Data-driven analysis and real-time adaptation","","","UC5—a digital assistant to help airport safety staff deal with difﬁcult incident patterns that are hard to eradicate, using data science techniques to analyze large, heterogeneous datasets. At the moment, this is a retrospective analysis approach, though if effective it could be made to operate in real-time, warning of impending incident occurrence or hotspots.","This excerpt mentions a digital assistant using data science techniques to analyze large, heterogeneous datasets, which aligns with AI capabilities involving data-driven decision-making and potential real-time adaptation, distinguishing it from conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Neural network-based physiological monitoring","","","The IA detects startle via a trained neural network that dynamically analyses a host of physiological parameters of the pilot, such as breathing rate, heart rate, skin conductance, etc., to determine whether startle has occurred.","This describes an AI capability using a neural network for data-driven analysis of physiological parameters, which is a characteristic of advanced AI systems versus conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Context-aware intermittent AI support","","","In the tower controller cognitive assistant scenario, an early study with a group of controllers suggests it is perceived as most useful when very busy, or when the work becomes complex. This again raises a more general issue concerning intermittent AI support systems, which are turned on only when needed.","This excerpt directly mentions an AI cognitive assistant that adapts its usefulness based on workload complexity and busyness, and describes intermittent AI systems that respond dynamically to need, aligning with context-aware/adaptive behavior characteristics.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: control | Feature: Human Override Capability","","","the pilots are still very much in control and ﬂying the plane and can switch off the AI at any moment.","This represents an AI feature because it specifies a control mechanism where human operators have the ability to override or disable the AI system, which is a critical aspect of human-AI interaction in safety-critical environments, ensuring that the AI does not operate autonomously without human supervision.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: adaptation | Feature: Tailoring AI to Individual Needs","","","The operators (i.e., airlines) may need to ﬁne-tune future AI support systems to individual pilots, to ensure smooth and ﬂuent performance in crisis situations.","This represents an AI feature because it describes the need for AI systems to be adapted or fine-tuned to individual human operators, highlighting a context-aware or adaptive characteristic where the system's behavior is tailored based on user-specific factors, which contrasts with conventional one-size-fits-all automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI autonomy levels and human-AI interaction friction","","","One study aiming for more extensive AI autonomy (2B) is attempting a co-design approach, and the pilots are currently more interested in categories 1B and 2A, believing these to be sufﬁcient. This raises a more general potential design friction issue with advanced AI, as humans may be reluctant to cede too much autonomy to an AI, for fear of replacement or other issues.","This excerpt directly mentions AI autonomy (2B, 1B, 2A) and human reluctance to cede autonomy, which relates to AI capabilities and interaction challenges in high-risk industries, as specified in the research focus on novel AI characteristics.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human-machine intelligence collaboration","","","This was an instance where instead of machine intelligence, there was human–machine intelligence, since neither alone could detect the pattern.","This excerpt highlights a key AI feature: the integration of human and machine intelligence to achieve a task (pattern detection) that neither could accomplish alone, reflecting a novel characteristic in AI systems compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Advanced AI concepts for aviation","","","Other advanced AI concepts include [ 34] digital assistants to help air traffic control provide more efficient and environmentally friendly (‘greener’) routes, advanced warning in the cockpit of impending flight instability, and digital help for evidence-based training to enhance performance during adverse events.","This directly mentions 'advanced AI concepts' and specific AI applications (digital assistants, advanced warning systems, digital training help) that represent AI capabilities beyond conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety enhancement capability","","","Rather than degrading or eroding safety, AI could therefore possibly enhance safety, offering new safety affordances.","This directly describes an AI capability - the potential to enhance safety and offer new safety affordances, which is a characteristic of advanced AI systems compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Lack of autonomous task management and initiative","","","There are currently no AI systems in aviation that autonomously share tasks with humans, or can negotiate, make trade-offs, change priorities, or start and execute tasks under their own initiative.","This directly describes specific AI capabilities (or lack thereof) that are relevant to human-AI interaction, including autonomous task sharing, negotiation, trade-offs, priority changes, and self-initiated task execution—key features distinguishing AI from conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Reliance on AI tools","","","If an AI tool is useful and meant to help aviation professionals, they will become, to an extent, reliant on it, and such reliance may reduce their own situational awareness.","This represents an AI feature as it discusses the impact of AI tool usage on human behavior and skill degradation in a high-risk industry, highlighting a novel characteristic compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Opacity and lack of transparency in AI systems","","","the question becomes one of whether there is transparency in terms of the equivalent of the AI’s algorithms and calculations made, its data—both used and unused—and its trade-offs, if any were made between different priorities, including safety. Such data forensics may prove inconclusive because of the innate complexity and opacity of how advanced AIs work.","This directly addresses the opacity/explainability characteristic from the research focus, highlighting the black-box nature of AI systems, challenges in understanding their internal reasoning, and issues with transparency in algorithms and data usage.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Autonomy levels in AI systems","","","a more autonomous AI system, which could initiate and execute tasks on its own, would have a higher safety certification requirement than a machine learning system simply advising a controller on weather pattern formation, since in the latter the human is more involved and in command.","This excerpt directly discusses AI system characteristics related to autonomy levels, which is a key feature distinguishing AI from conventional automation. It specifically mentions autonomous task initiation/execution versus advisory roles, which aligns with examining novel AI characteristics in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI agency and human intervention","","","In both cases the AI could hypothetically be considered to have a certain degree of agency. It is then a question of whether the human can detect erroneous AI behavior (or conditions outside the AI’s ‘competence’ or datasets) and intervene in time.","This represents an AI feature as it directly addresses AI's autonomous decision-making capability (agency) and the human oversight required to manage it, which is a key characteristic of novel AI systems versus conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Autonomous AI action with human collaboration or oversight","","","AI categories 2B and 3A, wherein the AI can act autonomously, either in collaboration with the human (2B) or under a human overseer/management-by-exception operational framework (3A), are the ‘ones to watch’ from a Just Culture perspective.","This excerpt directly mentions AI's capability to act autonomously, which is a key feature distinguishing AI from conventional automation, as it involves decision-making without direct human control in certain contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: automation | Feature: AI taking on larger safety role","","","But what if the system, through increasingly effective automation and AI, becomes ultra-safe? There is a concern that if people are effectively ‘closed out’ from safety, either via automation that excludes human intervention, or because it is simply ultra-safe, then ‘safety citizenship’— the innate desire to keep things safe for ourselves and others—may degrade or disappear altogether [42]. Seven factors can erode safety citizenship [ 42], all of which could be affected by AI taking on a larger share of the safety role, or occupying the ‘safety space’:","This excerpt explicitly mentions AI taking on a larger share of the safety role, which is a key AI capability in high-risk industries, as it highlights AI's increasing involvement in safety-critical functions compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: fairness and accountability","","","Fairness: This principle links to solidarity and justice, including redress against decisions made by AI or the companies operating/making them.","This describes an AI capability requirement related to fairness and accountability for decisions, representing ethical considerations that go beyond the technical capabilities of conventional automation systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: harm prevention and security","","","Prevention of harm: AI must not cause harm or adversely affect humans, and should protect human dignity, and not be open to malicious use or adverse effects because of information asymmetries or unequal balance of power;","This represents an AI capability requirement focused on safety and security considerations, particularly addressing risks from information asymmetries and power imbalances that are more complex than conventional automation safety concerns.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: human augmentation and oversight","","","AI systems should augment, complement and empower human cognitive, social and cultural skills, leave opportunity for human choice and secure human oversight over work processes, and support the creation of meaningful work;","This describes AI capabilities focused on enhancing human skills while ensuring human control, representing a feature of advanced AI systems that goes beyond simple automation to collaborative human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: autonomous decision-making","","","the AI becomes its own autonomous decision-maker.","This represents a key AI capability where the system makes decisions independently, distinguishing it from conventional automation that typically follows predetermined rules or requires human approval.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Anthropomorphism and Human-like Interaction","","","Human-AI teaming is itself an anthropomorphic term [ 27], conveying the notion that the AI is in some sense a team player, devolving human qualities to a machine. This is reminiscent of generative AI systems wherein people sometimes believe they are conversing with a person rather than a program (e.g., ChatGPT).","This represents an AI feature as it highlights the anthropomorphic perception of AI systems, where they are viewed as having human qualities like teamwork, which is a novel characteristic compared to conventional automation that typically lacks such human-like attributes, impacting trust and interaction in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Emotional awareness in AI for human support","","","However, there is a reciprocal question concerning whether AIs need to be aware of human emotions. Would it make sense, for example, for AIs supporting humans in an emergency to be aware of stress in the humans’ voices as conditions worsen?","This represents a context-aware/adaptive AI feature, as it involves AIs dynamically responding to human emotional states (e.g., stress in voices) in real-time environments like emergencies, highlighting variability and adaptation to human conditions.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Impact on human workload and team performance","","","Additional critical considerations include how the AI affects the human team members’ workload, and whether it has an overall positive impact on the team’s performance.","This highlights AI features related to its interaction effects (on human workload and team performance), which are important aspects of AI capabilities in team settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Task effectiveness","","","What will matter to the human members of the team and the executives deciding whether to deploy AI, is the effectiveness of the AI in doing its tasks.","This directly describes an AI capability (effectiveness in tasks) that is evaluated for deployment decisions, which is a general AI feature relevant to human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Transparency requirement for AI systems","","","The ﬁrst is that AI systems must be sufﬁciently transparent to enable users to interpret the system’s output and use it appropriately.","This represents an AI feature because it directly addresses the opacity/explainability characteristic, highlighting the need for transparency to mitigate black-box behavior and enhance user trust and regulation compliance.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Emotion recognition system","","","Interestingly, the provisional agreement intends to ban, for example, cognitive behavioral manipulation and emotion recognition in the workplace. However, it also states that there will be an obligation for users of an emotion recognition system to inform natural persons when they are being exposed to such a system.","This represents an AI feature as emotion recognition systems are AI-driven technologies that analyze human emotions, often using data-driven models, which aligns with the research focus on AI characteristics in high-risk industries like workplace settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Increasing invisible interactions","","","AI can lead to ‘ increasing invisible interactions’ . In such a case, the humans miss what is going on in terms of the system and sub-system interactions","This represents opacity/lack of explainability as it describes AI causing interactions that are invisible to humans, preventing them from understanding system complexity, which aligns with black-box behavior and trust challenges.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI inability to handle low-probability events (tail effects)","","","This is risky, as already identified in the maritime industry [48], since there can be ‘tail effects’, wherein low probability events are impractical to train AIs on, so that when they occur the AI cannot handle them.","This excerpt describes a specific AI feature: the limitation in handling rare or unpredictable events ('tail effects') due to training impracticalities, which is a key characteristic of data-driven AI systems in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI capability overestimation leading to reduced human control","","","However, if for example, AI’s capability is overestimated, such that human error is perceived as the problem and AI the solution, then the industry may work towards reducing human control inside the ‘safety space’, putting the safety of passengers and crews in the metaphorical hands of AI systems.","This excerpt directly addresses AI's capability as a feature that can be overestimated, influencing industry decisions to reduce human control in safety spaces, which relates to AI's role and perceived reliability in high-risk settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Inability to value like humans","","","Perhaps one thing CEOs need to know is that AIs cannot value things in the way humans can, especially safety, as currently it is not known how to program human values [30].","This directly describes a limitation of AI systems compared to human capabilities, specifically their inability to value things (especially safety) due to the unknown nature of programming human values, which relates to AI characteristics in decision-making and alignment.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Lack of emotional experience and mimicry-based behavior","","","An AI cannot experience any of these, and while various reward schemes and supervised learning could in theory reinforce safety in the machine’s workings, it will still be ‘running the numbers’, and if it gets them wrong, will experience neither remorse nor regret. Whilst AIs can mimic human behavior and even have a built-in ‘persona’, this remains mimicry; they are still machines, or simply ‘just more automation’ [ 27].","This excerpt highlights AI's inability to experience human emotions like remorse or regret, which underpins safety values, and its reliance on reward schemes and supervised learning for safety reinforcement, distinguishing it from conventional automation by emphasizing its data-driven, non-emotional nature and mimicry capabilities.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI difficulty with human-easy tasks","","","The maritime study authors also point out that in maritime operations managing VHF comms are easy for humans and hard for AI: part of the ‘ easy things are hard ’ paradox in AI.","It represents an AI feature by discussing AI's limitations in handling tasks that are straightforward for humans, which is a characteristic of AI systems in human-AI interaction contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Strategic AI deployment","","","The Defense AI Strategy, signiﬁcantly, also poses a set of questions around when to use AI, and when not to, which sometimes appear missing in the current rush to ‘try out AI’ in a myriad of projects in several industries, including aviation: Where is AI the right solution?","It represents an AI feature by focusing on the decision-making process for AI implementation, which is essential for managing AI capabilities and ensuring effective human-AI interaction in various domains.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Organizational AI readiness","","","The paper asks whether the defense industry has the right culture, leadership, policies, and skills in place to make the best use of AI, which it considers it must develop to counter signiﬁcant foreign threats now and in the future.","It represents an AI feature by addressing the systemic and organizational aspects required to implement and benefit from AI technologies in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human judgment in AI systems","","","“Machines are good at doing things right; humans are good at doing the right thing. ” Such a statement clearly shows that human judgement will continue to be valued in future AI-enhanced defense platforms and scenarios.","It represents an AI feature by highlighting the integration of human judgment with AI capabilities, which is a key aspect of human-AI interaction and automation levels.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Digital colleague with performance-based safety judgment","","","The IA would effectively be a digital colleague. The IA’s commitment to safety would likely be judged according to the IA’s performance.","This excerpt highlights an AI capability where the IA functions as a digital colleague, with its safety commitment assessed through performance, reflecting a novel AI characteristic in human-AI interaction compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Rule flexibility decision-making","","","The designer needs to decide whether to ‘hard code’ some of these rules or allow a little leeway (within limits); this determines whether the IA behaves like ‘one of the guys’ or never, ever breaks rules.","This represents an AI feature as it involves decision-making about rule implementation in an IA system, highlighting its potential for rigid or flexible behavior, which relates to AI capabilities in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Evidence-based assessment capability","","","the IA may be seen as adding dispassionate evidence and more balanced assessment of severity, and how close an event came to being an accident (e.g., via Bayesian and other statistical analysis techniques).","This excerpt describes specific AI capabilities - providing dispassionate evidence, balanced assessment of severity, and using statistical analysis techniques (Bayesian methods) to evaluate events, which represents AI's analytical capabilities.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Complex operating characteristics","","","the complexity of IA’s and human-AI teams’ operating characteristics and the local rationality at the time.","This excerpt directly mentions AI systems (IA) and their operating characteristics, which relates to AI capabilities and how they function in complex environments with human teams.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety knowledge base","","","The IA could serve as a ‘safety encyclopedia’ for its team, with all safety rules, incidents and risk models stored in its knowledge base.","This represents an AI feature where the IA has a knowledge base for safety information, enabling context-aware support and data-driven decision-making in high-risk environments.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human-trained bias and re-training","","","However, there is a secondary aspect, linked to B07, that the IA may be trained by humans, and may be biased by their own level of risk tolerance and safety–productivity trade-offs. If an IA is offering solutions judged too risky, or conversely ‘too safe’, nullifying operational efﬁciency, the IA will need ‘re-training’ or re-coding.","This describes an AI feature where the IA's behavior is influenced by human training and biases, and it can adapt through re-training, highlighting its data-driven and potentially non-deterministic nature.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety value and trust impact","","","The perceived safety value of IAs will depend on how useful the IA is for safety and will be a major question for the HAIKU use cases. One ‘wrong call’ could have a big impact on trust.","This highlights an AI feature related to trust and reliability, where the IA's performance in safety-critical decisions can influence its perceived value, touching on non-deterministic outcomes.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: control | Feature: Adjustable safety control","","","They may come to see the IA as a more manageable asset than people, one that can be ‘turned up or down’ with respect to safety.","This describes an AI feature where the IA's safety parameters can be controlled and adjusted, indicating adaptive and configurable behavior in operational settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Enhanced safety reporting and tracking","","","the IA could significantly increase reporting rates, depending on how its reporting threshold is set, and record and track how often a safety issue is raised.","This describes an AI capability to dynamically influence and monitor safety reporting processes, which is more adaptive and data-driven than static reporting systems in conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Continuous monitoring and evidential recording","","","the IA could monitor and record all events and interactions in real time and would be akin to a ‘living’ black box recorder. This could affect how humans behave and speak around the IA, if AI ‘testimony’ via data forensics was ever used against a controller in a disciplinary or legal prosecution case.","This highlights an AI feature of real-time, comprehensive data collection and potential use in forensic analysis, which goes beyond traditional automation's limited logging capabilities and introduces new ethical and operational considerations.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Proactive safety voicing and integration","","","the IA could ‘speak up’ if a key safety concern is not being discussed or has been missed. This could be integrated into crew resource management (CRM) and threat and error management (TEM) practices, and team resources management (TRM) in air traffic management.","This describes an AI capability to autonomously identify and communicate safety issues, which is a novel feature compared to conventional automation that typically follows predefined rules without such proactive communication.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Non-confrontational safety querying","","","the IA can ‘query’ behavior or decisions that may be unsafe. Rather than ‘policing’ the human team, the IA could possibly bring the risk to the human’s attention more sensitively, as a query.","This represents an AI feature of context-aware interaction, where the system can assess and communicate risks in a nuanced way, differing from rigid rule-enforcement in traditional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human factor insight with safeguards","","","the IA’s record of events could shed light on the human colleagues’ states of mind and decision-making. There needs to be safeguards around such use, however, so that it is only used for safety learning.","This illustrates an AI capability to analyze and interpret human behavior from recorded data, which is a sophisticated feature not typically found in conventional automation, and underscores the need for ethical frameworks in its application.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Continual learning and behavioral evolution","","","If the IA is a continual learning system, its behavior may evolve over time, and diverge from optimum, even if it starts off safe when first implemented.","This represents an AI feature as it describes context-aware/adaptive behavior where the IA dynamically responds and evolves based on learning, leading to variability and unpredictability in its actions over time.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: non_deterministic | Feature: Risk judgment influenced by data quality","","","Alternatively, if information is biased or counterfactual evidence is not considered, the way the IA judges risk may be incorrect, leading to a lack of trust by operational people.","This represents a non-deterministic/data-driven AI feature because it discusses how IA's risk judgments depend on the quality and completeness of input data (information bias, counterfactual evidence), leading to variable and potentially unpredictable outcomes that affect trust, which relates to uncertainty and data-driven decision-making.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: non_deterministic | Feature: Statistical decision-making for bias avoidance","","","Use of Bayesian inference and other similar statistical approaches could avoid some typical human statistical biases, to help ensure the right lessons are learned and are considered proportionately to their level of risk.","This represents a non-deterministic/data-driven AI feature because it involves statistical approaches (Bayesian inference) that rely on data and model states to make decisions, reducing human biases and handling uncertainty in risk assessment, which aligns with variable outputs and model-dependent decisions.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: lack of explainability","","","Conversely, the IA may give advice that makes little sense to the human team, or the organization yet be unable to explain its rationale. Humans may ﬁnd it difﬁcult to adhere to such advice.","This directly describes an AI system characteristic where the AI's decision-making process is opaque ('unable to explain its rationale'), which aligns with the research focus on opacity/lack of explainability in AI systems versus conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Model-based safety analysis","","","The IA could have a model of how things work and how safety is maintained, so any changes will need to be incorporated into that model, which may identify safety issues that may have been overlooked or played down.","This highlights an AI feature where the IA uses a model to represent system operations and safety, enabling it to assess changes and identify risks, reflecting AI's role in dynamic and context-aware risk management.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Information assimilation","","","If an IA becomes an assimilator of all safety relevant information and activities, it may become clearer how different roles contribute to safety.","This describes an AI capability where the IA integrates and processes safety-related data, potentially enhancing understanding of safety roles, which aligns with AI's data-driven and integrative functions in high-risk contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Real-time Operational Support","","","The IA could again be an outlet for information sharing, e.g., notices could be uploaded instantly, and the IA could ‘brief’ colleagues or inject new details as they become relevant during operations. The IA could also upload daily NOTAMs (Notices to Airmen) and safety brieﬁngs for controllers, and could distill the key safety points, or remind the team if they forget something from procedures/NOTAMs/brieﬁngs notes.","This represents a context-aware/adaptive AI feature, as it involves dynamic response to operational needs by instantly updating and injecting relevant details, distilling key points, and providing reminders, enabling adaptation to changing conditions in real-time, unlike static conventional systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Dynamic Risk Model Adaptation","","","If the IA incorporates a dynamically updated risk model, concerns about safety could be rapidly assessed and addressed according to their risk importance (this is the long-term intent of Use Case 5 in HAIKU).","This represents a context-aware/adaptive AI feature, as it involves real-time updates to a risk model to respond dynamically to safety concerns, enabling rapid assessment and prioritization based on changing conditions, which contrasts with static conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety Information Sharing","","","The IA could become a source of safety information sharing, but this would still depend on the organization in terms of how the information would be shared and with whom. The IA could however share important day-to-day operational observations, e.g., by ﬂight crew who can pass on their insights to the next crew ﬂying the same route, for example, or by ground crew at an airport. Some airports already use a ‘community app’ for rapid sharing of such information.","This represents a general AI capability for enhancing communication and information dissemination in safety-critical contexts, enabling dynamic sharing of insights and observations, which goes beyond basic automation by integrating into organizational workflows for improved situational awareness.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Automated Safety Reporting","","","An IA could reduce the reporting burden of operational staff if there could be an IA function to transmit details of concerns and safety observations directly to safety departments (though the ‘narrative’ should still be written by humans). An IA ‘network’ or hub could be useful for safety departments to assess safety issues rapidly and prepare messages to be cascaded down by senior/middle management.","This represents a general AI capability for automating and streamlining safety processes, reducing human burden and enabling faster issue assessment, which enhances efficiency and responsiveness in high-risk environments compared to manual conventional methods.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Objective Safety Analysis","","","The IA could provide useful and objective input for safety investigations, including inferences on causal and contributory factors.","This represents a general AI capability for providing data-driven, objective insights into safety incidents, aiding in analysis and decision-making, which leverages AI's potential for unbiased assessment beyond simple automation tools.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: black-box recording","","","The IA could serve as a living black box recorder, recording more decision-making rationales than is the case today.","This directly references the 'black box' nature of the IA, highlighting opacity and lack of explainability in its recording of decision-making rationales, which aligns with the research focus on opacity/lack of explainability in AI systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Increased autonomy leading to complacency","","","Humans may become less concerned with safety if the IA is seen as handling safety aspects. This is an extension of the ‘complacency’ issue with automation and may be expected to increase as the IA’s autonomy increases.","This directly mentions an AI feature (IA's autonomy increasing) and its impact on human behavior, contrasting with conventional automation by highlighting a heightened risk as autonomy grows.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: feedback | Feature: Safety feedback mechanism","","","The IA could ‘speak up’ if it assesses a human course of action as unsafe.","This represents a novel AI characteristic where the system actively assesses and communicates about safety, going beyond passive automation to enable human-AI interaction in risk management.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: lack of explainability","","","The IA may give (good) advice that makes little sense to the human team or the organization, yet it cannot explain its rationale.","This directly describes an AI system (IA) that cannot explain its rationale for advice, which aligns with the opacity/explainability characteristic of novel AI systems, where internal reasoning is not visible or interpretable.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Evidence-based procedure assessment","","","The IA could build up evidence of procedures that regularly require workarounds or are no longer fit for purpose.","This represents a general AI capability where the system builds up evidence over time to identify procedural issues, demonstrating data-driven analysis and learning from operational experiences.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Contextual risk assessment bridging","","","The IA might serve as a bridge between the way operational people and safety analysts think about risks, via considering more contextual factors not normally encoded in risk assessments.","This demonstrates context-aware/adaptive behavior as the AI considers additional contextual factors beyond standard risk assessments, enabling it to bridge different perspectives and adapt to complex risk environments.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Context-aware risk modeling and real-time warning dissemination","","","IAs could store information on incidents and associated (correlated) contextual factors, with live updates structured around risk models, and disseminate warnings of potential hazards on the day via an app or via the IA itself communicating with crews/staff.","This represents context-aware/adaptive behavior as the AI dynamically responds to incidents and contextual factors, uses live updates structured around risk models, and adapts communication methods (app or direct) to disseminate warnings in real-time.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: Rapid sensor examination and action supply","","","When humans ﬁnd themselves outside the procedures, e.g., in a ﬂight upset situation in the cockpit, an IA could rapidly examine all sensor information and supply a course of action for the ﬂight crew.","This represents a context-aware/adaptive AI feature, as the IA dynamically responds to real-time sensor information in unpredictable situations (flight upsets), enabling it to supply courses of action, which is characteristic of novel AI systems versus static conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human-supervisory training requirement","","","The AI’s advice might not be so helpful unless it is human-supervisory trained.","This represents an AI feature related to its capability and interaction with humans, highlighting the need for training under human supervision to ensure effectiveness, which contrasts with conventional automation that might operate independently.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety information oracle","","","The IA could serve as a safety encyclopedia, or oracle, able to give instant information on safety rules, risk assessments, hazards, etc.","This represents an AI feature where the system acts as an intelligent knowledge repository with rapid information retrieval capabilities.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Operational information management and advisory","","","The IA can upload all NOTAMs and brieﬁngs etc., so as to keep the human team current, or to advise them if they have missed something.","This demonstrates AI capabilities for information processing, monitoring, and providing context-aware advice to human operators.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Operational observation sharing","","","The IA could share important day-to-day operational observations, e.g., by ﬂight crew, controllers, or ground crew, who can pass on their insights to the incoming crew.","This represents an AI feature for information aggregation and dissemination across different human operators and contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: automation | Feature: Automated safety reporting transmission","","","The IA could reduce the reporting 'burden' of operational staff by transmitting details of human concerns and safety observations directly to safety departments.","This demonstrates AI automation capabilities for streamlining safety reporting processes and reducing human workload.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Configurable reporting and tracking","","","The IA could signiﬁcantly increase reporting rates, depending on how its reporting threshold is set, and could also record and track how often a safety-related issue is raised.","This demonstrates AI capabilities for data-driven decision-making (reporting thresholds) and automated monitoring/tracking of safety issues.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Human-trained risk tolerance adoption","","","An IA that is human trained may adopt its human trainers’ level of risk tolerance, which may not always be optimal for safety.","This represents an AI feature where the system's behavior (risk tolerance) is learned from human data/training, making it potentially non-deterministic and dependent on training data quality.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Objective safety investigation analysis","","","The IA could provide objective input for safety investigations, including inferences on causal and contributory factors.","This represents advanced AI capabilities for data analysis, inference generation, and providing objective input for complex safety investigations.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Programmable safety communication","","","The IA could also be programmed to ‘ speak up’ for safety if warranted, and this can be embedded into human CRM and TRM practices and training.","This represents an AI feature as it describes the IA's ability to be programmed for specific safety-related actions, such as speaking up, which is a novel capability compared to conventional automation that typically follows fixed rules without adaptive communication.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety reporting assistance","","","The IA could be a useful aid for safety reporting, able to rapidly capture events, their precursors, signals and actions, to which the human could then add a narrative.","This represents an AI feature as it highlights the IA's capability to rapidly process and capture complex safety-related data (events, precursors, signals, actions), which is a data-driven function that enhances human-AI collaboration in high-risk contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Contextual Briefing Tool","","","Similarly, the IA could be useful as a day-to-day briefing tool, letting the oncoming shift know of anything unusual, e.g. changes to procedures, or the status of ongoing maintenance, etc. that has happened on previous shifts.","This demonstrates an AI capability for context-aware information dissemination, supporting dynamic environment response and human-AI interaction in operational settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Automated Reminder System","","","NOTAMs (Notices to Airmen) could be automatically uploaded into the IA, which could remind human crews if they have forgotten or overlooked any relevant aspects during operations.","This represents an AI capability involving automated data processing and proactive human assistance, distinguishing it from conventional automation by potentially adapting to human oversight needs.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Living black box with post-event explainability","","","A first safeguard is the notion of the IA serving as a living black box, such that after an event the IA could reproduce the detailed flow of events, signals, interactions, decisions made and even the thinking underpinning those decisions, prior to and during the incident.","This represents an AI feature related to opacity/explainability, as it discusses the IA's ability to make its internal reasoning and decision-making process transparent after events, which is a key characteristic for trust and regulation in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Shift briefing and information dissemination","","","Similarly, the IA could be useful as a day-to-day briefing tool, letting the oncoming shift know of anything unusual, e.g. changes to procedures, or the status of ongoing maintenance, etc. that has happened on previous shifts.","This describes an AI capability to manage and communicate shift-related information, facilitating continuity and safety in operations.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Operational reminder system","","","NOTAMs (Notices to Airmen) could be automatically uploaded into the IA, which could remind human crews if they have forgotten or overlooked any relevant aspects during operations.","This shows an AI feature of integrating external data and providing timely reminders, enhancing operational safety and awareness.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Proactive safety communication","","","The IA could also be programmed to ‘ speak up’ for safety if warranted, and this can be embedded into human CRM and TRM practices and training.","This highlights an AI feature where it actively communicates based on programmed conditions, enhancing safety protocols in human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety reporting assistance","","","The IA could be a useful aid for safety reporting, able to rapidly capture events, their precursors, signals and actions, to which the human could then add a narrative.","This represents an AI capability to process and document safety-related information efficiently, supporting human decision-making.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety monitoring and assistance","","","Second is the fact that the IA can act as a second pair of eyes, whether aiding in an emergency, or noting a safety issue or deviation or risky course of action by the human operator.","This describes an AI capability to observe and intervene in safety-critical situations, which is a feature of advanced AI systems compared to basic automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: living black box with post-event reproducibility","","","A first safeguard is the notion of the IA serving as a living black box, such that after an event the IA could reproduce the detailed flow of events, signals, interactions, decisions made and even the thinking underpinning those decisions, prior to and during the incident.","This excerpt directly mentions 'living black box', which relates to opacity/explainability in AI systems, as it highlights the ability to reproduce internal reasoning and decisions, addressing transparency and trust challenges in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: context_adaptive | Feature: real-time learning and adaptation","","","This could pave the way for true 24/7 safety monitoring and real-time learning.","This represents an AI feature as it highlights dynamic, context-aware behavior where the system adapts and learns from ongoing operations in real-time, contrasting with static conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: opacity | Feature: Explainability (XAI) and human-AI interaction features","","","This is especially the case for ‘ explainablility ’ of the AI’s advice or decisions (XAI) to the human, as well as for human-supervised-learning, user validation, and human-AI team training prior to operational deployment.","This directly addresses the opacity/explainability characteristic from the research focus, specifically mentioning 'explainablility' of AI's advice or decisions (XAI), which relates to black-box behavior and trust/regulation challenges. It also mentions human-supervised-learning and human-AI team training, which are relevant to human-AI interaction in high-risk contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Explainability and Interaction Features","","","optimal solutions for explainability, team-working, shared situation awareness, supervised learning, human-AI interaction means and devices, and training strategies.","This excerpt directly lists AI-related features such as explainability, supervised learning, and human-AI interaction means, which are key characteristics of AI systems compared to conventional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI autonomy and integration","","","Safety management systems (SMS)—the key counterpart of safety culture in aviation—the SMS—will also need to adapt to higher levels of AI autonomy, as is already being suggested in [ 31,33]. This will probably require new thinking and new approaches, for example with respect to the treatment of human-AI teaming in risk models, rather than simply producing ‘old wine in new bottles.’ SMS maturity models, such as those that are used in air trafﬁc organizations around the globe [ 52], will also need to adapt to address advanced AI integration into operations.","This excerpt directly mentions 'higher levels of AI autonomy' and 'advanced AI integration into operations,' which are key AI capabilities that distinguish AI systems from conventional automation, as they imply autonomous decision-making and operational integration that require adaptation in safety management.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Autonomous intelligent assistant","","","focusing on human-AI teaming envisaged for the 2030+ timeframe, wherein an intelligent assistant could have a moderate or high degree of autonomy in an operational environment.","This excerpt directly mentions an AI feature - an intelligent assistant with moderate to high autonomy, which represents a novel AI capability compared to conventional automation that typically follows fixed rules without adaptive autonomy.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI conceptualization and interaction design","","","Such a charter could set out the principles that are to be adopted in AI conceptualization, interaction design, training, deployment, and post-operational system performance monitoring.","This excerpt mentions AI conceptualization and interaction design, which relate to AI capabilities and interface types, indicating features of AI systems in aviation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: AI autonomy levels in human-AI teaming","","","4. There are currently several human-AI teaming options on the table, e.g., from EASA’s 1B to 3A; see also [ 54]), with 2A, 2B and 3A offering the most challenges to the human’s agency for safety, and hence the most potential impacts on safety culture.","This excerpt directly discusses AI capabilities in terms of autonomy levels (1B to 3A) and their specific impacts on human agency and safety culture, which relates to AI system features in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","Category: AI_capability | Feature: Safety advantages and risks of AI autonomy levels","","","Yet, these are the levels of AI autonomy that could also bring signiﬁcant safety advantages. It would be useful, therefore, to explore the actual relative safety advantages and concomitant risks of these and other AI autonomy levels, via risk evaluations of aviation safety-related use cases.","This excerpt focuses on AI features by discussing the potential safety advantages and concomitant risks associated with different AI autonomy levels, emphasizing the need for risk evaluations in safety-critical applications.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 6","","This potential reluctance against high-autonomy AI is ‘one to watch’, as it could affect trustworthiness and safety culture.","Severity Justification: The text explicitly states this could affect trustworthiness and safety culture, indicating a moderate to significant impact on human performance and system safety. | Relevance Justification: Directly addresses trust issues (under-trust/reluctance) related to novel AI characteristics in high-risk contexts, aligning with the research focus on human-AI interaction degradations.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 6","","This raises a more general potential design friction issue with advanced AI, as humans may be reluctant to cede too much autonomy to an AI, for fear of replacement or other issues.","Severity Justification: The issue involves fear of replacement, which can lead to under-trust or reluctance to use AI, potentially degrading performance by hindering effective human-AI collaboration, but it is described as a general potential issue without explicit severe outcomes. | Relevance Justification: This directly relates to trust issues in human-AI interaction, as fear of replacement indicates under-trust or trust mismatch, which is a key degradation in performance when interacting with novel AI systems compared to traditional automation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 4","","it is difﬁcult to adapt to an individual pilot’s speed of accessing and assimilating data. This raises another more general question with human-AI teaming, namely, how far to tailor tools to individual needs and preferences. The operators (i.e., airlines) may need to ﬁne-tune future AI support systems to individual pilots, to ensure smooth and ﬂuent performance in crisis situations.","Severity Justification: The text indicates a potential performance issue related to individual adaptation and system tailoring, but it does not describe an active degradation or safety risk, rather a need for fine-tuning to prevent problems. | Relevance Justification: This directly addresses human performance in the context of novel AI systems (adaptive behavior and individual tailoring), focusing on ensuring smooth performance, which relates to potential degradations if not properly managed.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","This differs from what we have today, and could affect safety culture if safety became the province of the AI, rather than the human.","Severity Justification: The excerpt indicates a shift in safety responsibility from humans to AI, which could lead to reduced human vigilance or engagement in safety-critical tasks, but it does not specify severe immediate consequences. | Relevance Justification: This directly relates to human performance degradation in high-risk industries by highlighting how novel AI characteristics (e.g., autonomy) might alter human roles and safety culture, aligning with the research focus on AI vs. traditional automation effects.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","In a recent EUROCONTROL-FAA (Federal Aviation Administration) debate on aviation human-AI teaming [ 35], a critical threshold which could challenge the human’s ‘agency’ for safety appeared to be category 2B.","Severity Justification: The excerpt describes a 'critical threshold' that 'could challenge the human’s ‘agency’ for safety,' suggesting a moderate to high risk to human performance in safety-critical contexts, but it is speculative rather than confirmed. | Relevance Justification: This directly relates to human performance degradation in high-risk industries (aviation) due to novel AI characteristics, specifically addressing challenges to human agency and safety, aligning with the research focus on non-deterministic and adaptive AI behaviors.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 8","","Consider the pilot who ignores advice from an AI in favor of their own judgement, and then has an accident, but also the case wherein the pilot follows the AI’s advice which turns out to be unsafe, also resulting in an incident or accident.","Severity Justification: The scenarios involve accidents and incidents, indicating severe consequences from degraded human decision-making in high-risk contexts. | Relevance Justification: Directly addresses human performance degradation in novel AI systems through specific examples of decision-making failures.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","There may be a temptation, following an accident involving a human and AI working together, for the AI developer to claim that ‘the human remains in charge’. But if the AI is partly taking control or heavily influencing the user, then this is a disingenuous argument.","Severity Justification: This indicates a significant trust issue where human operators may be misled about their control, potentially leading to over-reliance or confusion, which can degrade performance in high-risk situations. | Relevance Justification: Directly addresses trust calibration and potential performance degradation in human-AI interaction, as it highlights how AI influence can undermine human authority and decision-making, aligning with the research focus on opacity and adaptive behavior.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: skill_degradation | Severity: 7","","If an AI tool is useful and meant to help aviation professionals, they will become, to an extent, reliant on it, and such reliance may reduce their own situational awareness. They may also lose skill ﬂuency over time, if not entire skill sets.","Severity Justification: The degradation involves both reduced situational awareness and potential loss of skill fluency or entire skill sets, which are significant performance issues in high-risk aviation contexts. | Relevance Justification: The excerpt directly addresses human performance degradation related to novel AI systems, specifically mentioning reduced situational awareness and skill degradation due to reliance on AI tools, which aligns with the research focus on AI/autonomous system characteristics in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: skill_degradation | Severity: 6","","If they are not well trained, it will be hard to blame them for actions, omissions or decisions arising from AI/ML situations . . .","Severity Justification: The excerpt implies potential performance degradation from inadequate training, but it does not explicitly describe severe or immediate harm, focusing more on accountability. | Relevance Justification: This directly addresses skill degradation in the context of AI/ML, aligning with the research focus on novel AI characteristics and human performance issues in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","If professionals stop reporting incidents, or fail to disclose everything, this would be a retrograde step for aviation, which today has an excellent safety learning system.","Severity Justification: The text explicitly states this would be a 'retrograde step' for aviation safety, indicating a significant negative impact on the safety learning system, though it is presented as a hypothetical consequence. | Relevance Justification: This directly relates to human performance degradation in the context of AI interaction, as it discusses how professionals' reporting behavior (a key aspect of safety performance) could degrade due to concerns about legal consequences from AI-related decisions, aligning with the research focus on novel AI characteristics and trust/regulation challenges.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","If aviation professionals are concerned about their accountability regarding AI, they will be reluctant to use it, or err on the side of caution, e.g., always agreeing with the AI if the situation is not clear-cut.","Severity Justification: The behavior described—reluctance to use AI or always agreeing with it in unclear situations—can lead to suboptimal decisions, reduced effectiveness, and potential safety risks, but it is not the most severe form of degradation. | Relevance Justification: This directly relates to human performance degradation in the context of novel AI systems, as it highlights how accountability concerns can alter professional behavior, leading to over-reliance or avoidance, which aligns with the research focus on human-AI interaction and trust issues.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","Just Culture is linked to the broader field of ethics. As noted above, there is concern that some people may lose their jobs to AI, or that their jobs will be less satisfying, or that they will gain new jobs but receive less remuneration and less favorable employment conditions.","Severity Justification: The excerpt describes significant negative impacts on employment (job loss, less satisfying jobs, worse conditions), which can degrade performance by reducing motivation, skills, and job stability, but it does not specify direct performance metrics. | Relevance Justification: The excerpt directly addresses human performance degradation related to AI, focusing on job-related issues that align with the research focus on novel AI characteristics in high-risk industries, though it does not explicitly mention traditional automation comparisons.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","It is plausible that diminishing the human role could impact safety culture, because the human crew member may see safety as the AI’s job rather than their own, especially if the AI becomes its own autonomous decision-maker.","Severity Justification: The degradation involves a shift in safety responsibility from human to AI, which could lead to complacency, reduced vigilance, and increased risk in high-stakes aviation contexts, directly impacting safety culture. | Relevance Justification: This directly addresses human performance degradation in the context of novel AI systems (autonomous decision-making AI) versus traditional automation, focusing on behavioral changes (shifting safety responsibility) that could degrade performance in high-risk industries like aviation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","There is a concern that if people are effectively ‘closed out’ from safety, either via automation that excludes human intervention, or because it is simply ultra-safe, then ‘safety citizenship’— the innate desire to keep things safe for ourselves and others—may degrade or disappear altogether [42].","Severity Justification: The degradation is described as a potential complete disappearance of safety citizenship, which is a significant behavioral change affecting safety culture. | Relevance Justification: Directly addresses human performance degradation in safety roles due to automation/AI, specifically the erosion of proactive safety behaviors.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","Taking away the human’s perception of identity and role can negatively affect self-determination. This may be expected to degrade safety culture, as the human’s role in the system’s overall","Severity Justification: The degradation is indirect but significant, as it impacts self-determination and safety culture, which are critical for performance in high-risk industries. | Relevance Justification: Directly mentions degradation of safety culture and negative effects on self-determination, aligning with human performance issues in AI/autonomous systems contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: situational_awareness | Severity: 7","","In such a case, the humans miss what is going on in terms of the system and sub-system interactions and relationships and cannot understand the complexity and gain a holistic picture.","Severity Justification: The degradation involves missing critical system interactions and failing to understand complexity, which directly impairs situational awareness and decision-making in high-risk contexts. | Relevance Justification: This directly addresses reduced situational awareness due to novel AI characteristics (opacity and adaptive behavior), as humans cannot track or comprehend the system's dynamic interactions and complexity.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","However, if for example, AI’s capability is overestimated, such that human error is perceived as the problem and AI the solution, then the industry may work towards reducing human control inside the ‘safety space’, putting the safety of passengers and crews in the metaphorical hands of AI systems.","Severity Justification: This degradation is severe because it involves over-trust in AI, leading to reduced human control in safety-critical areas like aviation, which could compromise safety and increase risks from AI failures. | Relevance Justification: Highly relevant as it directly addresses trust issues (over-trust) and potential human performance degradation due to reduced control and increased AI reliance in high-risk industries, aligning with the research focus on AI characteristics and human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","‘commitment to safety’ might suffer if the intelligent assistant appeared to handle safety ﬂawlessly. This could affect, for example, ﬂight crew focus on safety, or managers running aviation organizations.","Severity Justification: The degradation involves a direct impact on safety commitment and focus, which are critical in high-risk industries like aviation, but the text does not specify severe outcomes like accidents. | Relevance Justification: This relates to novel AI characteristics such as opacity/lack of explainability and context-aware behavior, as the assistant's flawless appearance might lead to over-trust or reduced vigilance, aligning with human-AI interaction research focus.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","Since an IA cannot effectively take responsibility, someone else may be held accountable for an IA’s ‘actions’. If a supervisor fails to see an IA’s ‘mistake’, who will be blamed? HAIKU use cases may shed light on this, if there can be scenarios where the IA gives ‘poor’ or incorrect advice.","Severity Justification: The excerpt highlights a significant trust issue where humans may be unfairly held accountable for AI errors, leading to potential performance degradation due to stress, blame, or misaligned responsibilities in high-risk settings. | Relevance Justification: This directly relates to human performance degradation in human-AI interaction by addressing trust calibration problems, accountability mismatches, and potential misinterpretation of AI recommendations, aligning with the research focus on novel AI characteristics like opacity and non-deterministic decision-making.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 6","","Simulator training with IAs should help pilots and others ‘calibrate’ their conﬁdence in the IA.","Severity Justification: The need for calibration implies potential trust mismatches (over-trust or under-trust), which can degrade decision-making and performance. | Relevance Justification: This is highly relevant to trust issues in human-AI interaction, as it addresses confidence calibration, a key aspect of trust degradation in novel AI systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","If an IA is fully autonomous, this may affect the human team’s collective sense of responsibility, since in effect they can no longer be held responsible.","Severity Justification: The degradation involves a fundamental shift in responsibility dynamics, which could lead to reduced accountability and engagement in safety-critical tasks. | Relevance Justification: This directly relates to human performance degradation in high-risk industries by highlighting how autonomy can alter human behavioral patterns and responsibility structures.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","There is a risk that if the IA is a very good information collector, people at the sharp end might be gradually excluded in updates to system changes, as the system’s developers will consult data from the IA instead.","Severity Justification: The risk involves gradual exclusion from critical updates, which could impair human performance and safety involvement over time, but it is speculative and not yet realized. | Relevance Justification: Directly addresses human performance degradation through reduced involvement in system changes due to AI reliance, aligning with the focus on novel AI-related issues in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","Alternatively, if information is biased or counterfactual evidence is not considered, the way the IA judges risk may be incorrect, leading to a lack of trust by operational people.","Severity Justification: A lack of trust by operational people in a high-risk environment can directly degrade performance by causing hesitation, rejection of system outputs, or increased workload as they second-guess or override the system. | Relevance Justification: This directly addresses a trust issue (under-trust/mistrust) arising from a novel AI characteristic (non-deterministic/data-driven decision-making and potential opacity), which is a core focus of the research on human performance degradations in human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","There could conceivably be a clash between an IA and a team member who, for example, was taking signiﬁcant risks or continually overriding/ignoring safety advice, or an IA that was giving poor advice.","Severity Justification: The clash involves significant risks and ignoring safety advice, which can lead to serious safety incidents, but it is speculative ('could conceivably'), so severity is moderate. | Relevance Justification: Directly mentions human behavior (taking risks, overriding safety advice) in interaction with IA, aligning with novel AI-related degradations like unpredictable failures and trust challenges.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","If the IA reports on human error or human risk-taking or other ‘non-nominal behavior’ it could be considered a ‘snitch’ for management and may not be trusted.","Severity Justification: Lack of trust in an integrated system can lead to under-reliance, rejection of valid recommendations, and degraded team performance, especially in high-risk environments. | Relevance Justification: Directly addresses trust degradation in human-AI interaction, specifically under-trust due to perceived adversarial reporting roles, which aligns with the research focus on novel AI characteristics like opacity and their impact on trust.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 7","","Humans may become less concerned with safety if the IA is seen as handling safety aspects. This is an extension of the ‘complacency’ issue with automation and may be expected to increase as the IA’s autonomy increases.","Severity Justification: This degradation involves a direct reduction in human safety concern, which is critical in high-risk industries like aviation, and is expected to intensify with increased autonomy, posing significant safety risks. | Relevance Justification: It explicitly addresses human performance degradation (reduced safety concern) related to novel AI/autonomous systems (IA handling safety, increasing autonomy), directly matching the research focus on human-AI interaction in high-risk contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 8","","Conversely, a risk-taker, or someone who puts productivity ﬁrst, may consult an IA until it gets around the rules (human ingenuity can be used for the wrong reasons).","Severity Justification: This behavior indicates a significant degradation in adherence to safety protocols and ethical decision-making, potentially leading to increased risks. | Relevance Justification: Highlights behavioral changes where humans exploit AI to bypass rules, relevant to novel AI interactions in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: skill_degradation | Severity: 6","","People know how to ‘ﬁll in the gaps’ when procedures do not really ﬁt the situation, and it is not clear how an IA will do this.","Severity Justification: Suggests a potential degradation in human skill or reliance on AI where human adaptability is needed, but AI may not be capable. | Relevance Justification: Addresses skill degradation and knowledge loss in adapting procedures, relevant to novel AI's context-aware behavior challenges.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: misinterpretation | Severity: 7","","Conversely, the IA may give advice that makes little sense to the human team, or the organization yet be unable to explain its rationale. Humans may ﬁnd it difﬁcult to adhere to such advice.","Severity Justification: The IA's inability to explain its rationale and the resulting difficulty for humans to adhere to its advice indicate a significant performance degradation in decision-making and trust. | Relevance Justification: Directly addresses misinterpretation of AI recommendations and trust issues due to lack of explainability, which are key novel AI-related degradations.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 5","","If the IA queries humans too often, it may be perceived as policing them, or as a troublemaker.","Severity Justification: This could moderately degrade human performance by creating resistance or distrust towards the IA, potentially leading to reduced collaboration or ignored advice, but it is not explicitly linked to severe outcomes like accidents. | Relevance Justification: Highly relevant as it directly addresses trust issues (over-trust/under-trust mismatch) in human-AI interaction, a key focus area for performance degradations with novel AI systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","If the IA makes unsafe suggestions, trust will be eroded rapidly.","Severity Justification: This poses a high risk to human performance as rapid trust erosion can lead to under-reliance on the IA, increased cognitive load in verifying suggestions, or safety compromises, especially in high-risk industries. | Relevance Justification: Extremely relevant as it explicitly discusses trust erosion due to AI behavior, aligning with the research focus on non-deterministic decision-making and opacity in novel AI systems affecting human interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: situational_awareness | Severity: 7","","There is a risk that if the IA is a very good information collector, that people at the sharp end are gradually excluded in updates to system changes, as the systems developers will consult data from the IA instead.","Severity Justification: Exclusion from updates can degrade human performance by reducing awareness and control, posing a significant risk in dynamic environments, though it is framed as a potential outcome. | Relevance Justification: This relates to awareness/task issues, specifically reduced situational awareness and mis-prioritization, as it describes how AI features (context-aware/adaptive behavior) might marginalize human operators.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 5","","There could conceivably be a clash between an IA and a team member who, for example, was taking signiﬁcant risks or continually over-riding/ignoring safety advice, or, conversely, an IA that was giving bad advice.","Severity Justification: Clashes can lead to conflicts and reduced cooperation, affecting performance, but the severity is moderate as it is speculative and not explicitly tied to severe outcomes. | Relevance Justification: This addresses trust issues and misinterpretation, as it involves conflicts arising from AI advice and human responses, relevant to novel AI characteristics like opacity/lack of explainability.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 6","","If information is biased or counterfactual evidence is not considered, the way the IA judges risk may be incorrect, leading to a lack of trust by operational people.","Severity Justification: The degradation involves a direct impact on trust, which can compromise safety and decision-making in high-risk industries, but it is described as a potential risk rather than a confirmed outcome. | Relevance Justification: This directly addresses trust issues, a priority search category, by linking AI characteristics (non-deterministic/data-driven decision-making) to human performance degradation through trust mismatch.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 7","","IAs may need regular maintenance and ﬁne-tuning, which may affect the perceived ‘stability’ of the IA by ops people, resulting in loss of trust or ‘rapport’.","Severity Justification: Loss of trust or rapport directly impacts human-AI collaboration and can lead to under-reliance or rejection of AI advice, which is a significant performance degradation in high-risk contexts. | Relevance Justification: This explicitly mentions 'loss of trust', which is a direct match to the priority search for trust issues, and it is related to novel AI characteristics like non-deterministic behavior and opacity affecting human performance.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: misinterpretation | Severity: 8","","The IA may give (good) advice that makes little sense to the human team or the organization, yet it cannot explain its rationale. Managers and operational staff may ﬁnd it difﬁcult to adhere to such advice.","Severity Justification: Difficulty adhering to AI advice due to lack of explainability can lead to errors, delays, or non-compliance, which are severe performance degradations in operational settings. | Relevance Justification: This directly addresses misinterpretation of AI recommendations and misunderstanding AI outputs, matching the priority search, and relates to novel AI features like opacity and non-deterministic decision-making.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: trust_issues | Severity: 4","","The AI’s advice might not be so helpful unless it is human-supervisory trained.","Severity Justification: It points to a conditional limitation of AI advice, which could undermine trust, but it is not a severe or immediate degradation. | Relevance Justification: Addresses the effectiveness of AI recommendations and the need for human oversight, relevant to trust and decision-making in performance contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: skill_degradation | Severity: 5","","People know how to ﬁll in the gaps when procedures don’t really ﬁt the situation, and it is not clear how an IA will do this.","Severity Justification: It implies a reliance on human expertise that may be lost or mismatched with AI capabilities, but it is speculative about AI performance. | Relevance Justification: Directly compares human and AI capabilities in dynamic situations, relevant to skill degradation and cognitive aspects of human-AI interaction.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","Category: behavioral_changes | Severity: 6","","A human risk-taker or someone who puts productivity ﬁrst, may consult (‘game’) an IA until it gets around the rules.","Severity Justification: The behavior involves actively gaming the system, which could lead to safety compromises, but it is described as a potential action rather than a confirmed degradation. | Relevance Justification: Directly addresses human interaction with AI, showing how individuals might exploit or misuse AI systems, aligning with trust and behavioral aspects of performance degradation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: failure to develop social capital for AI implementation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: users rejecting the AI tool’s implementation","A failure to develop the ‘social capital’ required to foster such a change, leading to users rejecting the AI tool’s implementation (for example, because it threatens job losses).","Causal Strength Justification: Direct causal language 'leading to' explicitly links the failure to the effect, with a clear example provided. | Relevance Justification: Directly relates to human-AI interaction by showing how social factors (lack of social capital) causally affect user acceptance and performance in implementing AI tools, though not explicitly tied to non-deterministic, opacity, or adaptive features.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: expert systems failure and subsequent machine learning advancements | Evidence Type: direct | Causal Strength: 8 | Performance Effect: introduction of AI prototypes, products, and services in aviation, including support for safe and expeditious air traffic flow","The failure of expert systems led to the so-called ‘AI winter’, which ended recently as computing power increased dramatically and machine learning ﬁnally became possible [ 24]. This has resulted in a host of early AI prototypes, products, and services being introduced into European aviation, from automatic speech recognition and passenger support, to optimising safe and expeditious air trafﬁc ﬂow both in normal and hazardous weather [25].","Causal Strength Justification: Direct causal language ('led to', 'resulted in') explicitly links the failure of expert systems to the AI winter and the introduction of AI in aviation, with clear sequential cause-effect relationships. | Relevance Justification: The excerpt involves AI features (expert systems, machine learning) and effects on aviation systems, but it does not directly address human performance or the specific novel AI characteristics (non-deterministic, opacity, adaptive) as requested in the task; it focuses on historical and technological developments.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: electronic flight strips | Evidence Type: direct | Causal Strength: 8 | Performance Effect: less need for flight data assistants","For example, implementing electronic flight strips has led to less need for flight data assistants, yet has proven itself in terms of aviation system efficiency and effectiveness.","Causal Strength Justification: Direct causal language 'has led to' explicitly connects the implementation to the effect. | Relevance Justification: Directly addresses how an automation/AI feature causally affects human performance (workforce need), though not specifically the novel AI characteristics listed in the research focus.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI tool (useful and meant to help aviation professionals) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: reduced situational awareness and loss of skill fluency or entire skill sets","If an AI tool is useful and meant to help aviation professionals, they will become, to an extent, reliant on it, and such reliance may reduce their own situational awareness. They may also lose skill ﬂuency over time, if not entire skill sets.","Causal Strength Justification: The causal relationship is explicit with 'may reduce' and 'may also lose', indicating a direct but probabilistic link between reliance on the AI tool and the specified human performance degradations. | Relevance Justification: Directly addresses how AI tool reliance causally affects human performance (situational awareness and skill fluency), aligning with the research focus on AI characteristics in high-risk industries like aviation.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI becoming its own autonomous decision-maker | Evidence Type: direct | Causal Strength: 7 | Performance Effect: Human crew member may see safety as the AI’s job rather than their own, impacting safety culture","It is plausible that diminishing the human role could impact safety culture, because the human crew member may see safety as the AI’s job rather than their own, especially if the AI becomes its own autonomous decision-maker.","Causal Strength Justification: Direct causal language with 'because' explicitly linking AI autonomy to human perception shift, though qualified with 'plausible' and 'could' indicating probability rather than certainty. | Relevance Justification: Directly addresses how AI features (autonomous decision-making) affect human performance (safety responsibility perception) in high-risk aviation context.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI/autonomous system characteristics (implied from research focus, but not explicitly named in chunk) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: degraded safety culture","Taking away the human’s perception of identity and role can negatively affect self-determination. This may be expected to degrade safety culture, as the human’s role in the system’s overall ‘safety space’ (the hypothetical landscape of all safety functions and activities) diminishes.","Causal Strength Justification: Direct causal language ('can negatively affect', 'may be expected to degrade') with a clear mechanism linking cause to effect. | Relevance Justification: Directly addresses human performance (self-determination and safety culture) in relation to system changes, though AI features are not explicitly named in the chunk.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: automation that excludes human intervention or ultra-safe AI | Evidence Type: direct | Causal Strength: 8 | Performance Effect: degradation or disappearance of safety citizenship (innate desire to keep things safe)","There is a concern that if people are effectively ‘closed out’ from safety, either via automation that excludes human intervention, or because it is simply ultra-safe, then ‘safety citizenship’— the innate desire to keep things safe for ourselves and others—may degrade or disappear altogether [42].","Causal Strength Justification: Direct causal language 'if...then...may degrade or disappear' indicates a strong, explicit cause-effect relationship. | Relevance Justification: Directly links AI/automation features to human performance effect (safety citizenship degradation), aligning with the task focus on causal relationships between AI and human performance.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI taking on a larger share of the safety role or occupying the safety space | Evidence Type: mechanism | Causal Strength: 7 | Performance Effect: erosion of safety citizenship (via factors like safety role ambiguity, conflict, etc.)","Seven factors can erode safety citizenship [42], all of which could be affected by AI taking on a larger share of the safety role, or occupying the ‘safety space’:","Causal Strength Justification: Uses 'could be affected by' to suggest a causal mechanism, though less direct than explicit 'causes'. | Relevance Justification: Explicitly connects AI features to potential effects on human safety performance, relevant to the task's focus on AI-human causal relationships.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI identity deception (pretending AI is human) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: less acceptance of solutions and no improvement in overall performance","Deception about AI teammate’s identity (pretending it is a human) did not improve overall performance, and led to less acceptance of their solutions, whereas knowing it is an AI led to better overall performance.","Causal Strength Justification: Direct causal language 'led to' is used to connect deception to less acceptance and no improvement, and 'led to' connects knowing it is AI to better performance, with clear cause-effect relationships. | Relevance Justification: Directly addresses how AI identity features (deception vs. transparency) causally affect human performance metrics like acceptance and overall performance, though not explicitly tied to non-deterministic, opacity, or adaptive characteristics.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI autonomy | Evidence Type: direct | Causal Strength: 8 | Performance Effect: humans miss what is going on in terms of the system and sub-system interactions and relationships and cannot understand the complexity and gain a holistic picture","As AI autonomy goes up, passive back-up is likely to be ineffective, in part because AI can lead to ‘ increasing invisible interactions’ . In such a case, the humans miss what is going on in terms of the system and sub-system interactions and relationships and cannot understand the complexity and gain a holistic picture.","Causal Strength Justification: Direct causal language 'lead to' explicitly connects AI to increasing invisible interactions, and the phrase 'In such a case' directly links this to human performance effects. | Relevance Justification: Directly addresses how AI characteristics (autonomy) affect human performance (missing interactions, lacking holistic understanding) with explicit causal language.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: AI capability overestimation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Reduced human control in safety space, leading to safety risks when AI cannot handle low-probability events","However, if for example, AI’s capability is overestimated, such that human error is perceived as the problem and AI the solution, then the industry may work towards reducing human control inside the ‘safety space’, putting the safety of passengers and crews in the metaphorical hands of AI systems. This is risky, as already identiﬁed in the maritime industry [ 48], since there can be ‘tail effects’, wherein low probability events are impractical to train AIs on, so that when they occur the AI cannot handle them.","Causal Strength Justification: Direct causation is indicated by 'such that... then' and 'since', explicitly linking overestimation to reduced control and resulting risks. | Relevance Justification: Directly addresses how AI characteristics (capability perception) affect human performance (control and safety outcomes), aligning with the research focus on novel AI features in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: Not applicable | Evidence Type: direct | Causal Strength: 10 | Performance Effect: Not applicable","“Failures in leadership and organizational safety culture led to the Nimrod incident where the aircraft developed serious technical failures, preceded by deﬁciencies in safety case and a lack of proper documentation and communication between the relevant organizations ”.","Causal Strength Justification: Direct causation is explicitly stated with 'led to', linking cause (failures in leadership and organizational safety culture) to effect (Nimrod incident with technical failures). | Relevance Justification: The excerpt does not mention AI features or human performance; it discusses organizational and leadership failures in a specific accident context, unrelated to the research focus on AI characteristics.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA (Intelligent Automation/AI) risk judgment process | Evidence Type: direct | Causal Strength: 8 | Performance Effect: lack of trust by operational people","Alternatively, if information is biased or counterfactual evidence is not considered, the way the IA judges risk may be incorrect, leading to a lack of trust by operational people.","Causal Strength Justification: Direct causal language 'leading to' explicitly connects incorrect risk judgment to lack of trust. The cause (incorrect risk judgment due to biased information/unconsidered evidence) and effect (lack of trust) are both stated. | Relevance Justification: Directly addresses how an AI characteristic (risk judgment process) affects human performance (trust), which is a core aspect of human-AI interaction in high-risk industries. The causal relationship is explicit and involves operational personnel.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA as a very good information collector | Evidence Type: direct | Causal Strength: 9 | Performance Effect: people at the sharp end might be gradually excluded in updates to system changes","There is a risk that if the IA is a very good information collector, people at the sharp end might be gradually excluded in updates to system changes, as the system’s developers will consult data from the IA instead.","Causal Strength Justification: Direct causal mechanism ('as' indicating cause) is used, showing a clear link between IA data collection and human exclusion, with strong explicit language. | Relevance Justification: It directly addresses human performance in high-risk contexts by showing how AI features (information collection) can negatively impact human involvement in safety-critical updates.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: implementation and deployment of IAs | Evidence Type: direct | Causal Strength: 8 | Performance Effect: very limited uptake of the IA","Failure to address such concerns may lead to very limited uptake of the IA.","Causal Strength Justification: Direct causal language ('may lead to') is used, indicating a strong, explicit cause-effect relationship between addressing concerns and uptake. | Relevance Justification: It relates to human-AI interaction in high-risk industries by showing how human concerns affect IA adoption, though it does not directly tie to non-deterministic, opacity, or adaptive features.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA's advice or consultation | Evidence Type: mechanism | Causal Strength: 7 | Performance Effect: Human may use IA to get around rules, misusing ingenuity","Conversely, a risk-taker, or someone who puts productivity ﬁrst, may consult an IA until it gets around the rules (human ingenuity can be used for the wrong reasons).","Causal Strength Justification: Causal mechanism: consulting IA leads to getting around rules, implying IA enables or facilitates this behavior. | Relevance Justification: Shows how IA interaction can lead to negative human performance outcomes like rule circumvention.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA's inability to explain its rationale | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Humans find it difficult to adhere to advice","Conversely, the IA may give advice that makes little sense to the human team, or the organization yet be unable to explain its rationale. Humans may ﬁnd it difﬁcult to adhere to such advice.","Causal Strength Justification: Direct causal link: 'unable to explain its rationale' leads to 'Humans may find it difficult to adhere to such advice'. | Relevance Justification: Directly addresses opacity/lack of explainability causing human performance issues in adhering to advice.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA handling safety aspects and increasing autonomy | Evidence Type: direct | Causal Strength: 7 | Performance Effect: humans becoming less concerned with safety","Humans may become less concerned with safety if the IA is seen as handling safety aspects. This is an extension of the ‘complacency’ issue with automation and may be expected to increase as the IA’s autonomy increases.","Causal Strength Justification: Direct causal language ('may become less concerned if') and explicit connection to automation complacency, with a mechanism ('may be expected to increase as') indicating a strong, plausible causal relationship. | Relevance Justification: Directly addresses how AI features (IA handling safety and autonomy) causally affect human performance (safety concern), aligning with the task focus on human-AI interaction in high-risk contexts.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA reporting on human error or risk-taking | Evidence Type: direct | Causal Strength: 8 | Performance Effect: lack of trust in the IA","This will lead to lack of trust in the IA.","Causal Strength Justification: Direct causal language 'will lead to' explicitly connects the antecedent to the effect. | Relevance Justification: Directly addresses how an AI feature (reporting on human behavior) causally affects human trust, a key aspect of human performance in high-risk interactions.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA's risk judgment based on information | Evidence Type: direct | Causal Strength: 8 | Performance Effect: lack of trust by operational people","If information is biased or counterfactual evidence is not considered, the way the IA judges risk may be incorrect, leading to a lack of trust by operational people.","Causal Strength Justification: Direct causal language 'leading to' explicitly connects incorrect risk judgment to lack of trust. | Relevance Justification: Directly addresses how AI feature (risk judgment) affects human performance (trust), matching the research focus on human-AI interaction in high-risk industries.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA giving advice without explaining its rationale | Evidence Type: indirect | Causal Strength: 7 | Performance Effect: difficulty for managers and operational staff to adhere to the advice","The IA may give (good) advice that makes little sense to the human team or the organization, yet it cannot explain its rationale. Managers and operational staff may ﬁnd it difﬁcult to adhere to such advice.","Causal Strength Justification: Strong implied causation through logical sequence: inability to explain rationale leads to difficulty in adherence. | Relevance Justification: Directly links an AI characteristic (opacity/lack of explainability) to a human performance effect (difficulty in following advice).","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: regular maintenance and fine-tuning of IAs | Evidence Type: direct | Causal Strength: 8 | Performance Effect: loss of trust or rapport by operational people","IAs may need regular maintenance and ﬁne-tuning, which may affect the perceived ‘stability’ of the IA by ops people, resulting in loss of trust or ‘rapport’.","Causal Strength Justification: Direct causal language with 'resulting in' explicitly linking the effect to the cause. | Relevance Justification: Directly addresses how an AI characteristic (need for maintenance) causally affects human performance (trust).","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: intelligent assistants (IAs) | Evidence Type: direct | Causal Strength: 7 | Performance Effect: reduction in human staff and potential resentment against IAs","Introducing intelligent assistants may inexorably lead to less human staff. Although there are various ways to ‘sugar-coat’ this, e.g., current and predicted shortfalls in stafﬁng across the aviation workforce, it may lead to resentment against IAs. This factor will likely be inﬂuenced by how society gets on with advanced AI and IAs.","Causal Strength Justification: Direct causal language is used: 'may inexorably lead to' and 'it may lead to', explicitly linking the introduction of IAs to the effects of less human staff and resentment. | Relevance Justification: Directly addresses how an AI feature (introduction of IAs) causally affects human workforce dynamics (staff reduction and resentment), which is a key aspect of human performance in organizational settings.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA querying behavior | Evidence Type: direct | Causal Strength: 8 | Performance Effect: perception of IA as policing or troublemaker","If the IA queries humans too often, it may be perceived as policing them, or as a troublemaker.","Causal Strength Justification: Direct conditional causal statement: 'If the IA queries humans too often, it may be perceived as...', explicitly linking the AI feature (querying frequency) to the human effect (negative perception). | Relevance Justification: Directly addresses how an AI feature (querying frequency) causally affects human perception and trust, which is critical to human-AI interaction and performance in collaborative tasks.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA making unsafe suggestions | Evidence Type: direct | Causal Strength: 9 | Performance Effect: rapid erosion of trust","If the IA makes unsafe suggestions, trust will be eroded rapidly.","Causal Strength Justification: Direct conditional causal statement: 'If the IA makes unsafe suggestions, trust will be eroded rapidly.', explicitly linking the AI feature (unsafe suggestions) to the human effect (trust erosion). | Relevance Justification: Directly addresses how an AI feature (making unsafe suggestions, potentially due to non-deterministic behavior) causally affects human trust, a key factor in human performance and reliance on AI systems.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA acting as a second pair of eyes (noting safety issues, deviations, or risky actions by the human operator) | Evidence Type: direct | Causal Strength: 8 | Performance Effect: IA becomes a ready-to-hand safety oracle that the human team can consult when considering the best course of action and safety risks","Second is the fact that the IA can act as a second pair of eyes, whether aiding in an emergency, or noting a safety issue or deviation or risky course of action by the human operator. This leads to a third useful aspect of an IA, that it can be a ready-to-hand safety oracle that the human team can consult at any point when considering the best course of action and the safety risks it might entail.","Causal Strength Justification: Direct causal language 'This leads to' explicitly connects the IA's action (cause) to its role as a safety oracle (effect). | Relevance Justification: The excerpt explicitly links an AI feature (IA acting as a second pair of eyes) to a human performance effect (IA becoming a consultable safety oracle), though it does not directly address the specific novel AI characteristics (non-deterministic, opacity, adaptive) from the research focus.","y"
"The Impact of Artificial Intelligence on Future Aviation Safety Culture","","","AI Feature: IA acting as a second pair of eyes | Evidence Type: direct | Causal Strength: 8 | Performance Effect: Human team can consult IA as a safety oracle when considering best course of action and safety risks","This leads to a third useful aspect of an IA, that it can be a ready-to-hand safety oracle that the human team can consult at any point when considering the best course of action and the safety risks it might entail.","Causal Strength Justification: Direct causal language 'leads to' explicitly connects the IA's role as a second pair of eyes to its function as a safety oracle, with clear cause-effect relationship. | Relevance Justification: Directly links an AI feature (IA as second pair of eyes) to human performance effect (consultation on safety decisions), though not specifically tied to non-deterministic, opacity, or adaptive characteristics from the research focus.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based digital team partner for safety-critical tasks","","","An initial ODD is defined for an AI-based digital team partner that supports Air Traffic Controllers in their daily work even for safety-critical tasks such as conflict detection and resolution.","This excerpt explicitly mentions an AI-based system (digital team partner) that assists in high-risk, safety-critical tasks, aligning with the research focus on novel AI characteristics in high-risk industries like transportation (Air Traffic Control).","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Human-AI relationship shift","","","Developing AI-based systems for this paradigm shift from systems with clear hierarchical relationships to systems with an almost equal relationship between humans and AI is one of the big tasks of research and development for AI-based systems [4].","This represents a novel AI characteristic compared to conventional automation, as it involves dynamic, collaborative interactions rather than fixed, hierarchical control, aligning with context-aware/adaptive behavior in human-AI teaming.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Operational Design Domain for AI systems","","","The ODD is intended to describe the conditions under which a given AI-based system is designed to function [7].","This directly mentions 'AI-based system' and describes a framework (ODD) for defining its operational boundaries, which relates to AI capabilities and safety assurance requirements.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-powered digital team partner","","","Therefore, ongoing research is focusing on the incorporation of SCO, involving a collaboration between a human controller and a digital team partner powered by AI [9]–[11].","This excerpt explicitly mentions a 'digital team partner powered by AI,' which represents an AI capability involving collaboration with humans, aligning with the research focus on novel AI characteristics in human-AI interaction for high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI capabilities and limitations","","","understand the capabilities and limitations of the AI-based digital team partner as well as to build appropriate levels of trust in the automation.","This excerpt directly discusses AI features by referencing the capabilities and limitations of an AI-based system, which is a key aspect of AI characteristics in human-AI interaction, as it relates to understanding and trust-building.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based system solutions and Human-AI Teaming challenges","","","Consequently, the proposed solutions of the AI-based system to resolve the conflicts are compared with the ODD’s defined limitations. In the end, future challenges of introducing ODD into the Human-AI Teaming context are discussed. The feasibility of the ODD concept in the field of ATC is tested, demonstrating a potential step towards safe Human-AI interaction.","This excerpt directly mentions an 'AI-based system' and its role in resolving conflicts, as well as 'Human-AI Teaming context' and 'safe Human-AI interaction,' which relate to AI capabilities and interaction features in high-risk industries like transportation (ATC).","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Safety-critical task performance by AI system","","","Once an aircraft is delegated to the system it also performs safety-critical tasks, such as conflict detection and conflict resolution (CD&R).","This represents an AI feature as it demonstrates the AI system's capability to autonomously manage high-risk functions, which is a key characteristic of advanced AI in safety-critical industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based digital team partner for control delegation","","","This process involves the intentional delegation of aircraft control to an AI-based digital team partner, emphasizing a human-centered approach to aviation operations collaboration.","This represents an AI feature as it involves an AI-based system taking on safety-critical tasks like conflict detection and resolution, highlighting its role in autonomous operations within a collaborative human-AI framework.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: opacity | Feature: Black-box-like behavior","","","However, applying safety measures on an AI-based system with black-box-like behavior is a challenging engineering task.","This directly references the opacity/explainability characteristic of AI systems, where internal reasoning is not transparent, making safety engineering challenging.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Operational Design Domain (ODD) definition","","","the ODD defines a set of conditions, such as environmental factors, dynamic elements, and scenery, under which the system can operate safely [13].","This describes a framework for defining AI system capabilities and limitations, which is relevant to novel AI characteristics like context-aware/adaptive behavior by specifying environmental conditions for safe operation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Trustworthy AI development building blocks","","","In EASAs concept paper [3] building blocks such as AI assurance, Human Factors for AI, and AI risk mitigation are defined for the development of trustworthy AI.","This directly references AI-specific capabilities and development considerations (AI assurance, Human Factors for AI, AI risk mitigation) that are characteristic of advanced AI systems, distinguishing them from conventional automation by focusing on trustworthiness and human-AI interaction factors.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: automation | Feature: Operational Design Domain (ODD) definition","","","According to the SAE J3016 standard [14], the ODD is defined as “the specific conditions under which a given driving automation system or feature thereof is designed to function, including, but not limited to, environmental, geographical, and time-of-day restrictions, and the requisite presence or absence of certain traffic or roadway characteristics”.","This represents an AI feature as it describes the specific conditions and constraints for automation systems, which is crucial for context-aware and adaptive behavior in high-risk industries like transportation, aligning with the research focus on novel AI characteristics such as dynamic environment response.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: opacity | Feature: explainability requirements for AI","","","This is in line with the requirements for explainability and acceptance of AI identified by previous work [27].","This directly references explainability as a requirement for AI, which relates to opacity/explainability characteristics in the research focus, indicating challenges in trust and regulation due to black-box behavior.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based decision-making for automation","","","leveraging various tools and techniques, including AI-based solutions. To prevent the overexertion of a single human ATCO due to too many tools, the concept of a digital ATCO as a collaborative team partner has been introduced [9], [10]. A digital ATCO would use AI-based decision-making to provide a single point of contact for ATCOs, reducing the need for manual system oversight and activation.","This directly mentions AI-based solutions and AI-based decision-making as part of a human-centered automation concept, which is a key AI capability discussed in the chunk.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: context_adaptive | Feature: adaptive trajectory adjustment","","","This requires a conflict resolution tool that is capable of adapting trajectories to several preferences defined by either the human operator or the digital ATCO, such as short trajectories, reaching the sector boundary in time, capable climb and descent rates, or sticking close to the original trajectory.","This represents context-aware/adaptive behavior as the tool dynamically responds to preferences, enabling real-time adaptation to varying conditions in high-risk air traffic control.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Human-Autonomy Teaming","","","Human-Autonomy Teaming can be defined as a “socio-technical system in which at least one human and one autonomous system interact interdependently over time to complete a common goal or task”.","This directly describes a key AI system feature - autonomous systems capable of interdependent interaction with humans to achieve common goals, which is a novel characteristic compared to conventional automation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: opacity | Feature: AI system transparency and explainability","","","the transparency of the AI-based system involves two interrelated components. The display transparency provides a real-time understanding of the actions of the AI system as a part of situation awareness (SA) and explainability provides information in a backward-looking manner on the logic, process, and factors, or reasoning upon which the system’s actions or recommendations are based.","This directly addresses the opacity/explainability characteristic by discussing how AI systems need transparency (display transparency) and explainability to make their internal reasoning visible and understandable to human operators, which is a key challenge with AI compared to conventional automation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based automation for perceptual and cognitive tasks","","","With the introduction of AI-based systems as a form of highly capable automation directed at highly perceptual and cognitive tasks [32], the same psychological phenomena occur as with conventional automation of Human-Technology interaction, as well as new ones.","This excerpt directly mentions AI-based systems as a form of automation, highlighting their capability for perceptual and cognitive tasks, which aligns with the research focus on novel AI characteristics in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Autonomous task allocation","","","when the allocation of flights within a sector is taken from a human ATCO to an AI-based system autonomously, the risk of reduced situational awareness for tasks that are not under human ATCOs active control is one of the core challenges.","This excerpt explicitly mentions an 'AI-based system autonomously' taking over tasks from humans, which represents a novel AI capability in high-risk industries like transportation, contrasting with conventional automation by emphasizing autonomous decision-making and control transfer.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Delegation in Human-AI Systems","","","This could be seen as a solution for delegation in Human-AI systems at EASA AI level 2A Cooperation [32]. Here the human is responsible for defining the objectives and the digital agent proceeds with the delegated tasks to fulfill these objectives.","This excerpt directly mentions 'Human-AI systems' and describes a specific AI capability (delegation) where digital agents execute tasks based on human-defined objectives, representing a key feature of AI systems in human-AI interaction contexts.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: automation | Feature: Trust-building mechanisms for automation","","","solutions are needed to mitigate the reduced situational awareness under passive control by adequate design of task delegation and to mitigate the risk of complacency effects by supporting ATCOs in building up an appropriate level of trust in technology.","This excerpt describes a key feature of automation systems - the need to design systems that build appropriate trust levels in human operators to mitigate complacency effects and maintain situational awareness.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: adaptation | Feature: Adaptable automation with human control","","","Parasuraman and Wickens [40] suggested keeping the human in charge by deciding when to automate, what is called adaptable automation.","This excerpt describes a specific type of automation system feature - adaptable automation where human operators maintain control over when to deploy automation, representing a control mechanism in human-AI interaction systems.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: interface | Feature: delegation interfaces","","","Parasuraman et al. [42] introduced delegation interfaces as a particular type of real-time supervisory control.","This represents an AI feature as delegation interfaces are a type of human-AI interaction mechanism that facilitates control and collaboration in automated systems, relevant to the research focus on novel AI characteristics in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: adaptation | Feature: flexible function allocation","","","To mitigate automation complacency, Parasuraman and Manzey [36] concluded that raising the perceived accountability of operators and using flexible strategies of function allocation are promising changes in situational conditions.","This represents an AI feature as flexible strategies of function allocation are adaptive mechanisms that adjust roles between humans and AI systems, addressing unpredictability and variability in dynamic environments, which aligns with the context-aware/adaptive behavior characteristic.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Human-centered design with explainability focus","","","DIRC is based on a human-centered design that operates under the supervision of a human controller. An essential component of DIRC is to communicate its decisions and actions in a way that is easy for humans to understand.","This represents an AI feature related to explainability and human-AI interaction, as it highlights the system's design for human supervision and communication of decisions, addressing opacity/explainability challenges in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based independent decision-making","","","DIRC uses AI methods and techniques to make independent decisions based on continuous analysis of information from various sources. It then translates this information into actions through its central decision-making module and can implement the selected solution.","This represents an AI feature as it explicitly mentions 'AI methods and techniques' enabling 'independent decisions' based on continuous data analysis, which is a core capability distinguishing AI from conventional automation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Human-AI Teaming and Task Re-delegation","","","conclusions on the feasibility of the ODD in collaborative Human-AI Teaming applications for level 2B automation according to EASA [2], especially for the re-delegation of tasks to the human controller","This excerpt explicitly mentions 'collaborative Human-AI Teaming applications' and 're-delegation of tasks to the human controller,' which are key AI features involving dynamic interaction and control mechanisms between AI and humans in automation systems.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: opacity | Feature: black-box behavior","","","Assuring safety in future Human-AI Teaming confronts engineers with an added layer of complexity, due to the black-box behavior of AI-based systems.","This directly references the opacity/explainability characteristic, as 'black-box behavior' indicates a lack of transparency in AI systems, aligning with the research focus on trust and regulation challenges.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based digital team partner","","","The use case of DIRC, the AI-based digital team partner for human ATCOs, was introduced for investigating Human-AI Teaming in the safety-critical aviation domain.","This explicitly mentions an AI-based system (DIRC) designed as a digital team partner for humans, representing a novel AI capability in a safety-critical domain, which aligns with the research focus on AI characteristics in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: Conflict detection and resolution capability","","","Special focus was put on the most important task in ATC DIRCs capability of CD&R.","This highlights a specific AI capability (CD&R - conflict detection and resolution) in air traffic control, demonstrating an AI feature relevant to safety-critical operations.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: AI_capability | Feature: AI-based digital team partner with conflict detection and resolution capabilities","","","By defining an initial ODD version for DIRC, an AI-based digital team partner for ATCOs, a basis for further ODD refinement was created. For the air traffic control environment in which DIRC operates, certain capabilities, such as conflict detection and resolution, are required to help ATCOs in their daily work.","This excerpt explicitly mentions 'AI-based digital team partner' and specific AI capabilities (conflict detection and resolution), highlighting novel AI characteristics in a high-risk industry (air traffic control) compared to conventional automation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: context_adaptive | Feature: ODD adaptation and perspective shift","","","However, the transfer of ODD from automotive to air traffic control required some adjustments. Compared to the automotive ODD, the ATC ODD does not operate from an ego vehicle perspective, but from a bird’s eye view.","This excerpt illustrates context-aware or adaptive behavior in AI systems, as it discusses adjustments in ODD for different domains (automotive vs. ATC), reflecting dynamic environment response and variability handling, which are key characteristics of novel AI systems compared to conventional automation.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: context_adaptive | Feature: Dynamic task allocation and ODD adaptation","","","Human-AI Teaming requires dynamic task allocation, which dynamically changes the focus of the ODD application. Consequently, depending on the tasks allocated to DIRC, several dynamically interchangeable sub-ODDs are needed.","This excerpt directly mentions 'dynamic task allocation' and 'dynamically changes the focus,' which aligns with the context-aware/adaptive behavior characteristic of novel AI systems, as it involves dynamic response and environment adaptation in human-AI interaction scenarios.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","Category: interface | Feature: Dynamic Task Allocation Interface","","","Furthermore, the interface between ATCO’s current task load and ODDs for identifying suitable conditions for dynamic task allocation will be an important factor for the success of safe Human-AI Teaming in the field of AI-based Air Traffic Controller Operations.","This excerpt describes an interface that enables dynamic task allocation based on conditions, which is a key AI feature for adaptive human-AI interaction in high-risk operations.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: situational_awareness | Severity: 7","","Engagement decreases and attention to other tasks increases over time, lowering situational awareness of automation-related information and increasing complacency [39].","Severity Justification: Reduced situational awareness and increased complacency are significant performance degradations in high-risk industries, as they can lead to errors and safety issues. | Relevance Justification: This directly addresses situational awareness issues related to automation, which is a key focus in human-AI interaction research for novel AI systems.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: trust_issues | Severity: 6","","This means that trust plays a fundamental role [35] when the human is uncertain that their teammate will perform competently and reliably (uncertainty) and when risks are tied to the teammate’s performance (vulnerability).","Severity Justification: The excerpt highlights uncertainty and vulnerability in human-technology interactions, which can lead to under-trust or trust calibration problems, moderately impacting performance in safety-critical domains. | Relevance Justification: Directly addresses trust issues (uncertainty and vulnerability) in human-AI interactions, aligning with the research focus on novel AI characteristics like opacity and non-deterministic decision-making that affect trust and performance.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: behavioral_changes | Severity: 7","","To sum up, people’s ability to oversee and interact with automation systems to maintain operational safety [37] is negatively affected when switching from automation on the level of information processing to the level of decision-making when actions are to be implemented based on collected information automatically [38].","Severity Justification: The degradation directly impacts operational safety, which is critical in high-risk industries, and involves a fundamental shift in human role from processing to decision-making oversight. | Relevance Justification: This explicitly describes a human performance degradation (negatively affected ability) related to automation/AI systems, matching the research focus on novel AI characteristics vs. conventional automation in high-risk contexts.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: automation_bias | Severity: 7","","To mitigate automation complacency, Parasuraman and Manzey [36] concluded that raising the perceived accountability of operators and using flexible strategies of function allocation are promising changes in situational conditions. These measures can help reduce issues of complacency and automation bias in interactions with various automated systems.","Severity Justification: Automation complacency and bias are significant cognitive biases that can lead to reduced vigilance and over-reliance on automation, potentially causing errors in high-risk industries. | Relevance Justification: The text explicitly mentions 'automation complacency' and 'automation bias' as issues in interactions with automated systems, which directly relates to cognitive biases in human-AI interaction.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: behavioral_changes | Severity: 7","","Parasuraman and Manzey [36] reported consistent findings from studies with ATCOs, where a greater proportion of controllers missed detecting the conflict with the automation compared to the detection rate without the help of automation.","Severity Justification: Missing conflict detection in air traffic control is a critical safety issue that could lead to accidents, representing a significant degradation in operational performance. | Relevance Justification: Directly describes a measurable performance degradation (missed conflict detection) when using automation compared to manual operation, which aligns with human performance degradation in human-AI interaction contexts.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","Category: situational_awareness | Severity: 8","","For these diversely proven automation effects on human users [36], [38], [40]–[42] solutions are needed to mitigate the reduced situational awareness under passive control by adequate design of task delegation and to mitigate the risk of complacency effects by supporting ATCOs in building up an appropriate level of trust in technology.","Severity Justification: Reduced situational awareness and complacency effects in high-stakes environments like air traffic control represent serious degradations that compromise safety and operational effectiveness. | Relevance Justification: Explicitly mentions two key performance degradation categories (reduced situational awareness and complacency effects) that are central to human-AI interaction research, with direct references to supporting literature.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","","AI Feature: AI-based system autonomously allocating flights within a sector | Evidence Type: direct | Causal Strength: 7 | Performance Effect: lowering situational awareness of automation-related information and increasing complacency","Engagement decreases and attention to other tasks increases over time, lowering situational awareness of automation-related information and increasing complacency [39].","Causal Strength Justification: Direct causal language ('lowering', 'increasing') connects the behavioral changes (decreased engagement, increased attention to other tasks) to the performance effects. | Relevance Justification: Directly addresses how automation (specifically AI-based autonomous allocation) affects human performance (situational awareness and complacency), which is a core focus of the research.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","","AI Feature: AI-based systems as a form of highly capable automation directed at highly perceptual and cognitive tasks | Evidence Type: direct | Causal Strength: 8 | Performance Effect: negatively affected ability to oversee and interact with automation systems to maintain operational safety","To sum up, people’s ability to oversee and interact with automation systems to maintain operational safety [37] is negatively affected when switching from automation on the level of information processing to the level of decision-making when actions are to be implemented based on collected information automatically [38].","Causal Strength Justification: Direct causal language 'is negatively affected when switching from' explicitly links the AI feature (decision-making automation) to the human performance effect. | Relevance Justification: Directly addresses how AI automation (decision-making level) causally impacts human performance (oversight and interaction for safety), aligning with the research focus on AI characteristics in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","","AI Feature: automation | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduced situational awareness and complacency effects","For these diversely proven automation effects on human users [36], [38], [40]–[42] solutions are needed to mitigate the reduced situational awareness under passive control by adequate design of task delegation and to mitigate the risk of complacency effects by supporting ATCOs in building up an appropriate level of trust in technology.","Causal Strength Justification: Direct causal language ('effects on human users' and 'mitigate the reduced situational awareness' and 'mitigate the risk of complacency effects') explicitly links automation to negative human performance outcomes. | Relevance Justification: Directly addresses how automation (a key AI/autonomous system feature) causally affects human performance (situational awareness and complacency), aligning with the research focus on human-AI interaction in high-risk industries.","y"
"Towards an Operational Design Domain for Safe Human-AI Teaming in the Field of AI-Based Air Traffic Controller Operations","","","AI Feature: adaptable automation | Evidence Type: direct | Causal Strength: 7 | Performance Effect: increase workload demands on the operator","However, adaptable automation may increase workload demands on the operator as a consequence [40].","Causal Strength Justification: Direct causation is indicated by 'as a consequence', linking the AI feature to the human performance effect. | Relevance Justification: Directly addresses how an adaptive AI characteristic (context-aware/adaptive behavior) causally affects human performance (workload demands), aligning with the research focus.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Adaptation in Autonomous Systems","","","As part of autonomous system capabilities, we consider adaptation as a key capability and we make a connection to adaptation of model-based solutions.","This excerpt directly mentions adaptation as a feature of autonomous systems, which relates to context-aware/adaptive behavior in AI, as it involves dynamic response and environment adaptation, distinguishing it from conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Adaptation in Autonomous Systems","","","As part of autonomous system capabilities, we consider adaptation as a key capability and we make a connection to adaptation of model-based solutions.","This excerpt directly mentions adaptation as a feature of autonomous systems, which relates to context-aware/adaptive behavior in AI, as it involves dynamic response and environment adaptation, distinguishing it from conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Adaptation in Autonomous Systems","","","As part of autonomous system capabilities, we consider adaptation as a key capability and we make a connection to adaptation of model-based solutions.","This excerpt directly mentions adaptation as a feature of autonomous systems, which relates to context-aware/adaptive behavior in AI, as it involves dynamic response and environment adaptation, distinguishing it from conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Adaptation capability in autonomous systems","","","As part of autonomous system capabilities, we consider adaptation as a key capability and we make a connection to adaptation of model-based solutions.","This represents an AI feature as it highlights adaptation, a novel characteristic of AI/autonomous systems compared to conventional automation, which aligns with context-aware/adaptive behavior in dynamic environments.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Adaptation capability in autonomous systems","","","As part of autonomous system capabilities, we consider adaptation as a key capability and we make a connection to adaptation of model-based solutions.","This excerpt directly discusses adaptation as a feature of autonomous systems, which relates to context-aware/adaptive behavior in AI, as it involves dynamic response and environment adaptation, though it does not explicitly mention real-time learning or variability handling.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Need for adaptation in autonomous systems","","","In Section 3, we focus on the automation of human cognitive input for unmanned and autonomous operations with considerations of complexity, process modelling, need for adaptation, and the relation of human operators and engineers with autonomous systems in collaborative and complementary settings.","This excerpt explicitly mentions 'need for adaptation' in the context of autonomous operations, which relates to the research focus on context-aware/adaptive behavior where AI systems dynamically respond to environments. It also discusses human-AI interaction in collaborative settings, aligning with the general AI capabilities theme.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Advanced Process Control with MPC","","","Some plants are equipped with advanced process control capabilities, such as model predictive controllers (MPC), which could take part of the cognitive burden from plant operators, but plant operators still monitor and supervise these advanced functions (Qin and Badgw ell, 2003).","This excerpt describes advanced automation features (model predictive controllers) that represent a step beyond conventional automation, as they involve predictive and potentially adaptive control mechanisms, though it does not explicitly mention AI-specific characteristics like non-determinism or opacity.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: machine learning-driven automation","","","The automation of monitoring, sensing, and inspection tasks in the process and energy industries has seen a significant increase in technology development over recent years, particularly due to rapid advances in machine learning methods (Salazar et al., 2020; Bae et al, 2018; van Kessel et al., 2018).","This excerpt directly mentions 'machine learning methods' as a key driver for technology development in automation, which aligns with AI capabilities in data-driven decision-making and adaptive systems, though it does not specify non-deterministic, opacity, or context-aware features explicitly.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: autonomous systems and adaptation","","","autonomous systems, the role of complexity, the need for adaptation and a more detailed analysis of the technical challenges involved.","This excerpt directly references 'autonomous systems' and 'adaptation', which are key AI features related to context-aware/adaptive behavior, as they imply dynamic response and environment adaptation in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Complexity assessment of algorithmic capabilities and autonomous systems","","","We will use these results to look at complexity as a key dimension to assess algorithmic capabilities and autonomous systems.","This directly references AI features by discussing the assessment of algorithmic capabilities and autonomous systems, which are core to AI systems, though it does not specify novel characteristics like non-determinism or opacity.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous takeover capability","","","Unlike the autonomous system level definitions for vehicles such as cars or aircraft, where the expectation is for a human to take over a task from an autonomous system, this situation would require an autonomous system to take over from a human.","This excerpt highlights an AI feature where the autonomous system is capable of taking over tasks from humans, indicating advanced autonomy and decision-making in high-risk industrial contexts, which aligns with novel AI characteristics like adaptive behavior and control mechanisms in human-AI interaction.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: adaptation requirement of autonomous systems","","","focus instead on the aspects of autonomous systems covering the relationship of autonomy and plant complexity, role of plant models, adaptation requirement of autonomous systems, and the interaction of autonomous systems and operators.","This directly mentions 'adaptation requirement of autonomous systems', which aligns with the context-aware/adaptive behavior characteristic in the research focus, indicating a feature where AI systems dynamically respond to environments.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Algorithmic capabilities and autonomous systems assessment","","","We will use these results to look at complexity as a key dimension to assess algorithmic capabilities and autonomous systems.","This directly references AI features by discussing algorithmic capabilities and autonomous systems, which are core to AI systems, and highlights complexity as a dimension for assessment, aligning with the research focus on novel AI characteristics.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous takeover capability","","","Unlike the autonomous system level definitions for vehicles such as cars or aircraft, where the expectation is for a human to take over a task from an autonomous system, this situation would require an autonomous system to take over from a human.","This excerpt highlights an AI feature involving autonomous systems taking over tasks from humans, indicating advanced AI capabilities in high-risk industries like process plants, which relates to novel AI characteristics such as context-aware or adaptive behavior in dynamic environments.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: adaptation requirement of autonomous systems","","","aspects of autonomous systems covering the relationship of autonomy and plant complexity, role of plant models, adaptation requirement of autonomous systems, and the interaction of autonomous systems and operators.","This excerpt mentions 'adaptation requirement of autonomous systems', which relates to context-aware/adaptive behavior as a novel AI characteristic, indicating dynamic response and environment adaptation in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Cognitive task automation","","","This approach also allows application of the analysis presented in Section 2 by considering autonomy as the automation of cognitive tasks carried out by human operators, engineers, and technicians.","This represents an AI/autonomous system feature by explicitly linking autonomy to the automation of cognitive tasks (decision-making, analysis, etc.) traditionally performed by human operators, engineers, and technicians, which is a core characteristic distinguishing advanced autonomous systems from conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Autonomy definition and application scope","","","We define autonomy as the ability of an automation system to complete a task without human intervention. In the context of this paper, we can use this definition for any complete task happening over a perception -situational awareness -decision making - action chain. This can apply to a feedback control loop in contact with the physical plant or it could apply to an inventory management application making inventory observations in a database and generating orders in another database.","This excerpt defines autonomy as a key feature of automation systems, describing their ability to operate without human intervention across various task types, which is foundational to understanding AI/autonomous system capabilities in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Automation of cognitive tasks","","","Automation of many cognitive tasks involving situational awareness and decision making is likely to involve the extension of the capabilities of such models.","This excerpt directly mentions automation of cognitive tasks involving situational awareness and decision making, which are key AI features in human-AI interaction, as they represent advanced capabilities beyond conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: varying autonomy levels","","","The figure illustrates the automation hierarchy and the exchange of information between the different entities . Unit 1 Unit 2 Unit 3Level 0 autonomyLevel 3 autonomyLevel 5 autonomyLevel 3 autonomyLevel 0 autonomy Unit level supervisory & regulatory controlPlantwide supervisory controlPlanning and scheduling","This excerpt explicitly mentions 'autonomous agents of varying autonomy levels' and details a hierarchy with levels 0, 3, and 5, which is a key AI feature distinguishing it from conventional automation by enabling dynamic control and adaptability in human-AI interaction.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Complex plant realization with maintained cognitive balance","","","automation technology advances could lead to the reali sation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems.","This excerpt discusses the interaction between humans and autonomous systems, specifically mentioning the 'balance of the cognitive load,' which is a key aspect of human-AI interaction in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Performance improvement through technology advances","","","technology advances could lead to an increase of the performance of the autonomous system in the form of increased output or fewer number of outages.","This excerpt directly mentions 'autonomous system' and describes how technology advances can enhance its performance, which relates to AI capabilities in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Modeling and Data Requirements for Autonomous Systems","","","A challenge for process plants is that their behaviour is not always possible to model accurately especially regarding dynamic responses. This presents a possible obstacle for building autonomous solutions for greenfield plants and could require extensive data collection and testing campaigns as part of the commissioning phase, further increasing costs.","This excerpt highlights AI-related challenges in building autonomous systems, such as the need for accurate modeling of dynamic responses and extensive data collection, which are key aspects of AI capabilities in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous decision-making requiring simulation capabilities","","","For handling these problems in an autonomous way, both model-based and model-free decision-making approaches will require a plant model capable of simulating the abnormal operation scenarios of interest. The widely used simulation packages in process and energy industries have currently very limited capabilities for simulating such scenarios and future technology developments should address these shortcomings for reaching higher levels of autonomous operations.","This represents an AI feature as it explicitly mentions 'autonomous way' and 'decision-making approaches' (model-based and model-free), which are key characteristics of AI systems, particularly in the context of handling problems and simulating scenarios for higher levels of autonomy, aligning with the research focus on AI/autonomous system features in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: context_adaptive | Feature: Model adaptation to plant changes","","","This will especially be important for model-based autonomous solutions, where the models need to be updated or adapted to the changes in the plants.","This represents a context-aware/adaptive AI feature as it describes autonomous solutions that must dynamically respond to environmental changes (plant modifications) by updating or adapting their models, which aligns with the characteristic of dynamic environment response and variability handling in novel AI systems.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Human Decision-Making in Abnormal Operations","","","Most critical decision-making tasks carried out by plant operators involve the handling of abnormal operations. Often such decision-making tasks involve discrete actions, such as turning machinery on or off, diverting flows, or activating or deactivating control functions.","This excerpt discusses human operator tasks in handling abnormal operations, which relates to conventional automation contexts where human oversight is critical, but it does not explicitly mention novel AI features like non-deterministic or adaptive behavior.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Partial Horizontal Autonomy","","","However, in case of partial horizontal autonomy, where some unit operations are controlled by an autonomous system and others by human operators, the testing process will require the input of human decisions – which could further complicate testing efforts.","This excerpt directly mentions an 'autonomous system' controlling unit operations, which is a key AI capability in high-risk industries, contrasting with full automation by involving human operators in a mixed-control setup.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: non_deterministic | Feature: Data-driven adaptation","","","The need for adaptation connects autonomous systems closely with data-driven methods and analysis.","This excerpt explicitly mentions 'data-driven methods and analysis' in connection with autonomous systems, which relates to the non-deterministic/data-driven characteristic where decisions vary with data/model states.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous control layer with human-AI integration","","","the plantwide control layer is autonomous at Level 3 – both a human operator and an autonomous system will be part of it.","This excerpt explicitly mentions an 'autonomous system' as part of a control layer, which represents an AI capability in contrast to conventional automation that typically follows fixed, pre-programmed rules without autonomy.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: context_adaptive | Feature: Adaptive decision-making via reinforcement learning","","","Autonomous systems relying on reinforcement learning for decision-making tasks would also have to utilise process data directly or indirectly (via a process model) to both improve their performance and to adapt to changing conditions.","This describes AI systems that adapt their behavior based on data and changing conditions, which is a key characteristic of context-aware/adaptive AI systems.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: non_deterministic | Feature: Data-driven forecasting for adaptation","","","Data-driven solutions such as algorithms for generating forecasts for various external factors, can also be integrated to autonomous solutions and be used to facilitate adaptation processes.","This mentions data-driven algorithms that generate forecasts, which implies non-deterministic, model-dependent decision-making based on data rather than fixed rules.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Automation of cognitive tasks","","","Automation of many cognitive tasks involving situational awareness and decision making is likely to involve the extension of the capabilities of such models.","This excerpt mentions automation of cognitive tasks involving situational awareness and decision making, which are key AI capabilities in high-risk industries, aligning with the research focus on AI/autonomous system characteristics.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: feedback | Feature: Feedback Mechanism for Autonomous Systems","","","In an engineering setting, this interface must be able to return feedback for the commands and objectives to inform the operators about the state of the autonomous system, the feasibility of the commands, and to what extent and at what cost the objectives can be achieved.","This represents an AI feature as it highlights the need for feedback mechanisms in autonomous systems to inform human operators, which is critical for trust, transparency, and effective collaboration in high-risk industries, aligning with general AI capabilities in human-AI interaction.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Level 5 Autonomous System for Regulatory Control","","","In this example, the regulatory control for Unit 3 is under the responsibility of a Level 5 autonomous system, which does not need human supervision for carrying out its function.","This represents an AI feature as it specifies a high level of autonomy (Level 5) where the system operates independently without human supervision, illustrating advanced AI capabilities in control mechanisms for industrial applications.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Information Exchange for Human-AI Collaboration","","","This information exchange is highly critical for those applications, where a human operator has to take over in case the autonomous system cannot fulfil its function but it is also important in situations, where a fully autonomous system is operating in a collaborative setting along human operators.","This represents an AI feature as it discusses the critical role of information exchange in scenarios where autonomous systems interact with humans, such as takeover situations or collaborative settings, which is essential for safety and adaptability in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous system definition and testing requirement","","","As we defined autonomous systems in Section 3.2 to emerge from the aggregation of many autonomous tasks, simulated testing of autonomous solutions for process plants will require a closed-loop integration of these tasks with a plant simulator.","This excerpt directly discusses autonomous systems, which are a key AI feature, by defining them and outlining testing needs, aligning with the research focus on AI characteristics in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: technology advances improving autonomous system performance and complexity","","","In Case D, technology advances could lead to an increase of the performance of the autonomous system in the form of increased output or fewer number of outages. In Case A, automation technology advances could lead to the realisation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems.","This excerpt highlights the impact of technology advances on autonomous systems, such as increased output and reduced outages, which relates to AI capabilities in automation, but it does not specify novel AI features like data-driven decision-making or adaptability.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: autonomous systems with human supervision and information exchange","","","the plantwide control layer and the unit level control for Unit 3 are autonomous systems under human supervision and these systems must exchange information with the human operators responsible for the supervision task, which is the third type of information exchange. The information that can be exchanged with other autonomous or software systems could be designed to maximise system","This excerpt mentions 'autonomous systems' and their interaction with human operators and other systems, which relates to AI capabilities in high-risk industries, though it does not explicitly detail novel AI characteristics like non-deterministic or adaptive behavior.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Partial Horizontal Autonomy","","","However, in case of partial horizontal autonomy, where some unit operations are controlled by an autonomous system and others by human operators, the testing process will require the input of human decisions – which could further complicate testing efforts.","This excerpt directly mentions an 'autonomous system' controlling unit operations, which is a key AI capability distinguishing it from conventional automation, as it involves AI systems operating alongside humans in a shared control environment.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: non_deterministic | Feature: Data-driven adaptation","","","The need for adaptation connects autonomous systems closely with data-driven methods and analysis.","This directly mentions 'data-driven methods and analysis' in the context of autonomous systems, which aligns with the non-deterministic/data-driven characteristic where outputs vary with data/model states, leading to uncertainty and unpredictable failures.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous decision-making requiring simulation capabilities","","","For handling these problems in an autonomous way, both model-based and model-free decision-making approaches will require a plant model capable of simulating the abnormal operation scenarios of interest. The widely used simulation packages in process and energy industries have currently very limited capabilities for simulating such scenarios and future technology developments should address these shortcomings for reaching higher levels of autonomous operations.","This represents an AI feature as it explicitly mentions 'autonomous way' and 'decision-making approaches' (model-based and model-free), which are key characteristics of AI systems, particularly in high-risk industries where handling abnormal scenarios autonomously is crucial for safety and efficiency.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: adaptation | Feature: Model adaptation to plant changes","","","This will especially be important for model-based autonomous solutions, where the models need to be updated or adapted to the changes in the plants. The updating of plant models is another cognitive task.","This represents an AI feature as it describes autonomous solutions that adapt their models to environmental changes (plants), which aligns with context-aware/adaptive behavior in AI systems, distinguishing them from static conventional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: non_deterministic | Feature: Data-driven forecasting for adaptation","","","Data-driven solutions such as algorithms for generating forecasts for various external factors, can also be integrated to autonomous solutions and be used to facilitate adaptation processes.","This mentions data-driven algorithms that generate forecasts, which implies non-deterministic decision-making based on data and models.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: context_adaptive | Feature: Adaptive decision-making via reinforcement learning","","","Autonomous systems relying on reinforcement learning for decision-making tasks would also have to utilise process data directly or indirectly (via a process model) to both improve their performance and to adapt to changing conditions.","This describes AI systems that adapt their behavior based on data and changing conditions, which is a key characteristic of context-aware/adaptive AI systems.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Autonomous system integration","","","the plantwide control layer is autonomous at Level 3 – both a human operator and an autonomous system will be part of it.","This explicitly mentions an autonomous system as part of the control layer, representing an AI capability in high-risk industrial settings where both human and AI systems operate together.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: context_adaptive | Feature: Collaborative information exchange","","","the neighbouring units to Unit 3 are in a possible collaborative setting and Unit 3 could be envisioned to share information about its state and receive in return information about the state of its neighbours.","This describes context-aware behavior where units dynamically share information about their states in a collaborative setting, enabling adaptation to neighboring units' conditions.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: context_adaptive | Feature: context-dependent information exchange","","","the information exchange with human operators and supervisors will depend on the context of interaction and for real-time operation needs to be constrained to the capability of the humans for handling this information.","This represents a context-aware/adaptive feature as it involves dynamic response to interaction context and real-time operation constraints, aligning with novel AI characteristics that adapt to environments and human factors.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Information exchange for human-AI collaboration","","","This information exchange is highly critical for those applications, where a human operator has to take over in case the autonomous system cannot fulfil its function but it is also important in situations, where a fully autonomous system is operating in a collaborative setting along human operators.","This represents an AI feature as it discusses the critical role of information exchange in scenarios where autonomous systems interact with humans, such as takeovers and collaboration, which is essential for safety and adaptability in high-risk industries, reflecting general AI capabilities in human-AI interaction.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: feedback | Feature: Feedback mechanism for autonomous systems","","","In an engineering setting, this interface must be able to return feedback for the commands and objectives to inform the operators about the state of the autonomous system, the feasibility of the commands, and to what extent and at what cost the objectives can be achieved.","This represents an AI feature as it highlights the need for feedback mechanisms in autonomous systems to inform human operators, which is critical for trust, transparency, and effective collaboration in high-risk industries, aligning with general AI capabilities in human-AI interaction.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: automation | Feature: Level 5 autonomous system without human supervision","","","In this example, the regulatory control for Unit 3 is under the responsibility of a Level 5 autonomous system, which does not need human supervision for carrying out its function.","This represents an AI feature as it specifies a high level of autonomy (Level 5) where the system operates independently without human oversight, demonstrating advanced AI capabilities in control mechanisms for high-risk applications, aligning with general AI capabilities in automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Limitations in handling disturbances and abnormal conditions","","","Persistent or abrupt disturbances go beyond the current capabilities of autonomous solutions (Khan et al., 2020) and abnormal operating conditions such as a tripping unit or a strong disturbance such as a temporary power loss, are in most cases handled by the plant operators.","This excerpt directly addresses the capabilities of autonomous solutions (a form of AI/automation) by highlighting their current limitations in dealing with unpredictable disturbances and abnormal conditions, which relates to the research focus on AI characteristics in high-risk industries where such scenarios are critical.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","Category: AI_capability | Feature: Machine learning and symbolic AI combination","","","a combination of machine learning and symbolic artificial intelligence approaches could be a way to address this challenge. At the same time, training such solutions will require more advanced simulation and computational capabilities.","This directly describes AI system features (machine learning and symbolic artificial intelligence) as approaches to address challenges, highlighting their capabilities and training requirements, which aligns with the research focus on novel AI characteristics in high-risk industries.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: situational_awareness | Severity: 7","","Automation systems of today cannot handle most abnormal events without operator intervention. In such cases operators decide and implement a course of action, usually within a limited time window and with limited situational awareness.","Severity Justification: Limited situational awareness during abnormal events in high-risk industries can lead to poor decisions and increased risk of accidents. | Relevance Justification: Directly mentions 'limited situational awareness' as a condition under which operators must act, which is a key human performance degradation in the context of automation reliance.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: performance_metrics | Severity: 7","","most upsets in plant operations are due to human error ( Nivolianitou et al., 2006) . This is not surprising when the majority of oversight for plant operations are residing with humans , and in most cases operator errors stem from organi sational or system design faults .","Severity Justification: The text explicitly links human error to most upsets in plant operations, indicating a significant performance degradation issue, though it does not specify novel AI-related aspects. | Relevance Justification: The text mentions human error as a cause of upsets in operations, which relates to performance degradation, but it does not address novel AI characteristics or compare them to traditional automation as specified in the research focus.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: performance_metrics | Severity: 7","","most upsets in plant operations are due to human error ( Nivolianitou et al., 2006) . This is not surprising when the majority of oversight for plant operations are residing with humans , and in most cases operator errors stem from organi sational or system design faults .","Severity Justification: Human error is identified as the primary cause of most upsets in plant operations, indicating significant performance degradation, though it is linked to organizational or system design issues rather than solely individual failure. | Relevance Justification: The text explicitly mentions human error as a cause of operational upsets, which relates to performance degradation, but it does not specifically address novel AI-related degradations vs. traditional automation as requested in the research focus.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: performance_metrics | Severity: 1","","The solid curve and the dashed curve represent the current and possible future technology boundaries, respectively.","Severity Justification: The text only describes graphical elements (curves representing technology boundaries) without any mention of negative impacts on human performance, operators, or engineers. | Relevance Justification: The text does not mention any human performance degradation, cognitive load, trust issues, biases, awareness problems, skill loss, misinterpretation, or specific evaluation metrics related to the research focus on novel AI characteristics vs. traditional automation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: situational_awareness | Severity: 7","","automating the situational awareness and decision-making tasks carried out by the human personnel is still an open problem.","Severity Justification: The excerpt highlights a fundamental problem in automating critical human tasks, suggesting significant performance risks in high-risk industries where situational awareness and decision-making are essential. | Relevance Justification: This directly addresses the research focus on novel AI characteristics, specifically context-aware/adaptive behavior, and relates to human performance degradation in situational awareness and decision-making tasks.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","Category: situational_awareness | Severity: 7","","With increasing automation levels the engagement of these fewer human operators with the plants gets reduced. Sometimes referred to as the paradox of automation, this disengagement is thought to decrease the effectiveness of operators especially since they now have to selectively react to only the most critical situations as the automation system handles most other upsets.","Severity Justification: The text explicitly states that disengagement decreases operator effectiveness, which is a direct performance degradation, though it doesn't specify catastrophic outcomes. | Relevance Justification: This directly addresses reduced situational awareness and engagement due to automation, which aligns with the research focus on human performance degradations in human-AI interaction contexts.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: algorithmic solution | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduced number of upsets, higher efficiency, increased profits","Consequently, in the case when an algorithmic solution is available, it can be argued that such solutions will lead to a reduced number of upsets, higher efficiency, and increased profits, which will make them attractive even when a shortage of workers is not a consideration.","Causal Strength Justification: Direct causal language 'will lead to' explicitly links algorithmic solutions to specific outcomes. | Relevance Justification: Directly addresses causal impact of AI/algorithmic features on performance outcomes, though not specifically tied to human performance degradation.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: algorithmic solution | Evidence Type: direct | Causal Strength: 8 | Performance Effect: reduced number of upsets, higher efficiency, increased profits","Consequently, in the case when an algorithmic solution is available, it can be argued that such solutions will lead to a reduced number of upsets, higher efficiency, and increased profits, which will make them attractive even when a shortage of workers is not a consideration.","Causal Strength Justification: Direct causal language ('will lead to') explicitly links algorithmic solutions to specific outcomes. | Relevance Justification: The text directly connects AI/algorithmic features to performance outcomes (upsets, efficiency, profits), though it does not explicitly mention human performance degradation; it focuses on system-level benefits.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: technology advances | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increase of the performance of the autonomous system in the form of increased output or fewer number of outages","In Case D, technology advances could lead to an increase of the performance of the autonomous system in the form of increased output or fewer number of outages.","Causal Strength Justification: Direct causation is explicitly stated with 'lead to', linking cause (technology advances) to effect (performance increase), though the effect is on autonomous systems, not directly human performance. | Relevance Justification: Low relevance because the causal link involves technology advances affecting autonomous system performance, not directly human performance or the specified AI features (non-deterministic, opacity, adaptive).","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: automation technology advances | Evidence Type: direct | Causal Strength: 8 | Performance Effect: the reali sation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems","In Case A, automation technology advances could lead to the reali sation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems.","Causal Strength Justification: Direct causation is explicitly stated with 'lead to', linking cause (automation technology advances) to effect (realisation of a more complex plant), though the effect is on plant complexity, not directly human performance. | Relevance Justification: Low relevance because the causal link involves automation technology advances affecting plant complexity, not directly human performance or the specified AI features (non-deterministic, opacity, adaptive).","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: automation technology advances | Evidence Type: direct | Causal Strength: 8 | Performance Effect: realisation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems","In Case A, automation technology advances could lead to the realisation of a more complex plant without changing the balance of the cognitive load between humans and autonomous systems.","Causal Strength Justification: Direct causation is explicitly stated with 'could lead to', indicating a strong causal link between automation technology advances and the realisation of a more complex plant, with a specified effect on cognitive load balance. | Relevance Justification: The excerpt involves AI/autonomous systems and human cognitive load, which relates to human performance, but it does not specifically address the novel AI characteristics (non-deterministic, opacity, adaptive) or their direct effects on human performance as requested in the task focus.","y"
"Unmanned and Autonomous Systems: Future of Automation in Process and Energy Industries","","","AI Feature: technology advances | Evidence Type: direct | Causal Strength: 8 | Performance Effect: increase of the performance of the autonomous system in the form of increased output or fewer number of outages","In Case D, technology advances could lead to an increase of the performance of the autonomous system in the form of increased output or fewer number of outages.","Causal Strength Justification: Direct causation is explicitly stated with 'could lead to', indicating a strong causal link between technology advances and system performance improvement. | Relevance Justification: The excerpt involves AI/autonomous systems and performance effects, but it does not specifically address human performance or the novel AI characteristics (non-deterministic, opacity, adaptive) as requested in the task focus.","y"
